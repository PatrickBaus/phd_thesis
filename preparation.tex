\chapter{Preparation}
\section{Grounding and Shielding}
Add parts from "references\\Grounding and Shielding.pdf"
\section{Laser Current Driver}
\subsection{Design}
\subsubsection{Simulation}
\paragraph{Op Amp Stability}
\subsection{Noise Considerations}
\subsection{Voltage Reference}
\subsection{MOSFET Selection}

\section{LabKraken}
\subsection{Design Goals}
LabKraken is a designed to be a asynchronous, resilient data aquisition suite, that scales to thousands of sensors and accross different networks.
\subsection{Hardware}
\subsection{Software Architecture}
LabKraken needs to scale to thousands of sensors, which need to be served concurrently. This problem is commonly referered to as the C10K problem as dubbed by Dan Kegel back in 1999 \cite{10kProblem} and refers to serving \num{10000} concurrent connections via network sockets. While today millions of concurrent connections can be handled by servers, handling \num{10000} can still be challenging, especially, if the data sources are heterogeneous as is typical for sensor networks of different sensors from different manufacturers.

In order to meet the design goals, an asynchronous architecture was chosen and several different architectures were implemented over time. All in all four complete rewrites of the software were made to arrive at the architecture presented here. The reason for the rewrites is mostly historic and can be explained by the history of the programming language Python, which was used to write the code. The first first version was written for Python 2.6 and exclusively supported sensors made Tinkerforge. In 2015, Python 3.5 was released, which supported a new syntax for asynchronous coroutines. The software was rewritten from scratch to support this new syntax, because it made the code a lot more verbose and easier to follow. With the release of Python 3.7 in 2018 asynchronous generator expressions where mature enough to be used in productions and the programm was again rewritten to use the new syntax. In 2021 a new approach was taken and the programm was once more rewritten with a functional programming style. I will discuss each approach in the next sections to highlight the improvements, that were made over time. Each of these sections discusses the same programm, but written in different styles to show the differences.

\subsubsection{Threaded Design}
The first version of LabKraken used a threaded design approach, because the original libraries of the Tinkerforge sensors are built around threads. The following simplified example shows some code to connect to a temperature sensor over the network and read its data.

\inputpython{source/lab_kraken_threads.py}{1}{26}

\subsubsection{Device Identifiers}
Every sensor network needs device identifiers. Preferably those identifiers should be unique. Typically a device has some kind of internal indetifier. Here are a few examples of the sensors used in our network:

\begin{table}[h]
\centering
\begin{tabularx}{0.95\textwidth}{|l|p{6.5cm}|X|}
    \hline
    Device Type& Identifiers& Example\\
    \hline
    GPIB (SCPI)& \textit{*IDN?} returns \newline \$manufacturer,\$name,\$serial,\$revision& \\
    \hline
    Tinkerforge& Each sensor has a base58 encoded integer device id& QE9 (163684)\\
    \hline
    Labnode& Universal Unique Identifier (UUID) & cc2f2159-e2fb-4ed9-\newline8021-7771890b37ad\\
    \hline
\end{tabularx}
\end{table}

As it can be seen above, these identifiers do not guarantee to uniquely identify a device within a network. The Tinkerforge id is the weakest, as it is a \qty{32}{\bit} integer (4.294.967.295 options), which might easily collide with another id from a different manufacturer. The tinkerforge id is presented as a base58 encoded string. An encoder/decoder example can be found in the TinkerforgeAsync library \cite{TinkerforgeAsync}.

The id string returned by a SCPI device is slightly better, but again does not guarantee uniqueness. As it is shown in the example the same device might return a different id defpending on its settings. This typically done by manufacturers for compatibility reasons.

The only reasonably unique id is the universal unique identifier (UUID) or globally unique identifier (GUID), as dubbed by Microsoft, used in the Labnodes. Their id can be used for networks with participant numbers going into the millions.

Calculating the probability of a collision between two random UUIDs is called the birthday problem \cite{BirthdayProblem} in probability theory. A randomly generated version 4 UUID of variant 1 as defined in RFC 4122 \cite{RFC-UUID} has \qty{122}{\bit} of entropy, that is out of \qty{128}{\bit}, \qty{4}{\bit} are reserved for the UUID version and \qty{2}{\bit} for the variant. This gives the probability of at least one collision in $n$ devices out of $M = 2^{122}$ possibilities:
\begin{align}
    p(n) &= 1 - 1 \cdot \left(1 - \frac{1}{M}\right) \cdot \left(1 - \frac{2}{M}\right) \dots \left(1 - \frac{n-1}{M}\right) \nonumber\\
    &= 1 - \prod_{k=1}^{n-1} \left(1 - \frac{k}{M} \right)
\end{align}
Using the Taylor series $e^x = 1+x \dots$, assuming $n \ll M$ and approximating we can simplify this to:
\begin{align}
    p(n) &\approx 1 - \left(e^\frac{-1}{M} \cdot e^\frac{-2}{M} \dots e^\frac{-(n-1)}{M} \right) \nonumber\\
    &\approx 1 - \left(e^\frac{-n(n-1)/2}{M} \right) \nonumber\\
    &\approx 1 - \left(1 - \frac{n^2}{2 M} \right) = \frac{n^2}{2 M}
\end{align}
For one million devices, this gives a probability of about \num{2e-25}, which is negligible.

In the Kraken implementation, all devices, except for the Labnodes, will be mapped to UUIDs using the underlying configuration database. It is up to the user to ensure the uniqueness of the non-UUID ids reported by the devices to ensure proper mapping.


\subsubsection{Limitations} % FIXME: Different title
There is one inherent limitation to the ethernet bus for instrumentation. The ethernet bus is inherently asynchronous and multiple controllers can talk to the device at the same time. Not only that, but different processes within the same controller can talk to the same device. This makes deterministic statements about the device state challenging.

While it is impossible to rule out the possibility of multiple controllers on a network, care was taken to synchronize the workers within Kraken.
\subsection{Databases}
\subsubsection{Cardinality}
\begin{itemize}
 \item TimescaleDB vs Influx
 \item Example Sensors vs. Experiment
\end{itemize}


\section{Short Introduction to Control Theory}
This section will give a very brief introduction into some basic concepts of control theory. Many systems require control over one or more process variables. For example, temperature control of a room or a device, or creating a current from a voltage. All of this requires control over a process and is established trough feedback, which allows a controller to sense the state of the system.

The focus of this section lies on the principels feedback and control and will be detailed in the following sections.

\subsection{Transfer Functions}

\subsection{Open and Closed Loop Systems}
To understand feedback, one needs to take a look at dynamical systems. There are two types of systems: open and closed loop systems. A system is called open loop, if the output of a system does not influece its input as in figure \ref{fig:open_loop}. On the other hand, if the output is connected to the input of the system it is called closed loop system, an example is shown in figure \ref{fig:closed_loop}. $G(s)$ is called the transfer function of the system, while $R(s)$ is the input, $Y(s)$ is the output and $s$ the Laplace variable.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \import{figures/}{open_loop.tex}
        \caption{Open loop system.}
        \label{fig:open_loop}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \import{figures/}{closed_loop.tex}
        \caption{Closed loop system.}
        \label{fig:closed_loop}
    \end{subfigure}
\end{figure}

It is convenient to express the transfer function as its Laplace transform. The unilateral Laplace transform is definded as:
\begin{equation}
    \mathscr{L}\left( f(t) \right) = F(s) = \int_0^\infty f(t) e^{-st}\,dt.
\end{equation}

with $f: \mathbb{R}^+ \to \mathbb{R}$, that is integrable and grows no faster than $e^{s_0t}$ for $s_0 \in \mathbb{R}$. The latter property is important for deriving the rules of differentiation and integration.

To understand the benefits of using the Laplace representation for transfer function a few useful properties must be discussed. First of all the Laplace transform is linear:
\begin{align}
    \mathscr{L}\left(a \cdot f(t) + b \cdot g(t) \right) &= \int_0^\infty (a \cdot f(t) + b \cdot g(t)) e^{-st}\,dt \nonumber\\
    &= a \int_0^\infty f(t) e^{-st}\,dt + b \int_0^\infty g(t) e^{-st}\,dt \nonumber\\
    &= a \mathscr{L}\left(f(t)\right) + b \mathscr{L}\left(g(t)\right)
\end{align}

Another interesting property is the derivative and integral of a function $f$:

\begin{align}
    \mathscr{L}\left(\frac{df}{dt}\right) &= \int_0^\infty \underbracket{f'(t)}_{v'(t)} \underbracket{\vphantom{f'(t)}e^{-st}}_{u(t)}\,dt \nonumber\\
    &= \left[e^{-st} f(t) \right]_0^\infty - \int_0^\infty (-s)f'(t)\,dt \nonumber\\
    &= -f(0) + s \int_0^\infty f'(t)\,dt \nonumber\\
    &= s F(s) - f(0)
\end{align}

\begin{align}
    \mathscr{L} \left( \int_0^t f(\tau)\,d\tau \right) &= \int_0^\infty \left(\int_0^t f(\tau)\,d\tau e^{-st} \right)\,dt \nonumber\\
    &= \int_0^\infty \underbracket{e^{-st}\vphantom{\int_0^t}}_{v'(t)} \underbracket{\int_0^t f(t)\,d\tau}_{u(t)}\,dt \nonumber\\
    &= \left[\frac{-1}{s} e^{-st} \int_0^t f(t)\,d\tau \right]_0^\infty - \int_0^\infty \frac{-1}{s} e^{-s\tau} f(\tau)\,d\tau \nonumber\\
    &= 0 + \frac{1}{s} \int_0^\infty e^{-s\tau} f(\tau)\,d\tau \nonumber\\
    &= \frac{1}{s} F(s) \label{eqn:lapace_integration}
\end{align}

If the initial state $f(0)$ can be chosen to be $0$, the differentiation becomes a simple multiplication by $s$, while the integration becomes a division by $s$. Finally, the most important aspect is, that a simple relation between the input $r(t)$ and the ouput $y(t)$ of a system can be given. The relation between input and the ouput of a system as shown in figure \ref{fig:open_loop} is given by the convolution, see e.g. \cite{pid_basics}. Assuming the system has an initial state of $0$ for $t<0$, hence $r(t<0) = 0$ and $g(t<0) = 0$, one can calculate:

\begin{equation}
    y(t) = (r \ast g)(t) = \int_0^\infty r(\tau) g(t-\tau)\,d\tau
    \label{eqn:convolution}
\end{equation}

Applying the Laplace transformation, greatly simplifies this:
\begin{align}
    Y(s) &= \int_0^\infty e^{-st} y(t)\,dt \nonumber\\
    \overset{\ref{eqn:convolution}}&{=} \int_0^\infty \underbrace{e^{-st}}_{e^{-s(t-\tau)}e^{-s\tau}} \int_0^\infty r(\tau) g(t-\tau)\,d\tau\,dt \nonumber\\
    &= \int_0^\infty \int_0^t e^{-s(t-\tau)} e^{-s\tau} g(t-\tau) r(\tau)\,d\tau\,dt \nonumber\\
    &= \int_0^\infty e^{-s\tau} r(\tau)\,d\tau \int_0^\infty e^{-st} g(t)\,dt \nonumber\\
    &= R(s) \cdot G(s)
\end{align}

This formula is a lot simpler than the convolution of $r(t)$ and $g(t)$, therefore the use of the Laplace transform has become very popular in control theory.

Another property that is heavily used in control theory is the time delay of functions. To show this property, let $f(t-\theta)$ be
\begin{equation}
    g(t) \coloneqq \begin{cases} f(t-\theta), & t \geq \theta \\ 0, & t < \theta \end{cases} \label{eqn:delayed_f}
\end{equation}

The reason for this definition is, that the system must be causal. This means, it is impossible to get data from the future ($t<\theta$). An example is shown in figure \ref{fig:heaviside}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \scalebox{0.75}{%
            \import{figures/}{laplace_no_delay.tex}
        } % scalebox
        \caption{Original signal $f(t)$.}
        \label{fig:heaviside}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \scalebox{0.75}{%
            \import{figures/}{laplace_time_delay.tex}
        } % scalebox
        \caption{Delayed signal $f(t-2)$.}
        \label{fig:heaviside_delayed}
    \end{subfigure}
\end{figure}

The Laplace transform of a delayed signal can be calculated as follows:

\begin{align}
    \mathscr{L}\left( g(t) \right) &= \int_0^\infty f(t-\theta) e^{-st}\,dt \nonumber\\
    \overset{\ref{eqn:delayed_f}}&{=} \int_\theta^\infty f(t-\theta) e^{-st}\,dt \nonumber\\
    \overset{u \coloneqq t-\theta}&{=} \int_0^\infty f(u) e^{-s(u+\theta)}\,du \nonumber\\
    &= e^{-s\theta} \int_0^\infty f(u) e^{-su} \nonumber\\
    &= e^{-s\theta} F(s) \label{eqn:laplace_delayed}
\end{align}

To satisfy the causaulity requirement, the Heaviside function $H(t)$ can be used:
\begin{align}
    \mathscr{L}\left( f(t-\theta) H(t-\theta) \right) = e^{-s\theta} F(s) \label{eqn:laplace_causality}
\end{align}

Lastly, the Laplace transform of $e^{at}$, which is commonly used in differential equations:
\begin{align}
    \mathscr{L}\left(e^{at} \right) &= \int_0^\infty e^{(a-s)t}\,dt = \frac{1}{a-s} \left[e^{(a-s)t} \right]_0^\infty = \frac{1}{s-a} \label{eqn:laplace_exponential}
\end{align}


Using these tools, it is possible calculate the transfer function of a temperature controller. This is done in the next section.

\subsection{A Model for Temperature Control}
\begin{figure}[h]
    \centering
    \scalebox{1}{%
        \import{figures/}{first_order_model.tex}
    } % scalebox
    \caption{Simple temperature model of a generic system.}
    \label{fig:first_order_model_room}
\end{figure}

In order to describe a closed-loop system, one has to first create a model for the process and the controller involved. A simple model can be derived from the idea, that the system at temperature $T_{system}$ has a thermal capacitance $C_{system}$, an influx of heat $\dot Q_{load}$ from a thermal load and a controller removing heat from the system through a heat exchanger with a resistance of $R_{force}$. Additionally, there is some leakage through the walls of the system to the ambient environment via $R_{leakage}$. The analogy of thermodynamics with electrondynamics allows to create the model in figure \ref{fig:first_order_model_room}. Since this this model is to be used for a temperature controller, an assumption to simplify it can be made.

The controller will keep $T_{system}$ constant and if the ambient temperature and $\dot Q_{load}$ is \textit{reasonably stable}, it is easy to see, that a constant thermal flux must flow through $R$ since it cannot pass through the thermal capacitance $C$. \textit{Reasonably stable} means that it can be treated as constant with respect to the temperature controller time constants. This will be further discussed in section \ref{} with regards to system stability. If this assumption holds, the thermal flux from the system load will only cause a constant offset of $T_{in}$, since the heat must be removed by the controller, and the model can be simplified further:

\begin{figure}[h]
    \centering
    \scalebox{1}{%
        \import{figures/}{first_order_model_kirchhoff.tex}
    } % scalebox
\end{figure}

Neglecting the constant thermal flux from the system load and exploiting the analogy of thermodynamics and electrondynamics again, using Kirchhoff's second law, we find:

\begin{align}
    \sum T_i &= 0 \nonumber\\
    T_{in}(t) - \dot{Q}(t) R - \frac 1 C \int \dot{Q}(t)\,dt &= 0 \label{eqn:first_order_model_kirchhoff}
\end{align}

Taking the Laplace transform, applying equation \ref{eqn:lapace_integration} and using $T_{out} = \frac{1}{sC} \dot Q(s)$ to replace $\dot Q$, equation \ref{eqn:first_order_model_kirchhoff} can be written as:
\begin{align*}
    T_{in}(s) - \dot{Q}(s) R - \frac{1}{sC} \dot{Q}(s) &= 0\\
    \dot{Q}(s) = \frac{T_{in}(s)}{R-\frac{1}{sC}} &= \frac{T_{out}}{\frac{1}{sC}}
\end{align*}

This allows to calculate the transfer function of the process $P$:
\begin{align}
    P(s) &= \frac{T_{out}}{T_{in}} = \frac{\frac{1}{sC}}{R-\frac{1}{sC}} \nonumber\\
    &= \frac{1}{sRC + 1} \nonumber\\
    &= \frac{K}{1 + s\tau} \label{eqn:first_order_model}
\end{align}
with the system gain $K$ and the time constant $\tau$. In case of the $RC$ circuit, the gain is $1$, but other systems may a gain or attenuation of $K \neq 1$ in the sensor.

Equation \ref{eqn:first_order_model} is called the transfer function of a first-order model, because its origin is a differential equation of first order. This model describes homogeneous systems, like a room, very well, as can be seen in section \ref{}, but in order to derive the transfer function including the controller and the sensor some more work is required.

Expanding on figure \ref{fig:closed_loop} and equation \ref{eqn:convolution} the closed-loop transfer function becomes:
\begin{equation}
    G(s) = P(s) \cdot S(s)
\end{equation}

and the block diagram becomes

\begin{figure}[ht]
    \centering
    \import{figures/}{open_loop_full.tex}
    \caption{Open loop system with sensor.}
\end{figure}

The transfer funciton of the sensor can, in the most simple case, be modeled as a delay line with delay $\theta$ and $f(t-\theta) = H(t-\theta)$. Using equation \ref{eqn:laplace_delayed} $S(s)$ can be written as
\begin{equation}
    S(s) = e^{-\theta s} .
\end{equation}

The full process model including the time delay is:
\begin{equation}
    G(s) = \frac{K}{1 + s\tau} e^{-\theta s} \label{eqn:first_order_plus_dead_time_model}
\end{equation}

This is called a first-order plus dead-time model (FOPDT) or first-order plus time-delay model (FOPTD). To fit experimental data to this model it is more convenient to transform the transfer function \ref{eqn:first_order_plus_dead_time_model} into the time domain. To calculate the output response an input $U(s)$ is required. In principal any function can do, but a step function is typically used, for example by \citeauthor{ziegler_nichols} \cite{ziegler_nichols} and many others \cite{tuning_rules,pessen_integral,simc,smic2,pid_controllers_for_time_delay_systems,pi_stabilization_of_fopdt_systems, pid_basics}. It is both simple to calculate and apply to a real system. Using equations \ref{eqn:laplace_delayed} and \ref{eqn:laplace_exponential}, the Heaviside $H(t)$ step function transforms as
\begin{equation}
    \mathscr{L} \left(u(t) \right) = U(s) = \mathscr{L} \left( \Delta u H(t) \right) = \frac{\Delta u}{s}
\end{equation}

with the step size $\Delta u$. The output $Y(s)$ can then be calculated analytically.

\begin{align}
    Y(s) &= \frac{\Delta u}{s} \frac{K}{1 + s\tau} e^{-\theta s} \nonumber\\
    &=  K \Delta u \frac{1}{s (1 + s\tau)} e^{-\theta s} \nonumber\\
    &= K \Delta u \left(\frac{1}{s} - \frac{\tau}{s\tau+1} \right) e^{-\theta s} \nonumber\\
    &= K \Delta u \left(\frac{1}{s} - \frac{1}{s+\frac{1}{\tau}} \right) e^{-\theta s}
\end{align}

To derive $y(t)$, the inverse Laplace transform of $Y(s)$ is required. Unfortunately, this is not as simple as the Laplace transform. Fortunately, using \ref{eqn:laplace_exponential} while making sure causaulity is guaranteed as shown in \ref{eqn:laplace_causality}, the simple first order model can easily be transformed back into the time domain.

\begin{align}
    \mathscr{L}^{-1} \left(Y(s)\right) = y(t) &= K \Delta u \mathscr{L}^{-1} \left(\frac{1}{s} e^{-\theta s} \right)  - K \mathscr{L}^{-1} \left( \frac{1}{s+\frac{1}{\tau}} e^{-\theta s} \right) \nonumber\\
    \overset{\ref{eqn:laplace_exponential}}&{=} K \Delta u \cdot 1 \cdot H(t-\theta) - \left(e^{-\frac{t-\theta}{\tau}} \right) H(t-\theta) \nonumber\\
    &= K \Delta u \left(1- e^{-\frac{t-\theta}{\tau}} \right) H(t-\theta)
\end{align}

The time domain solution of the FOPDT model can now be used extract the parameters $\tau$, $\theta$ and $K$ from a real physical system using a fit to the measurement data. The parameter $\Delta u$ is already known, since it is an input parameter. A simulation of the step response of a first-order model with time delay is shown in figure \ref{fig:fopdt}. Here it can be clearly seen, that the output does not change until the time delay $\theta$ has passed and the Heaviside function changes from $0$ to $1$.

\begin{figure}[ht]
    \centering
    \input{images/FOPDT_theory.pgf}
    \caption{Time domain plot of a first-order plus dead time model, showing induvidual components of the model and the composite function $y(t)$. Model parameters: $K= \Delta u = 1$, $\tau=2$, $\theta=4$.}
    \label{fig:fopdt}
\end{figure}

%\cite{pi_first_order_system}



% https://apmonitor.com/pdc/index.php/Main/FirstOrderPlusDeadTime

\subsection{PID tuning rules}

%\subsubsection{SIMC}
We use $\tau_c = \tau$ as suggested by \cite{simc,smic2} for “\textit{tightest possible subject to maintaining smooth control}“.

\section{Allan Deviation}
The Allan variance \cite{adev} $\sigma_y^2(\tau)$ is a two-sample variance and used as a measure of stability. The Allan deviation $\sigma_y(\tau)$ is the square root of the variance. Originally, the Allan variance was used to quantify the performance of oscillators, namely the frequency stability, but it can be used evaluate any quantity. In order to define the Allan variance, a few terms need to be defined first. A single measurement value of the time series $y(t)$ can be written as
\begin{equation}
    \bar y_k(t) = \frac 1 \tau \int_{t_{k}}^{t_{k}+\tau} y(t)\,dt .
\end{equation}
This is the $k$-th measurement with a measurement time or integration time $\tau$. The latter term is frequently used for DMMs. $t_k$ is the sampling inverval including the dead time $\theta$
\begin{equation}
    t_{k+1} = t_k + T
\end{equation}
with
\begin{equation}
    T = \tau + \theta .
\end{equation}

Using this, the standard deviation over $N$ sampled is defined as \cite{adev,psd_to_adev}
\begin{equation}
    \sigma_y^2(N,T,\tau) = \left\langle \frac{1}{N-1} \left(\sum _{n=0}^{N-1}\bar y_n^2(t)-\frac{1}{N}\left(\sum _{n=0}^{N-1} \bar y_n(t)\right)^2\right)\right\rangle
\end{equation}
The $\langle \; \rangle$ denotes the (infinite time) average over all measurands $y_k$. Hence for all $k$.

The Allan variance is a special case of this definition with zero dead-time ($\theta=0$) and only 2 samples:
\begin{align}
    \sigma_y^2(\tau) &= \sigma_y^2(N=2,T=\tau,\tau) \label{eqn:allan_coefficients}\\
    &= \left\langle \frac{\left(\bar y_{k+1} - \bar y_k \right)^2}{2} \right\rangle
\end{align}
In practice, no experiment can take an infinite number of samples, so typically the Allan variance is estimated using a number of samples $m$:
\begin{equation}
    \sigma_y^2(\tau) \approx \frac1 m \sum_{k=1}^m \frac{\left(\bar y_{k+1} - \bar y_{k} \right)^2}{2} \label{eqn:adev_estimator}
\end{equation}

It can be shown \cite{psd_to_adev}, that \ref{eqn:adev_estimator} is indeed more usefull than $\sigma_y^2(N\to\infty,T=\tau,\tau)$, becuase $\sigma_y^2(\tau)$ even for $m \to \infty$ converges for processes, that do not have a convergent $\sigma_y^2(N\to\infty,T=\tau,\tau)$.

Additionally, the Allan variance is mathematically related to the two-sided power spectral density $S_y(f)$ \cite{psd_to_adev}:
\begin{equation}
    \sigma_y^2(\tau) = 2 \int_0^\infty S_y(f) \frac{\sin^4\left( \pi f t \right)}{(\pi f \tau)^2}\,df
\end{equation}

and therefore all processes, that can be seen in the power spectral density can also be seen in the allan deviation. The inverse transform, however, is not always possible as shown by \citeauthor{inverse_adev} \cite{inverse_adev}.

Distinguishing different noise processes using the Allan deviation will be elaborated in the next section.

\subsection{Identifying Noise in Allan Deviation Plots}
It was already mentioned by \citeauthor{adev} in \cite{adev}, that types of noise, whose spectral density follows a power law
\begin{equation}
    S(f) = C \cdot f^\alpha
\end{equation}
can be easily identified in the Allan deviation plot. The most common power coefficients encountered in experimental data can be found in table \ref{tab:adev_alpha_mu} and warants a further discussion.
% TODO: put in (4) from Generation-Recombination Noise, Allan Variance, and Low-Frequency Gain Instabilities in Microwave Amplifiers

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Amplitude noise type& Power-law coefficient $\alpha$& Allan deviation& $\sigma_y(N=2,T=\tau+\theta,\tau)$\\
        \midrule
            White noise & 0& $\propto \tau^{-1/2}$ \cite{adev_noise_types}& \\
            Flicker/Burst noise& -1& $\propto \tau^0$ \cite{adev_noise_types}& \\
            Random walk noise& -2& $\propto \tau^{1/2}$ \cite{adev_noise_types}& \\
            Drift & --& $\propto \tau^1$ \cite{adev_drift}& \\
        \bottomrule
    \end{tabular}
    \caption{Power law representations using the Allan variance.}
    \label{tab:adev_alpha_mu}
\end{table}

In the previous section in equation \ref{eqn:allan_coefficients}, the Allan deviation was defined as the two-sample variance without dead-time. The effect of dead-time will be discussed later. The more important aspect is the general shape of the Allan deviation of the different power laws and will be discussed first. Linear drift, which can not be considered noise, but deterministic process has a very characteristic shape in the Allan deviation plot and will therefore be discussed here as well.

\minisec{White Noise}
White noise is probably the most common type of noise found in measurement data. Johnson noise found in resistors, caused by the random fluctuation of the charge carriers, is one example of mostly white noise up to bandwidth of \qty{100}{\MHz}, from where on quantum corrections are required \cite{nist_johnson_noise}. Amplifiers also tend to have a white noise spectrum at higher frequencies. For these reasons, white noise typically makes up for a considerabe amount of noise in a measurement, unless one measures at very low frequencies. White noise is a series of uncorrelated random events and therefore characterised by a uniform power spectral density, which means there is the same power in a given bandwidth at all frequencies. Another one of its important and often used properties is, that the variance of two uncorrelated variables adds:
\begin{equation}
    \sigma_{x+y}^2  = \sigma_x^2 + \sigma_y^2 + \underbrace{2\,\mathrm{Cov}(x,y)}_{\text{uncorrelated} = 0}\ = \sigma_x^2 + \sigma_y^2
\end{equation}

This allows the simple addition of variances from different sources, but it must be stressed here, that this property is only valid for uncorrelated sources like white noise, although it is usually incorrectly applied to all measurements in disreagard of the dominant noise present. This Unfortunately obscures rather than clarifies the uncertainties involved.

In order to demonstrate the effect of white noise in Allan deviation plots, it was simulated using Python and the excellent AllanTools library \cite{allantools}. The noise generator chosen in the AllanTools library is based on the work of \citeauthor{noise_generation} \cite{noise_generation}. The full Python program code is published online \cite{}. For better comparison, all noise densities are normalized to give an Allan deviation of $\sigma_y(\tau_0)=1$.

Figure \ref{fig:white_noise_simulated} shows a sample of white noise in three different forms. Figure \ref{fig:white_noise_time} is the time series representation. From this sample, the power spectral density was calculated and is shown in figure \ref{fig:white_noise_psd}. The dashed line shows the expectation value of the power spectral density and the Allan deviation.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \scalebox{0.75}{%
            \input{images/white_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:white_noise_time}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
        \scalebox{0.75}{%
            \input{images/white_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:white_noise_psd}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
        \scalebox{0.75}{%
            \input{images/white_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:white_noise_adev}
    \end{subfigure}
    \caption{Different representations of white noise.}
    \label{fig:white_noise_simulated}
\end{figure}

From this simulation, several features can be observed. First of all, the power spectral density is flat and constant with $h_0 = 2$, which is in accordance with table \ref{tab:adev_alpha_mu} and the normalization mentioned earlier. Figure \ref{fig:white_noise_adev} shows the typical $\tau^{-\frac1 2}$ dependence of white noise in the Allan deviation plot. This immediatly explains, why filtering white noise scales with $\frac{1}{\sqrt{n}}$ with $n$ being the number of samples averaged.

\minisec{Burst Noise}
Burst noise, popcorn noise, or sometimes referred to as random telegraph signal is a random bi-stable change in a signal and is caused by a generation recombination processes. This, for example, happens in semiconductors if there is a site, that can trap an electrons for a prologned period of time and then randomly release it. Imporities causing lattice defects are discussed in this context \cite{kay2012operational,burst_noise_psd,popcorn_noise_orgin,technote_ti_popcorn_noise}. Such latttice defects can also be introduced by ion implantation during doping.

The discussion is split into two parts. First the power spectral density is derived and then the Allan variance. The spectral density of burst noise caused by a single trap site was derived in \cite{burst_noise_wiener_khinchin} by \citeauthor{burst_noise_wiener_khinchin}. The author used the autocorrelation function of the burst noise signal and applied the Wiener-Khinchin (Wiener-Хи́нчин) theorem, which connects the autocorrelation function with the power spectral density. A nicely detailed derivation can be found in \cite{fundamentals_of_noise_processes}, which also discusses the preconditions, that must be met, like stationarity of the process. The burst noise signal consists of two energie levels, called $0$ and $1$, split by $\Delta y$. Multiple burst noise signals can be superimposed in a real device. This would then result in mutiple levels, but they can be treated separately. The measurement interval over an even number of transitions, so that one ends in the same state as the measurement has started, is the time $T$. The mean lifetime of the levels is called $\bar \tau_0$ and $\bar \tau_1$:
\begin{equation}
    \bar \tau_{0} \approx \frac 1 N \sum_{i}^N \tau_{0,i} \qquad \bar \tau_{1} \approx \frac 1 N \sum_{i}^N \tau_{1,i}
\end{equation}

Figure \ref{fig:burst_noise} shows a burst noise signal along with the definitions above.

\begin{figure}[h]
    \centering
    \scalebox{1}{%
        \import{figures/}{burst_noise.tex}
    } % scalebox
    \caption{A random burst noise signal.}
    \label{fig:burst_noise}
\end{figure}

Using these definitions, one can then derive \cite{burst_noise_wiener_khinchin}:
\begin{align}
    R_{xx} (T) &= (\Delta y)^2 \cdot \frac{\bar \tau_1 \bar \tau_0 e^{-\left(\frac{1}{\bar \tau_1}+\frac{1}{\bar \tau_0}\right)T}}{\left(\bar \tau_1 + \bar \tau_0\right)^2} \, \text{and}\\
    S(\omega) &= 4 R_{xx}(0) \frac{\frac{1}{\tau_1} + \frac{1}{\tau_0}}{1 + \omega^2 \left(\frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0}\right)^2} \qquad \omega > 0 . \label{eqn:burst_noise}
\end{align}
Note, that the power spectral density is the one-sided version, hence an additional factor of $2$ is included. The d.c. term was ommitted here and can usually be neglected, because it is not relevant for calculating the power spectral density of real measurement data as it only contributes a single peak at $\omega=0$. Using $\frac{1}{\bar \tau} = \frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0}$ equation \ref{eqn:burst_noise} can be rewritten to give a more intuitive form:
\begin{equation}
    S(\omega) = 4 R_{xx}(0) \frac{\bar \tau}{\omega^2 + \bar \tau^2}
\end{equation}
This is a Lorentzian function and it can be seen, that a single trap site has a power spectral density that is proportional to $\frac{1}{f^2}$ at higher frequencies and it is also immediately evident, that for low frequencies, the power spectral density must be flat.

With the spectral density in hand, it is now possible to calculate the Allan variance as it was done by

%\begin{figure}[ht]
%    \centering
%    \input{images/burst_noise_psd.pgf}
%    \caption{Burst noise for $\tau_0=\qty{1}{\second}$ and different $\tau_1$}
%    \label{fig:burst_noise_psd}
%\end{figure}




This type of noise has become less prevalent in modern manufacturing processes, because the quality of the semiconductors has improved. But if, for example a trap site is located very close to an important structure, for example a high precision Zener diode, its effect might be so strong, that it can be clearly seen.


The small wiggles at longer $\tau$ are typical end-of-data errors caused by spectral leakage, because there are insufficient samples to average over \cite{adev_long_tau}. As it was discussed above, the Allan deviation can only be estimated given a limited number of samples using equation \ref{eqn:adev_estimator}. This leads to the fact, that at $\frac{\tau}{2}$ there are only $2$ samples left, so there no averaging possible to improve the estimate of the Allan deviation. This leads to oscillations at low frequencies or large $\tau$.


% TODO: Add adev simulations

The coefficients given here are using the assumption, that the Allan deviation is the appropriate measure for the sample data. This might not always be the case, because the Allan deviation assumes a dead time of $\theta = 0$. This problem was extensively discussed by \citeauthor{psd_to_adev} \cite{psd_to_adev} and even special models were developed to account for the algorithms of modern frequency counters \cite{adev_frequency_counter}. It is therefore important to discuss typical measurement settings for voltmeter to estimate errors that arise from those settings. Typical settings, that affect the dead time of a voltmeter are auto-zeroing and line synchronization. Auto-zeroing is typically done by adding additional measurements to the normal input integratiion cycle. These measurements are a zero measurement to correct for offset drift and a measurement of the reference voltage to correct for gain errors. The implementation details and type of measurements are manufacturer dependant and must be determinded for every multimeter used.

% check \cite{psd_to_adev} Appendix II for details on dead time
% Compare PSD in Generation-Recombination Noise, Allan Variance, and Low-Frequency Gain Instabilities in Microwave Amplifiers to our controller. The hump look similar. Due to popcorn noise

\section{Temperature Controller}

\subsection{Tuning of a PID controller}
The number of emperical algorithms to determine a set of PID parameters ($\mathrm{K_p, K_i, K_d}$) are numerous. In this work only the most common algorithms and a few notable exceptions will be presented.
\subsection{Design}


