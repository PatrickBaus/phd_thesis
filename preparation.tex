% FIXME: Change variable names according to Noise Sources in Bulk CMOS
\chapter{Preparation}
\begin{chapquote}{Lewis Carroll, \textit{Alice in Wonderland}}
``Begin at the beginning,'' the King said gravely, ``and go on till you come to the end: then stop.''
\end{chapquote}
\section{Grounding and Shielding}
Add parts from "references\\Grounding and Shielding.pdf"

\section{Laser System}
%Explanations exist; they have existed for all time; there is always a well-known solution to every human problem — neat, plausible, and wrong.
\subsection{Requirements Laser System}
One purpose of the laser system is to be used for the spectroscopy of highly charged ions in a Penning trap. For example, an interesting transition of \ce{Ar^13+} can be found at $\lambda = \qty{441.25575(17)}{\nm}$ \cite{ar13+_wavelength} with a lifetime of \qty{9.573(6)}{\ms} \cite{ar13+_lifetime}, which corresponds to natural linewidth of $\Gamma \approx 2 \pi \times \qty{16.63(1)}{\Hz}$. While this linewidth is fairly small, there is substantial doppler broadening at \qty{4}{\K} of
\begin{equation}
    \Delta \nu (\lambda = \qty{441}{\nm}, T=\qty{4}{\K}, m=\qty{39.948}{u}) = \frac{2}{\lambda}\sqrt{2 \ln 2 \frac{k_B T}{m}} \approx 2 \pi \times \qty{150}{\MHz} \, . \label{eqn:doppler_broadening}
\end{equation}

\clearpage
\section{Laser Current Driver}
\label{sec:laser_current_driver}
% Include Emission wavelength dependence of characteristic temperature of InGaN laser diodes
% Check Diode Laser Characteristics
% I-lamda in Wavelength Dependence of InGaN Laser Diode Characteristics
% also Determination of piezoelectric fields in strained GaInN quantum wells using the quantum-confined Stark effect
Laser diodes are current driven devices, because
\begin{equation}
    P_{out} \propto I\,, \nonumber
\end{equation}

and the diode current $I$ approximately follows the Shockley equation \cite{shockley_diode}
\begin{equation}
    I = I_0 \left( e^{\frac{qV_d}{k_B T}} - 1\right) \, .
\end{equation}

$k_B$ is Boltzmann constant, $T$ the temperature, $q$ the electron charge and $V_d$ the diode voltage. The exponential dependence of the current on the supply voltage calls for a current source to drive a laser diode safely, without risking thermal damage.

The primary function of a laser driver is to provide a stable, but user adjustable, current. The user adjustable current can typically be modulated at frequencies up to serveral \unit{\MHz} to shape the frequency and ampltitude of the laser beam. Additional features like current and voltage limits aid in protecting the expensive laser diodes. This section deals with the design challenges of such a device used for high precission laser spectroscopy. First the design requirements are derived and then technical specifications are developed.

The focus of this work many lies on two types of laser diodes, indium gallium nitride (InGaN) and aluminium gallium arsenide (AlGaAs), but is not limited to those two types. The former material is used for blue laser didoes at around \qty{450}{\nm}, but also cover up to green wavelengths, and the latter for near-infrared laser diodes at \qty{780}{\nm}, both wavelengths used for experiments in this group.

The design requirements are split into four parts, that need to be discussed. The ambient environment, the diode voltage and current requirements, the modulation bandwith and finally the noise specifications.

\clearpage
\subsection{Design Goals: Ambient Environment}
The laser driver is to be used in a clean laboratory environment. Typical lab temperatures are in the range of \qtyrange{20}{25}{\celsius} and ware mostly met in our labs, before improvements were implemented as part of this work. Humidity is only controlled with dehumidifiers and therefore in the range of \qtyrange{15}{60}{\percent rH}. The air is typically filtered using H14 HEPA filters. Figure \ref{fig:lab_temperature_start_of_project} shows a typical \qty{1}{d} span of the lab temperature as it was found at the start of this project.

\begin{figure}[ht]
    \centering
    \input{images/temperature_011_2016.pgf}
    \caption{Temperature in Lab 011 on 2016-11-26.}
    \label{fig:lab_temperature_start_of_project}
\end{figure}

As it can be seen there are strong oszillations of the temperature as a result of the on–off air conditioning temperature controller. The commercial controller used back then was realized using an IMI Heimeier EMO T Valve \cite{datasheet_heimeier_emo_t}, which is a 2 step valve. Altough this solution was later replaced by a custom design described in section \ref{}, these type controllers are found in many other labs and temperature swings of \qty{2}{\kelvin} must therefore be expected.

These expected environmental parameters can now be used to estimate the design requirements for the laser driver. The more demanding laser system is the \qty{450}{\nm} system \cite{thesis_baus} required for the spectroscopy of highly charged ions \cite{thesis_alex} at GSI. This system was found to be more susceptible to changes of the drive current since the wavelength selective filter element was far broader in comparison to a \qty{780}{\nm} system \cite{two_filter_paper}. This laser is stable over regions of tens of \unit{\uA} and requires a maximum drive current of \qty{145}{\mA} \cite{datasheet_osram_pl450b}.

From these thoughts, the requirements for the driver can be inferred. It should be able to supply at least \qty{150}{\mA} and stay well within \qty{10}{\uA} over the whole environmental range. For a worst-case assumption a tolerance of $3\sigma$ (\qty{99.7}{\percent}) must be met \cite{worst_case_design}.

The environmental parameters that mostly affect current sources are temperature and humidity. Air pressure is typically a matter of concern for high voltage systems \cite{IPC-2221B} and secondary to consider for this design as it is a low voltage system (\qty{<= 48}{\V}). Air pressure effects are also the most expensive to test for, as a pressure chamber is required. While humidity does affect electronics due to corrosion and also indirectly because the epoxy resin used in the FR-4 PCBs and component moulding is hygroscopic and the absorbed humidty leads to swelling and mechanical stress. This effect is very slow at ambient temperature and can easily take days to show \cite{epoxy_humidity}. This parameter is therefore handled via the long-term stability and not specified separately.

Given environmental conditions, the relative coefficients can be calculated. This estimation assumes a minimum setpoint resolution of 2 steps within the mode-hop-free region of the laser and calculates the \qty{99.7}{\percent} confidence interval. The steps are given in table \ref{tab:dgdrive_tempco}:

\begin{table}[hb]
    \centering
    \begin{tabular}{llr}
        Property& Value& Result \\
        \midrule
        Stable range & \qty{10}{\uA}& \qty{10}{\uA}\\
        2 steps of resolution  & $\div 2$& \qty{5}{\uA} \\
        $1 \sigma$  & $\div 2$& \qty{2.5}{\uA} \\
        Maximum output& \qty{150}{\mA}& \qty{17}{\uA \per \A}\\
        Temperature range& \qty{5}{\K}& \qty{3}{\uA \per \A \per \K}\\
        Worst case ($3 \sigma$)& $\div 3$& \qty{1}{\uA \per \A \per \K}\\
    \end{tabular}
    \caption{Estimated requirement for the temperature coefficient of the laser driver.}
    \label{tab:dgdrive_tempco}
\end{table}

While the requirements look moderate at first sight, doing a quick estimation leeds to a temperature coeffiction of\qty[per-mode = symbol]{1}{\uA \per \A \per \K}, preferably better than that when using a higher output driver -- a rather formidable specification for a current source.

Regarding the long-term stability, a \qty{30}{\day} figure can be estimated. One may be inclined to call for a drift, that is smaller than the stable range, but this would be short sighted, as there are other factors to consider. The laser including the external resonator has its own figure of merit regarding the spectral drift rate. \citeauthor{ecdl_stability} \cite{ecdl_stability} reported a drift of \qty{2.9}{\MHz \per \hour}, which was attributed either to the external resonator itself, the piezo or the collimation lens. It is most likely, that this drift was caused by mechanical changes of the external resonator as it defines the output mode of the laser. The mechanical drift limits the required stability of the current source considerably, as a typical frequency change of the internal resontor with the current of \qty[per-mode=symbol]{3}{\MHz \per \micro \A} \cite{diodelaser_modulation} can be assumed. The (linear) ageing drift of the external resonator over \qty{30}{\day} is equivalent to a \qty{720}{\uA} drift over the same period. For the electronics, the drift is assumed to follow an Arrhenius-like equation resulting from stress induced during manufacturing. This may eventually change to a slow linear drift after several months of relaxation. The coefficient may either be a positive or negative and leads to

\begin{table}[hb]
    \centering
    \begin{tabular}{llr}
        Property& Value& Result \\
        \midrule
        Ageing drift limit & \qty{720}{\uA}& \qty{720}{\uA}\\
        $1 \sigma$  & $\div 2$& \qty{360}{\uA} \\
        Maximum output& \qty{150}{\mA}& \qty{2400}{\uA \per \A}\\
        Worst case ($3 \sigma$)& $\div 3$& \qty{800}{\uA \per \A}\\
    \end{tabular}
    \caption{Estimated requirement for the long-term stability of the laser driver.}
    \label{tab:dgdrive_stability}
\end{table}

From these number, it straightforward to see, that the long-term stability of a laser driver is less important than the short-term temperature coefficient since the limiting factor is the mechanical construction of the laser. This necessitates an atomic reference for long-term stability and to compensate for accoustic resonances of the external resonator. Regarding the choice of suitable devices, the tight specification of the temperature coefficient most likely leads to a choice of components, that will pass these long-term criteria as well, alleviating the burden of proof a bit as long-term drift specifications are hard to come by since they literally need solid time to come by and cannot be extrapolated from high temperature burn-in tests \cite{voltage_reference_drift}.

%While \qty{800}{\uA \per \A} over a \qty{30}{\day} period may seem large at first, it is actually very hard to accurately produce a current. To put this number in perspective, a commercial high-end current source like the Keithley \device{2600B} is specified for the \qty{100}{\mA} range at about \qty{170}{\uA \per \A} for a \qty{30}{\day} period when calcuated from the 1-year specification \cite{datasheet_keithley2600}, again assuming an Arrhenius-like equation as the basis.

This leads to the following design specifications regarding the stabilty of the current driver:

\begin{center}
    \begin{tcolorbox}[
        new/auto counter,
        new/number within=chapter,
        colback=red!5!white,
        colframe=red!75!black,
        title=Current controller stability specifications,
        width=0.8\linewidth,
        label={lst:dgDrive_specs_environment}
    ]
    \begin{itemize}
        \item Temperature range \qtyrange[text-series-to-math, reset-text-series = false, reset-math-version = false]{20}{25}{\celsius}
        \item \textbf{Temperature coefficient \qty[text-series-to-math, reset-text-series = false, reset-math-version = false]{<= 1}{\uA \per \A \per \K}}
        \item Humidty (non-condensing) \qty{<= 75}{\percent rH}
        \item Humidty coefficient not specified, but included in the long-term drift
        \item Maximium altitude not specified
        \item Long-term drift over \qty{30}{\day} \qty{<= 800}{\uA \per \A}
    \end{itemize}
    \end{tcolorbox}
\end{center}

%A basic laser current driver design, that has some of the  can be found in the work of \citeauthor{libbrecht_hall} \cite{libbrecht_hall}. While this design contains all the basic features, like a current source, a modulation input and a voltage limit, there are several shortcomings that have emerged over the years with new generations of laser didoes. The laser driver used by legacy applications in this group is based on the aforementioned paper and has been successfully employed in several projects over the years, but several limitations have come up in recent years. In order to derive the design requirements of a new generation of laser drivers the important design elements need to be identified first. The essential design elements are the bulk current source, the modulation current source, the reference element and output programming. The next 4 sections will deal with each element and outline the design goals that were identified while employing the legacy generation of diode drivers in several experiments.

\clearpage
\subsection{Design Goals: Current Source}
% https://www.laserdiodecontrol.com/laser-diode-parameter-overview
% Diode Lasers and Photonic Integrated Circuits (Characteristic temperature)
% Near Threshold Operation of Semiconductor Lasers and Resonant-Type Laser Amplifiers
The change in output current caused by load impedance should be an order of magnitude less than the drift specification to ensure a negligible effect compared to the drift over time. The load resistance presented by the laser diodes most commonly used in our experiments ranges from \qty{50}{\ohm} \cite{datasheet_osram_pl450b} to \qty{30}{\ohm} \cite{datasheet_adl_785} and \qtyrange{10}{15}{\ohm} for \qty{780}{\nm} laser diode \cite{datasheet_sharp_780nm,datasheet_thorlabs_780nm}. It can therefore be estimated as
\begin{align}
    \frac{R_{load}}{R_{out}} &= \frac{I_{set}}{I_{out}} - 1 \leq \qty[per-mode = symbol]{6.7}{\uA \per \A} \nonumber\\
    R_{out} &\geq \frac{\qty{50}{\ohm}}{\qty[per-mode = symbol]{6.7}{\uA \per \A}} = \qty{7.5}{\mega \ohm}
\end{align}

An output impedance of more than \qty{7.5}{\mega \ohm} for slowly changing loads is a fairly moderate requirement and can typically be realised using a high precision control loop with an operational amplifier, but another important aspect is the compliance voltage of the current source.

The compliance voltage is the maximum voltage the current source can apply to the load and is another non-ideal component of a real current source. The required voltage strongly depends on the type of laser diode used. The near-inrared laser diodes discussed above have an operating voltage of \qtyrange{1.5}{3}{\V}, while the Osram \device{PL 450B} blue laser diode is specified for \qtyrange{5.5}{7}{\V}. The \qty{7}{\V} required by the Osram laser diode is fairly high for a Fabry–Perot laser diode and has proven difficult in the past \cite{thesis_baus} as most laser current driver available are designed for the much forward voltage of the near infrad laser diodes. Even higher voltages of around \qtyrange{12}{15}{\V} are required for quantum cascade lasers, but these are currently neither used nor is their use planned in our experiments.

The maximum output current of the laser driver currently required for laser diodes used in our group is \qty{250}{\mA} for the Thorlabs \device{L785H1} \cite{datasheet_thorlabs_780nm}. Therefore a maximum output current of \qty{300}{\mA} is considered sufficient.

The current noise of the laser driver can be estimated from the laser linewidth sought as the laser frequency is sensitive to the injection current. At low frequencies, about \qty[per-mode=symbol]{-3}{\MHz \per \micro \A} can be attributed to the thermal expansion of the internal resonator of the diode due resistive heating \cite{diodelaser_modulation}. Above \qty{1}{\MHz} this effect starts declining and exposes the change of the refractive index due the presence of charge carriers. This second effect is an order of magnitude weaker. Since the frequency sensitivity to current variations of the laser diode drops with higher frequencies the most important range is from DC to \qty{100}{\kHz}.

To estimate the linewidth requirement, it is important to look at the experimental setup. While the spectroscopy of \ce{Ar^13+} at \qty{4}{\K} is limited to around \qty{150}{\MHz}  as shown on page \pageref{eqn:doppler_broadening}, the quantum computing experiments in our group have more stringent needs. It was shown in \cite{ecdl_stability, ecdl_silicone_housing,ecdl_linewidth_scholten}, that with reasonable expense a passive linewidth of less than \qty{100}{\kHz} can be achieved. Using the frequency sensitivity to a current modulation of laser diodes \qty{100}{\kHz} translates to a current noise of \qty{30}{\nA_{rms}} from \qty{1}{\Hz} to \qty{100}{\kHz}. The lower \qty{1}{\Hz} limit is chosen fairly arbitrary, but the presence of $\frac 1 f$-noise inhibits a definition down to DC. There should be negligible amounts noise below \qty{1}{\Hz} compared to the upper \qty{100}{\kHz} though.

The final aspect of the current source, that needs to be specified, is the bandwidth of the current steering input. The bandwidth in these terms define a reasonably flat (\qty{\leq 3}{\dB}) response. As it was discussed above, beyond a frequency of \qty{1}{\MHz}, the frequency sensitivity of the laser diode to current modulation drops by an order of magnitude, altering the transfer function and introducing new challenges for control loops. Therefore a bandwidth of \qty{1}{\MHz} or more is considered sufficient.

Above \qty{1}{\MHz} it is recommended to either use more dedicated solutions like the direct modulation at the laser head presented in \cite{current_mod_paper} or switch to acousto-optic modulators (AOMs) or electro-optic modulators (EOMs).

This leads to the following requirements regarding the current source of the laser driver:

\begin{center}
    \begin{tcolorbox}[
        new/auto counter,
        new/number within=chapter,
        colback=red!5!white,
        colframe=red!75!black,
        title=Current source specifications,
        width=0.8\linewidth,
        label={lst:dgDrive_specs_electrical}
        ]
    \begin{itemize}
        \item Maximum output current \qty{300}{\mA}, optionally \qty{500}{\mA}
        \item \textbf{Compliance voltage \qty[text-series-to-math, reset-text-series = false, reset-math-version = false]{\geq 8}{\V}}
        \item Output impedance \qty{\geq 7.5}{\mega \ohm} at low frequencies (close to DC)
        \item \textbf{Current noise \qty[text-series-to-math, reset-text-series = false, reset-math-version = false]{\leq 30}{\nA_{rms}} from DC to \qty[text-series-to-math, reset-text-series = false, reset-math-version = false]{100}{\kHz}}
        \item \qty{3}{\dB}-bandwidth of the modulation source \qty{\geq 1}{\MHz}
    \end{itemize}
    \end{tcolorbox}
\end{center}

\clearpage
\subsection{Design Goals: User Interface and Form Factor}
The user interface must be remote controllable, as the Penning trap and the laser system is spacially separated with the laser system being located in a special laser lab for environmental and safety reasons. The spatial separation is about \qty{30}{\meter}. Ideally this remote interface is computer controlled to give full access to all features of the laser system. USB or Ethernet is preferred as this does not require extra hardware in the lab.

Regarding the application programming interface (API) support for both Labview and Python, with a strong tendency to Pyhton is favoured. The reason is, that most of the group has switched from Labview to labscript suite \cite{labscript_2013} on Python to run the experiments.

The local interface must be accessible without a computer to allow simple adjustment of the parameters while on the bench.

The form factor should allow integration into standard 19-inch racks to allow simple transportation from the experiment site at GSI to the university for testing and calibration.

\begin{center}
    \begin{tcolorbox}[
        new/auto counter,
        new/number within=chapter,
        colback=red!5!white,
        colframe=red!75!black,
        title=Current source user interface and form factor,
        width=0.8\linewidth,
        label={lst:dgDrive_form_factor}
        ]
    \begin{itemize}
        \item Remote computer interface required
        \item Python and optionally Labview drivers
        \item Rack mountable form factor preferred
    \end{itemize}
    \end{tcolorbox}
\end{center}

\clearpage
\section{LabKraken}
\subsection{Design Goals}
LabKraken is a designed to be a asynchronous, resilient data aquisition suite, that scales to thousands of sensors and accross different networks.
\subsection{Hardware}
\subsection{Software Architecture}
LabKraken needs to scale to thousands of sensors, which need to be served concurrently. This problem is commonly referered to as the C10K problem as dubbed by Dan Kegel back in 1999 \cite{10kProblem} and refers to serving \num{10000} concurrent connections via network sockets. While today millions of concurrent connections can be handled by servers, handling \num{10000} can still be challenging, especially, if the data sources are heterogeneous as is typical for sensor networks of different sensors from different manufacturers.

In order to meet the design goals, an asynchronous architecture was chosen and several different architectures were implemented over time. All in all four complete rewrites of the software were made to arrive at the architecture presented here. The reason for the rewrites is mostly historic and can be explained by the history of the programming language Python, which was used to write the code. The first first version was written for Python 2.6 and exclusively supported sensors made Tinkerforge. In 2015, Python 3.5 was released, which supported a new syntax for asynchronous coroutines. The software was rewritten from scratch to support this new syntax, because it made the code a lot more verbose and easier to follow. With the release of Python 3.7 in 2018 asynchronous generator expressions where mature enough to be used in productions and the programm was again rewritten to use the new syntax. In 2021 a new approach was taken and the programm was once more rewritten with a functional programming style. I will discuss each approach in the next sections to highlight the improvements, that were made over time. Each of these sections discusses the same programm, but written in different styles to show the differences.

\subsubsection{Threaded Design}
The first version of LabKraken used a threaded design approach, because the original libraries of the Tinkerforge sensors are built around threads. The following simplified example shows some code to connect to a temperature sensor over the network and read its data.

\inputpython{source/lab_kraken_threads.py}{1}{26}

\subsubsection{Device Identifiers}
Every sensor network needs device identifiers. Preferably those identifiers should be unique. Typically a device has some kind of internal indetifier. Here are a few examples of the sensors used in our network:

\begin{table}[ht]
\centering
\begin{tabularx}{0.95\textwidth}{|l|p{6.5cm}|X|}
    \hline
    Device Type& Identifiers& Example\\
    \hline
    GPIB (SCPI)& \textit{*IDN?} returns \newline \$manufacturer,\$name,\$serial,\$revision& \\
    \hline
    Tinkerforge& Each sensor has a base58 encoded integer device id& QE9 (163684)\\
    \hline
    Labnode& Universal Unique Identifier (UUID) & cc2f2159-e2fb-4ed9-\newline8021-7771890b37ad\\
    \hline
\end{tabularx}
\end{table}

As it can be seen above, these identifiers do not guarantee to uniquely identify a device within a network. The Tinkerforge id is the weakest, as it is a \qty{32}{\bit} integer (4.294.967.295 options), which might easily collide with another id from a different manufacturer. The tinkerforge id is presented as a base58 encoded string. An encoder/decoder example can be found in the TinkerforgeAsync library \cite{TinkerforgeAsync}.

The id string returned by a SCPI device is slightly better, but again does not guarantee uniqueness. As it is shown in the example the same device might return a different id defpending on its settings. This typically done by manufacturers for compatibility reasons.

The only reasonably unique id is the universal unique identifier (UUID) or globally unique identifier (GUID), as dubbed by Microsoft, used in the Labnodes. Their id can be used for networks with participant numbers going into the millions.

Calculating the probability of a collision between two random UUIDs is called the birthday problem \cite{BirthdayProblem} in probability theory. A randomly generated version 4 UUID of variant 1 as defined in RFC 4122 \cite{RFC-UUID} has \qty{122}{\bit} of entropy, that is out of \qty{128}{\bit}, \qty{4}{\bit} are reserved for the UUID version and \qty{2}{\bit} for the variant. This gives the probability of at least one collision in $n$ devices out of $M = 2^{122}$ possibilities:
\begin{align}
    p(n) &= 1 - 1 \cdot \left(1 - \frac{1}{M}\right) \cdot \left(1 - \frac{2}{M}\right) \dots \left(1 - \frac{n-1}{M}\right) \nonumber\\
    &= 1 - \prod_{k=1}^{n-1} \left(1 - \frac{k}{M} \right)
\end{align}
Using the Taylor series $e^x = 1+x \dots$, assuming $n \ll M$ and approximating we can simplify this to:
\begin{align}
    p(n) &\approx 1 - \left(e^\frac{-1}{M} \cdot e^\frac{-2}{M} \dots e^\frac{-(n-1)}{M} \right) \nonumber\\
    &\approx 1 - \left(e^\frac{-n(n-1)/2}{M} \right) \nonumber\\
    &\approx 1 - \left(1 - \frac{n^2}{2 M} \right) = \frac{n^2}{2 M}
\end{align}
For one million devices, this gives a probability of about \num{2e-25}, which is negligible.

In the Kraken implementation, all devices, except for the Labnodes, will be mapped to UUIDs using the underlying configuration database. It is up to the user to ensure the uniqueness of the non-UUID ids reported by the devices to ensure proper mapping.


\subsubsection{Limitations} % FIXME: Different title
There is one inherent limitation to the ethernet bus for instrumentation. The ethernet bus is inherently asynchronous and multiple controllers can talk to the device at the same time. Not only that, but different processes within the same controller can talk to the same device. This makes deterministic statements about the device state challenging.

While it is impossible to rule out the possibility of multiple controllers on a network, care was taken to synchronize the workers within Kraken.
\subsection{Databases}
\subsubsection{Cardinality}
\begin{itemize}
 \item TimescaleDB vs Influx
 \item Example Sensors vs. Experiment
\end{itemize}

\clearpage
\section{Short Introduction to Control Theory}
This section will give a very brief introduction into some basic concepts of control theory. Many systems require control over one or more process variables. For example, temperature control of a room or a device, or creating a programmable current from a voltage. All of this requires control over a process and is established trough feedback, which allows a controller to sense the state of the system.

The focus of this section is narrowed down to the concept of feedback and control with regard of developing and understanding of PID controllers for temperature control. In the following sections, first, a model for the system and its controller will be develop, and then using the model, tuning of the control parameters using different tuning algorithms will be discussed.

\subsection{Introduction to the Transfer Function and the Laplace Domain}
\label{sec:transfer_function}
There are two types of systems: open- and closed-loop systems. A system is called open loop, if the output of a system does not feed back to its input as in figure \ref{fig:open_loop}. On the other hand, if the output influces the input of the system via feedback it is called a closed-loop system, as shown in figure \ref{fig:closed_loop}. Although feedback can be treated in static systems, it is more useful to treat it in dynamic systems in either time-domain or frequency-domain. $G(s)$ is called the transfer function of the system, while $U(s)$ is the input, $Y(s)$ is the output, $\beta$ is the feedback parameter or feedback fraction. In this section, upper case letters  denote functions in the Laplace domain, while lower case letters are referring to functions in the time domain.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{open_loop.tex}
        \caption{Open-loop system.}
        \label{fig:open_loop}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{closed_loop.tex}
        \caption{Closed-loop system.}
        \label{fig:closed_loop}
    \end{subfigure}
    \caption{Block diagram of closed- and open-loop systems.}
\end{figure}

It is convenient to express the transfer function as its Laplace transform for a number of reasons shown below. The unilateral Laplace transform is definded as:
\begin{equation}
    \mathscr{L}\left( f(t) \right) = F(s) = \int_0^\infty f(t) e^{-st}\,dt.
\end{equation}

with $f: \mathbb{R}^+ \to \mathbb{R}$, that is integrable and grows no faster than $e^{s_0t}$ for $s_0 \in \mathbb{R}$. The latter attribute is important for deriving the rules of differentiation and integration.

To understand the benefits of using the Laplace representation of the transfer function, a few useful properties should be discussed. First of all, the Laplace transform is linear:
\begin{align}
    \mathscr{L}\left(a \cdot f(t) + b \cdot g(t) \right) &= \int_0^\infty (a \cdot f(t) + b \cdot g(t)) e^{-st}\,dt \nonumber\\
    &= a \int_0^\infty f(t) e^{-st}\,dt + b \int_0^\infty g(t) e^{-st}\,dt \nonumber\\
    &= a \mathscr{L}\left(f(t)\right) + b \mathscr{L}\left(g(t)\right)
\end{align}

Another interesting property is the derivative and integral of a function $f$:

\begin{align}
    \mathscr{L}\left(\frac{df}{dt}\right) &= \int_0^\infty \underbracket{f'(t)}_{v'(t)} \underbracket{\vphantom{f'(t)}e^{-st}}_{u(t)}\,dt \nonumber\\
    &= \left[e^{-st} f(t) \right]_0^\infty - \int_0^\infty (-s)f'(t)\,dt \nonumber\\
    &= -f(0) + s \int_0^\infty f'(t)\,dt \nonumber\\
    &= s F(s) - f(0)
\end{align}

\begin{align}
    \mathscr{L} \left( \int_0^t f(\tau)\,d\tau \right) &= \int_0^\infty \left(\int_0^t f(\tau)\,d\tau e^{-st} \right)\,dt \nonumber\\
    &= \int_0^\infty \underbracket{e^{-st}\vphantom{\int_0^t}}_{v'(t)} \underbracket{\int_0^t f(t)\,d\tau}_{u(t)}\,dt \nonumber\\
    &= \left[\frac{-1}{s} e^{-st} \int_0^t f(t)\,d\tau \right]_0^\infty - \int_0^\infty \frac{-1}{s} e^{-s\tau} f(\tau)\,d\tau \nonumber\\
    &= 0 + \frac{1}{s} \int_0^\infty e^{-s\tau} f(\tau)\,d\tau \nonumber\\
    &= \frac{1}{s} F(s) \label{eqn:lapace_integration}
\end{align}

If the initial state $f(0)$ can be chosen to be $0$, the differentiation becomes a simple multiplication by $s$, while the integration becomes a division by $s$. Finally, the most important aspect is, that it is possible to give a simple relation between the input $u(t)$ and the ouput $y(t)$ of a system. The relationship between input and the ouput of a system as shown in figure \ref{fig:open_loop} is given by the convolution, see e.g. \cite{pid_basics}. Assuming the system has an initial state of $0$ for $t<0$, hence $u(t<0) = 0$ and $g(t<0) = 0$, one can calculate:

\begin{equation}
    y(t) = (u \ast g)(t) = \int_0^\infty u(\tau) g(t-\tau)\,d\tau
    \label{eqn:convolution}
\end{equation}

Applying the Laplace transform, greatly simplifies this:
\begin{align}
    Y(s) &= \int_0^\infty e^{-st} y(t)\,dt \nonumber\\
    \overset{\ref{eqn:convolution}}&{=} \int_0^\infty \underbrace{e^{-st}}_{e^{-s(t-\tau)}e^{-s\tau}} \int_0^\infty u(\tau) g(t-\tau)\,d\tau\,dt \nonumber\\
    &= \int_0^\infty \int_0^t e^{-s(t-\tau)} e^{-s\tau} g(t-\tau) u(\tau)\,d\tau\,dt \nonumber\\
    &= \int_0^\infty e^{-s\tau} u(\tau)\,d\tau \int_0^\infty e^{-st} g(t)\,dt \nonumber\\
    &= U(s) \cdot G(s)
\end{align}

This formula is a lot simpler than the convolution of $u(t)$ and $g(t)$, therefore the use of the Laplace transform has become very popular in control theory.

Another property, that is heavily used in control theory, is the time delay of functions. To demonstrate this property, let $f(t-\theta)$ be
\begin{equation}
    g(t) \coloneqq \begin{cases} f(t-\theta), & t \geq \theta \\ 0, & t < \theta \end{cases} \,. \label{eqn:delayed_f}
\end{equation}

The reason for this definition is, that the system must be causal. This means, it is impossible to get data from the future ($t<\theta$). To satisfy this requirement, any constant other than \num{0} may be chosen as well, as is done in later in section \ref{sec:pid_tuning_rules}, when determining tuning parameters and fitting experimental data to a model. An example of such a time delayed function $g(t)$ is shown in figure \ref{fig:heaviside_delayed}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \scalebox{0.75}{%
            \import{figures/}{laplace_no_delay.tex}
        } % scalebox
        \caption{Original signal $f(t)$.}
        \label{fig:heaviside}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \scalebox{0.75}{%
            \import{figures/}{laplace_time_delay.tex}
        } % scalebox
        \caption{Delayed signal $f(t-2)$.}
        \label{fig:heaviside_delayed}
    \end{subfigure}
\end{figure}

The Laplace transform of a delayed signal $g(t)$ can be calculated as follows:

\begin{align}
    \mathscr{L}\left( g(t) \right) &= \int_0^\infty f(t-\theta) e^{-st}\,dt \nonumber\\
    \overset{\ref{eqn:delayed_f}}&{=} \int_\theta^\infty f(t-\theta) e^{-st}\,dt \nonumber\\
    \overset{\tau \coloneqq t-\theta}&{=} \int_0^\infty f(\tau) e^{-s(\tau+\theta)}\,d\tau \nonumber\\
    &= e^{-s\theta} \int_0^\infty f(\tau) e^{-s\tau} \,d\tau \nonumber\\
    &= e^{-s\theta} F(s) \label{eqn:laplace_delayed}
\end{align}

To satisfy the causaulity requirement in the time domain, the Heaviside function $H(t)$ can be used to give a more concise representation of $g(t)$:
\begin{align}
    \mathscr{L}\left( f(t-\theta) H(t-\theta) \right) = e^{-s\theta} F(s) \label{eqn:laplace_causality}
\end{align}

Lastly, the Laplace transform of $e^{at}$ is given, which is commonly used in differential equations:
\begin{align}
    \mathscr{L}\left(e^{at} \right) &= \int_0^\infty e^{(a-s)t}\,dt = \frac{1}{a-s} \left[e^{(a-s)t} \right]_0^\infty = \frac{1}{s-a} \label{eqn:laplace_exponential}
\end{align}


Using these tools, it is possible calculate the transfer function of a closed-loop temperature controller. This is done in the next section.

\clearpage
\subsection{A Model for Temperature Control}
\label{sec:temperature_control_model}
\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{first_order_model.tex}
    } % scalebox
    \caption{Simple temperature model of a generic system.}
    \label{fig:first_order_model_room}
\end{figure}

In order to describe a closed-loop system using a transfer function $G(s)$, one has to first create a model for the process and the controller involved. This section will derive the simple, but very useful first-order model with dead-time. This model can be derived from the idea, that the system at temperature $T_{system}$ has a thermal capacitance $C_{system}$, an influx of heat $\dot Q_{load}$ from a thermal load and a controller removing heat from the system through a heat exchanger with a resistance of $R_{force}$. Additionally, there is some leakage through the walls of the system to the ambient environment via $R_{leakage}$. This analogy of thermodynamics with electrondynamics allows to create the model shown in figure \ref{fig:first_order_model_room}. Since this model is to be used for a temperature controller, more simplifications can be made and a so-called small-signal model can be developed as opposed to the large signal model shown above. The small-signal model is an approximation around a working point, that is valid for small excursions around it, similar to a Taylor approximation. The small signal model can be used calculate the system response to small changes of the controller output in order to estimate the controll parameters.

Using the small signal approach, the system response can be split into a constant and a dynamic part -- the 0\textsuperscript{th} and 1\textsuperscript{st} order of the Taylor approximation. In order to simplify the system shown in figure \ref{fig:first_order_model_room} an assumption can made, that the system load $\dot Q_{load}$ and the flux through $R_{leakage}$ is \textit{reasonably stable}. \textit{Reasonably stable} means that it can be treated as small deviations and additionally any changes are within the bandwidth of the controller and well suppressed. This allows to treat them as (almost) constant effects, which result in an offset applied to the output of the controller. This allows to solely treat the room and its heat capacity in the dynamic model shown in figure \ref{fig:first_order_model}. Here $T_{force}$ and $T_{system}$ were replaced by $T_{in}$ and $T_{out}$ for better readability:

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{first_order_model_kirchhoff.tex}
    } % scalebox
    \caption{First order model.}
    \label{fig:first_order_model}
\end{figure}

This is the classic $RC$ circuit and exploiting the analogy of thermodynamics and electrodynamics again, using Kirchhoff's second law, one finds:

\begin{alignat}{1}
    \sum T_i &= 0 \nonumber\\
    T_{in}(t) - \dot{Q}(t) R - \frac 1 C \int \dot{Q}(t)\,dt &= 0 \label{eqn:first_order_model_kirchhoff}
\end{alignat}

Taking the Laplace transform, applying equation \ref{eqn:lapace_integration}, solving for $ \dot Q(s)$ and using $T_{out} = \frac{1}{sC} \dot Q(s)$ to replace $\dot Q$, equation \ref{eqn:first_order_model_kirchhoff} can be written as:
\begin{align*}
    T_{in}(s) - \dot{Q}(s) R - \frac{1}{sC} \dot{Q}(s) &= 0\\
    \dot{Q}(s) = \frac{T_{in}(s)}{R-\frac{1}{sC}} &= \frac{T_{out}}{\frac{1}{sC}}
\end{align*}

This allows to calculate the transfer function of the process $P$ using:
\begin{align}
    P(s) &= \frac{T_{out}}{T_{in}} = \frac{\frac{1}{sC}}{R-\frac{1}{sC}} \nonumber\\
    &= \frac{1}{sRC + 1} \nonumber\\
    &= \frac{1}{1 + s\tau} = \frac{K}{1 + s\tau} \label{eqn:first_order_model}
\end{align}
with the system gain $K$ and the time constant $\tau$. In case of the $RC$ circuit, the gain is $1$, but other systems may have a gain factor of $K \neq 1$, so it is included here for the sake of generality.

Equation \ref{eqn:first_order_model} is called the transfer function of a first-order model, because its origin is a differential equation of first order. This model describes homogeneous systems, like a room, very well, as can be seen in section \ref{}, but in order to derive the transfer function including the controller and the sensor some more work is required derive the sensor transfer function.

Expanding on figure \ref{fig:open_loop} and equation \ref{eqn:convolution} the open-loop transfer function becomes:
\begin{equation}
    G(s) = P(s) \cdot S(s)
\end{equation}

and the block diagram becomes
\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{open_loop_full.tex}
    }% scalebox
    \caption{Open-loop system with sensor.}
\end{figure}

The transfer function of the sensor, given an ideal linear transducer, can be modeled as a delay line with delay $\theta$ and $f(t-\theta) = H(t-\theta)$. A gain of $1$ is assumed here, because any system gain can already be included in the parameter $K$. Using equation \ref{eqn:laplace_delayed} $S(s)$ can be written as
\begin{equation}
    S(s) = e^{-\theta s} .
\end{equation}

The full process model including the time delay is:
\begin{equation}
    G(s) = \frac{K}{1 + s\tau} e^{-\theta s} \label{eqn:first_order_plus_dead_time_model}
\end{equation}

This is called a first-order plus dead-time model (FOPDT) or first-order plus time-delay model (FOPTD). To fit experimental data to this model it is more convenient to transform the transfer function \ref{eqn:first_order_plus_dead_time_model} into the time domain. To calculate the output response an input $U(s)$ is required. In principal any function can do, but a step function is typically used, for example by \citeauthor{ziegler_nichols} \cite{ziegler_nichols} and many others \cite{tuning_rules,pessen_integral,simc,simc_paper,pid_controllers_for_time_delay_systems,pi_stabilization_of_fopdt_systems, pid_basics}. It is both simple to calculate and to apply to a real system. Using equations \ref{eqn:laplace_delayed} and \ref{eqn:laplace_exponential}, the Heaviside $H(t)$ step function transforms as
\begin{equation}
    \mathscr{L} \left(u(t) \right) = U(s) = \mathscr{L} \left( \Delta u H(t) \right) = \frac{\Delta u}{s}
\end{equation}

with the step size $\Delta u$. The output $Y(s)$ can then be calculated analytically.
\begin{align}
    Y(s) &= U(s) \cdot G(s)\nonumber\\
    &= \frac{\Delta u}{s} \frac{K}{1 + s\tau} e^{-\theta s} \nonumber\\
    &=  K \Delta u \frac{1}{s (1 + s\tau)} e^{-\theta s} \nonumber\\
    &= K \Delta u \left(\frac{1}{s} - \frac{\tau}{s\tau+1} \right) e^{-\theta s} \nonumber\\
    &= K \Delta u \left(\frac{1}{s} - \frac{1}{s+\frac{1}{\tau}} \right) e^{-\theta s}
\end{align}

To derive $y(t)$, the inverse Laplace transform of $Y(s)$ is required. Unfortunately, this is not as simple as the Laplace transform. Fortunately, the required equations were already derived in equations \ref{eqn:lapace_integration} and \ref{eqn:laplace_exponential}. Now, making sure causaulity is guaranteed as shown in \ref{eqn:laplace_causality}, the simple first order model can be transformed back into the time domain.
\begin{align}
     y(t) &= \mathscr{L}^{-1} \left(Y(s)\right) \nonumber\\
     &= K \Delta u \mathscr{L}^{-1} \left(\frac{1}{s} e^{-\theta s} \right)  - K \mathscr{L}^{-1} \left( \frac{1}{s+\frac{1}{\tau}} e^{-\theta s} \right) \nonumber\\
    \overset{\ref{eqn:laplace_exponential}}&{=} K \Delta u \cdot 1 \cdot H(t-\theta) - \left(e^{-\frac{t-\theta}{\tau}} \right) H(t-\theta) \nonumber\\
    &= K \Delta u \left(1-e^{-\frac{t-\theta}{\tau}} \right) H(t-\theta) \label{eqn:first_order_plus_dead_time_model_time-domain}
\end{align}

The time domain solution of the FOPDT model can now be used extract the parameters $\tau$, $\theta$ and $K$ from a real physical system.

The procedure can be summarized from the above as follows. The controller must be set to a constant output and the room must be given time reach equlibrium. Once the temperature has settled, an output step of $\Delta u$ is applied. The system will respond after a time delay and then follow an exponential function. A simulation of the step response applied to a first-order model with time delay is shown in figure \ref{fig:fopdt}. The gain is $K=1$. The solid black line showns the response of the transfer function, including the system and the sensor. The dashed lines show the individual components, the Heaviside function and the exponential term. The controller output step $\Delta u = 1$ is applied at $t=0$ and not shown explicitely. Now, it can be clearly seen, that the sensor does not register a change until the time delay $\theta$ has passed and the Heaviside function changes from $0$ to $1$. Then the system responds with an exponential decay towards \num{1}.

\begin{figure}[ht]
    \centering
    \input{images/FOPDT_theory.pgf}
    \caption{Time domain plot of a first-order plus dead time model showing individual components of the model and the composite function $y(t)$. Model parameters used: $K= \Delta u = 1$, $\tau=2$, $\theta=4$.}
    \label{fig:fopdt}
\end{figure}

So far, only open-loop systems were discussed. With the system parameters in hand, it is now possible to design a controller around the system and close the loop to achieve a stable system. This is shown in the next section.

\clearpage
\subsection{PID Controller Basics}
\label{sec:pid_tuning_rules}
While there are many different types controllers, like the bang–bang controller utilized in the original lab temperature controller, which turns on at a certain threshold and turns off at another threshold, that resulted in the saw-tooth shaped room temperature curve shown in figure \ref{fig:lab_temperature_start_of_project}, a continuous control system is desired to keep fluctions to a minimum. The most commonly used controller type for non-integrating systems is the proportional–integral–derivative (PID) controller \cite{pid_in_industry}. A non-integrating system is a system without memory, that does not depend on previous inputs. Given the same input, a non-integrating will always return to the same steady state. The advantage of applying a PID controller is, that the controller does not need any special knowledge of system model. A universal PID is simple to implement and can be tuned to control a wide range of systems. While there are many different variations of the PID algorithm \cite{pid_controller}, this section only introduces the basic, parallel, PID controller and deals with some of the shortcomings in a practical application.

In order to extend the FOPDT system, derived in the previous section \ref{sec:temperature_control_model}, with the PID controller, one must move to a closed-loop system. Extending \ref{fig:closed_loop} and inserting a new control block into the transfer function yields figure \ref{fig:closed_loop_pid}.
\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{closed_loop_pid.tex}
    }% scalebox
    \caption{Closed-loop system with a PID controller.}
    \label{fig:closed_loop_pid}
\end{figure}

The error signal $E(s)$ used by the PID controller is the difference between the setpoint and the control parameter, in this case the room temperature. The transfer function of the PID controller can be split into three parts. A proportional part, that is proportional to the error representing the present, an integral part, that is proportional to the accumulated error, representing the past, and a derivative part, that is proportional to the change in the the error, extrapolating into the future.
\begin{align}
    c(t) &= k_p e(t) + k_i \int_0^t e(\tau) \,d\tau + k_d \frac{\mathrm{d}e(t)}{\mathrm{d}t} \label{eqn:pid_controller}\\
    C(s) &= k_p + k_i \frac{1}{s} + k_d s \label{eqn:pid_controller_laplace}
\end{align}

The following discussion will mostly focus on equation \ref{eqn:pid_controller}, because, the time-domain equation is the one, that can be implemented in software. As hinted above, there are a few shortcommings with the classic PID equation, when used in a real system, when there are dynamic changes of the PID parameters, e.g. the setpoint or $k_i$.

The first problem to be addressed is occuring, when changing the PID parameter $k_i$. Assuming a settled system without external disturbances, the output is fully determined by the integrator value. Now, when $k_i$ is changed, the output immediate changes, due to the change of the integral term. This is unintended. To fix this, the integral term must changed to
\begin{equation}
    k_i \int_0^t e(\tau) \,d\tau \Rightarrow \int_0^t k_i(\tau) e(\tau) \,d\tau \,.
\end{equation}

This way, when adjusting $k_i$, its new value is applied to future error values only and there is no sudden kick.

The next issue is called \text{derivative kick}. When looking at the derivative part of equation \ref{eqn:pid_controller}, it can be seen that when instantantly changing the setpoint, as in a step function, $\frac{\mathrm{d}e(t)}{\mathrm{d}t} \to \infty$. This behaviour is not intended and to fix this, the derivative part can be modified as follows.
\begin{align}
    \frac{\mathrm{d}e(t)}{\mathrm{d}t} &= \frac{\mathrm{d}\left(u(t) - y(t)\right)}{\mathrm{d}t} \nonumber\\
    &= \underbrace{\cancel{\frac{\mathrm{d}u(t)}{\mathrm{d}t}}}_{\to \infty} - \frac{\mathrm{d}y(t)}{\mathrm{d}t} \nonumber\\
    &=- \frac{\mathrm{d}y(t)}{\mathrm{d}t}
\end{align}

The new derivative term is equal to the unmodfied one, except in case of setpoint changes. Removing the setpoint from the equation, the controller behaves as intended. This solution is sometimes called \textit{derivative on measurement} as opposed to \textit{derivative on error}.

The derivative term is also the cause the final problem to be discussed. Assuming a noisy input and by chance there is a very short input spike due to noise. The differential in the derivative term will again be sent to very high values, pushing the output await from the optimal value forcing the controller to rebalance.
\begin{figure}[hb]
    \centering
    \input{images/sim_pid_controller_bode.pgf}
    \caption{Magnitude plot over frequency of the PID controller transfer function. Both the ideal PID controller and the PID controller with a filtered derivative are shown.}
    \label{fig:sim_pid_controller}
\end{figure}

To further discuss the problem and the solution it is best to visit the frequency domain and visualize the transfer function as in figure \ref{fig:sim_pid_controller}. The ideal PID controller without filtering of the derivative can be seen to show a very strong response to frequecy inputs. This is due to the integral action, which removes any (constant) offset. It needs to have infinite gain at DC to push the offset to zero. In reality this is limited by the input noise. Then follows a plateau, whith a magnitude of $k_p$ for the proportinal term and finally the differential gain start growing in magnitude and is ever growing with rising frequency, just as expected.

With some knowledge about the process or the sensor it is possible to define an upper frequecy, above which inputs become unrealistic and must therefore be unwanted noise. By filtering the derivative term with a first order filter causes it to roll off and its gain becomes constant. The tranfer function then changes to

\begin{equation}
    C(s) = k_p + k_i \frac{1}{s} + \frac{k_d s}{1 + s \alpha k_d} \,. \label{eqn:pid_controller_filtered}
\end{equation}

Typically $\alpha$ is in the range of \numrange{0.05}{0.2} \citep[p. 129]{pid_controller}.

Another alternative is to filter the whole input. Depending on the filter cutoff, there is not much difference to equation \ref{eqn:pid_controller_filtered}, because the filter will not touch the proportional and integral part of the transfer function if it is well within its passband.

From figure \ref{fig:sim_pid_controller} it also be seen, why in some publications, the gain $k_p$ is applied to all three terms and $k_i$ and $k_d$ are replaced with $T_i$ and $T_d$ to accomodate for that.
\begin{equation}
    C(s) = k_p \left(1 \frac{1}{T_i s} + \frac{T_d s}{1 + s \alpha T_d} \right) \label{eqn:pid_controller_series}
\end{equation}

Using this form allows to shift the curve up and down keeping its shape instead of just the $k_p$ part, thus chaging the corner frequencies. The alternative form is only given here for the sake of completeness. The authors only uses the ideal form shown in equation \ref{eqn:pid_controller_laplace} with the parameters $k_p$, $k_i$ and $k_d$.

This concludes the discussion of the PID controller and begs the question of how to derive the optimal PID parameters from a given system or model. The next section discusses these tuning rules.

\clearpage
\subsection{PID Tuning Rules}
\label{sec:pid_tuning_rules}
While there are many PID tuning rules to be found in literature, their application depends on the underlying system and the desired system response. This section will discuss several of the proposed solutions and compare them to the authors use case. It aims to give a simple method to determine decent PI(D) parameters for the applications found in the lab. Among the methods discussed are the most classic set of tuning rules developed by \citeauthor{ziegler_nichols} \cite{ziegler_nichols}, an improved version of \citeauthor{simc_paper} \cite{simc_paper}, that promises better performance for non-integrating systems. These rules all include simple instruction to extract the neccesary parameters using pen and paper. Using a computer and fitting algorithms, the bar for \textit{simple} has been raised considerably, so more complex approaches can be undertaken, that extract more parameters from the system. Using these additional parameter more precise control is promised by \citeauthor{pid_basics} \cite{pid_basics, advanced_pid_control} with a method called AMIGO. Finally, it is possible to shape the control loop to result in a desired transfer function. This technique is mostly used in motor control \cite{pid_controller,advanced_pid_control} and also requires the model parameters.

All of these rules will be compared against a demo model of a room to explain the details. It is a first order model with delay, that was derived in equation \ref{eqn:first_order_plus_dead_time_model}. The discussion is limited to the FOPDT model only, because the systems treated in this work could be modelled very well using these equation. Higher models are discussed in more details for example in \cite{advanced_pid_control,pid_controller} should the reader encounter such system and require the parameters.

\begin{equation}
    G(s) = \frac{K e^{-\theta s}}{1 + s \tau} \label{eqn:demo_process_model}
\end{equation}

The following parameters were extracted from Lab 011, using the techniques shown in section \ref{sec:temperature_control_model} using equation \ref{eqn:first_order_plus_dead_time_model_time-domain}. The details are discussed in section \ref{}. The gain has be scaled to the full scale output of the controller.
\begin{table}[hb]
    \centering
    \begin{tabular}{ccc}
        \toprule
        Gain K& Lag $\tau$& Delay $\theta$ \\
        \midrule
        \qty{13.07}{\K}& \qty{395}{\s}& \qty{187}{\s}\\
        \bottomrule
    \end{tabular}
\end{table}

Before detailing the tuning parameters, the loop shaping method shall be explained first, because it used to derive the SIMC rules proposed by \citeauthor{simc_paper} and can also be used to derive custom rules. The aim of this method is to derive a controller, that shapes the model in such a way, that a desired system response to setpoint changes is achieved. A general closed-loop system with a controller $C$ and a system $G$ is shown in figure \ref{fig:closed_loop_controller}. This will be used a basis to find the required controller for a desired transfer function $\frac{Y(s)}{U(s)}$.
\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{closed_loop_controller.tex}
    }% scalebox
    \caption{Closed-loop system $G$ with a controller $C$.}
    \label{fig:closed_loop_controller}
\end{figure}

Starting with the transfer function of the controlled system including the controller and the system, most experimenters would, at least in a feaverish dream, prefer a transfer function of the following divine form
\begin{equation*}
    \frac{Y(s)}{U(s)} = 1 \,,
\end{equation*}
but unfortunately life is more profane and there is no controller, that will always (and with warp speed) force a system to a certain setpoint. One may therefore settle for the second-best choice, a first-order low pass with a slow roll-off, also a small delay must be added, to ensure causality. One therefore arrives at
\begin{equation}
    \frac{Y(s)}{U(s)} = \frac{e^{-\theta s}}{1 + s \tau_c}\,, \label{eqn:desired_transfer_function}
\end{equation}
where $\tau_c$ is the closed-loop time constant and a measure for the aggressiveness of the controller. A small $\tau_c$ results in a more aggressive controller.

From \ref{fig:closed_loop_controller} the closed-loop transfer function is
\begin{align*}
    \frac{Y(s)}{U(s)} &= \frac{C(s) G(s)}{C(s) G(s) + 1} \\
    \Rightarrow C(s) &= \frac{1}{G(s)} \frac{1}{\frac{Y(s)}{U(s)} -1}
\end{align*}

Using the desired tranfer function \ref{eqn:desired_transfer_function} yields
\begin{align}
    C(s) &= \frac{1}{G(s)} \frac{e^{-\theta s}}{s \tau_c +1 - \underbrace{e^{-\theta s}}_{\approx 1 - \theta s}}\\
    &\approx \frac{1}{G(s)} \frac{e^{-\theta s}}{s (\tau_c + \theta)} \,.
\end{align}

$e^{-\theta s}$ was approximated using a first-order Taylor expansion. Finally, substituting equation \ref{eqn:demo_process_model} results in
\begin{align}
    C(s) &= \frac{1}{K} \frac{s \tau + 1}{(\tau_c + \theta) s} \nonumber\\
    &= \underbrace{\frac{1}{K} \frac{\tau}{\tau_c + \theta}}_{k_p} + \underbrace{\frac{1}{K} \frac{1}{\tau_c + \theta}}_{k_i} \frac{1}{s}\,.
\end{align}

This is a PI controller with $k_p = \frac{1}{K} \frac{\tau}{\tau_c + \theta}$ and $k_i = \frac{1}{K} \frac{1}{\tau_c + \theta}$. From these calculations, it can be seen, that a first-order model can typically be treated using a PI controller. Second-order (and higher order) models typically neccesitate a PID or more sophisticated controller for optimal control. The problems discussed in this work mainly focus in temperature control of (mostly) homogeneous objects, so the focus lies on the PI controller for most the remaining section, but the ideas and simulations can similarily be applied to the PID controller as well. Any caveats to be expected when treating a PID instead of a PI controller will be mentioned.

Using the loop shaping technique, it is fairly easy to derive custom rules in case the model parameters can be extracted. As it was said above, one such loop-shaped tuning rule is the SIMC ruleset and the authors give rules for an ample variety of different models and also investigate the parameter choice regarding stability, load and setpoint disturbances. It is therefore recommended to check \cite{simc_paper} for an appropriate set of rules for more complex models in order to save time and effort before attempting a custom approach.

\begin{table}
    \centering
    \begin{tabular}{lcccc}
        \toprule
        Tuning Rule& $k_p$& $T_i$ & $T_d$ & Source \\
        \midrule
        Z-N PI & $\frac{0.9 \tau}{K \theta}$ & $\frac{\theta}{0.3}$ & -- & \cite{ziegler_nichols}\\
        Z-N PID & $\frac{1.2 \tau}{K \theta}$ & $2 \theta$ & $\frac{\theta}{2}$ & \cite{ziegler_nichols}\\
        SIMC PI & $\frac{\tau}{K (\tau_c + \theta)}$ & $\min\left(\tau, 4 (\tau_c+\theta)\right)$ & -- & \cite{simc_paper}\\
        SIMC PID & $\frac{\tau_1}{K (\tau_c + \theta)}$ & $\min\left(\tau_1, 4 (\tau_c+\theta)\right)$ & $\tau_2$ & \cite{simc_paper}\\
        AMIGO PI & $\frac{0.15}{K} + \left(0.35 - \frac{\tau \theta}{\left(\tau + \theta\right)^2}\right) \frac{\tau}{K \theta}$ & $0.35 \theta + \frac{13 \tau^2 \theta}{\tau^2 + 12 \tau \theta + 7 \theta^2}$ & -- & \citep[p. 228]{advanced_pid_control}\\
        AMIGO PID & $\frac{1}{K} \left(0.2 + 0.45 \frac{\tau}{\theta}\right)$ & $\frac{0.4 \theta + 0.8 \tau}{\theta + 0.1 \tau} \theta$ & $\frac{0.5 \tau \theta}{0.3 \theta + \tau}$ & \citep[p. 233]{advanced_pid_control}\\
        \bottomrule
    \end{tabular}
    \caption{PI/PID parameters for different tuning rules. The PI controllers assume a first-order model, the PID rules are required when dealing with a second-order model.}
    \label{tab:pid_tuning_parameters}
\end{table}

For reasons of brevity, in table \ref{tab:pid_tuning_parameters}, the PID parameters are given as $k_p$, $T_i$ and $T_d$ as introduced in equation \ref{eqn:pid_controller_series}. $k_i$ and $k_d$ can be calculated from
\begin{align*}
    k_i &= \frac{k_p}{T_i}\\
    k_d &= k_p T_d\,.
\end{align*}

Regarding the SIMC PI/PID algrorithm, \citeauthor{simc_paper} \cite{simc_paper} and \citep[ch. 5]{simc} suggests using $\tau_c = \theta$ for “\textit{tightest possible subject to maintaining smooth control}“. Following this recommendation, the minimum can be calculated from the parameters of this example as $\min\left(\tau, 4 (\tau_c+\theta)\right) = \min\left(\tau, 8 \theta\right) = \tau$.

Using the rules above, the full system can be simulated now. This was done using Python. The simulation source code can be found in \external{data/simulations/sim\_pid\_controller.py} as part of the online suplemental material \cite{supplemental_material}. The simulation can be used to model arbitrary PI(D) controller and arbitrary models can be used as well. It allows to compare different settings before applying them to a real system. It also considereably shortens deployment times, because especially for systems with long timescales, it becomes difficult to test several parameter sets on the fly, so using a simulation can reduce this time to a few minutes instead of hours.

The simulation emulates the PID controller developed for the lab temperature controller. By default is has a sampling rate of \qty{1}{\Hz}. The simulation  will apply a setpoint change of \qty{+1}{\K} \qty{10}{\s} into the simulation. After the simulation it will plot the time domain response of the controlled system. The setpoint change in this scenario is very similar to the load disturbances, that are expected. Typically a noise source is used instead, but in contrast to the statistical noise, that is used to test for disturbance rejection, the situation in the labs are different and cannot be moddeled with stationary noise. While there is some noise coming from the sensor, the major disturbances are usually caused by experimenters instead of the lab itself. These are event like a device being switched on or off for an extended period of time, longer, than the controller needs to settle. This is equivalent to a setpoint change in terms of the error term in equation \ref{eqn:pid_controller}, since there is no difference in the error term between a setpoint and a process variable change. Do note, this is not true for the PID controller, whose derivative term directly works on the measurement (or process variable) as this was explicitely implemented above. For PID controllers, there is therefore a difference between setpoint change behaviour and noise rejection. This must be kept in mind and tested accordingly.

Simulating the model above and using the PI parameters derived from table \ref{tab:pid_tuning_parameters}, gives the plot shown in figure \ref{fig:pid_controller_comparison}.

\begin{figure}[ht]
    \centering
    \input{images/sim_pid_controller_comparison.pgf}
    \caption{Different PI Controllers tuned with parameter derived using the following methods: Ziegler-Nichols, SIMC and AMIGO. The system model is the FOPTD model for room 011.}
    \label{fig:pid_controller_comparison}
\end{figure}

As it can be seen in figure \ref{fig:pid_controller_comparison}, the Ziegler-Nichols tuning rule produces a very aggressive PI controller, that shows quite a bit ringing, which is undesired for this application. The AMIGO rules are rather conservative, but do not produce any overshoot. The SIMC rules have proven the most useful for this application so far. This experience is in line with the results from \citeauthor{liebmann_thesis} \cite{liebmann_thesis}, who tested different PID tuning algorithms for their viability for temperature control in the labs discussed here.

To conclude, several PID tuning rules were presented and using a Python simulation tool it is possible test a set of PID parameters beforehand. Using an example, the different tuning rules were applied to a model for a real lab and the SIMC tuning rules were found to give the best results for this application.

\clearpage
\section{Noise and Allan Deviation}
\label{sec:allan_deviation}
The Allan variance \cite{adev} $\sigma_A^2(\tau)$ is a two-sample variance and used as a measure of stability. The Allan deviation $\sigma_A(\tau)$ is the square root of the variance. Originally, the Allan variance was used to quantify the performance of oscillators, namely the frequency stability, but it can be used evaluate any quantity. In order to define the Allan variance, a few terms need to be defined first. A single measurement value of the time series $y(t)$ can be written as
\begin{equation}
    \bar y_k(t) = \frac{1}{\tau} \int_{t_{k}}^{t_{k}+\tau} y(t)\,dt . \label{eqn:allan_variance_measurement}
\end{equation}
This is the $k$-th measurement with a measurement time or integration time $\tau$. The latter term is frequently used for DMMs. $t_k$ is the start of the $k$-th sampling inverval including the dead time $\theta$
\begin{equation}
    t_{k+1} = t_k + T
\end{equation}
with
\begin{equation}
    T \coloneqq \tau + \theta .
\end{equation}

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{allan_variance_definitions.tex}
    }% scalebox
    \caption{Measurement interval according to equation \ref{eqn:allan_variance_measurement}}
    \label{fig:allan_variance_definitions}
\end{figure}

Using this, the deviation over $N$ samples is defined as \cite{adev,psd_to_adev}
\begin{equation}
    \sigma_y^2(N,T,\tau) = \left\langle \frac{1}{N-1} \left(\sum _{k=0}^{N-1}\bar y_k^2(t)-\frac{1}{N}\left(\sum _{k=0}^{N-1} \bar y_k(t)\right)^2\right)\right\rangle
\end{equation}
The $\langle \; \rangle$ denotes the (infinite time) average over all measurands $y_k$ or, simply put, the expected value.

The Allan variance is a special case of this definition with zero dead-time ($\theta=0$) and only 2 samples:
\begin{align}
    \sigma_A^2(\tau) &= \sigma_A^2(N=2,T=\tau,\tau) \label{eqn:allan_coefficients}\\
    &= \left\langle \frac{\left(\bar y_{k+1} - \bar y_k \right)^2}{2} \right\rangle
\end{align}
It can be shown \cite{psd_to_adev}, that \ref{eqn:adev_estimator} is indeed more useful than $\sigma_A^2(N\to\infty,T=\tau,\tau)$, because $\sigma_A^2(N=2,T=\tau,\tau)$ converges for processes, that do not have a convergent $\sigma_A^2(N\to\infty,T=\tau,\tau)$.

In practice, no experiment can take an infinite number of samples, so typically the Allan variance is estimated using a number of samples $m$:
\begin{equation}
    \sigma_A^2(\tau) \approx \frac1 m \sum_{k=1}^m \frac{\left(\bar y_{k+1} - \bar y_{k} \right)^2}{2} \label{eqn:adev_estimator}
\end{equation}
This esitmation can lead to artifacts in the results as discussed later. In order to derive the Allan variance from a set of data points, the different values of $\tau$ are usually obtained by averaging over a number of samples since there is no dead time.

Additionally, the Allan variance is mathematically related to the two-sided power spectral density $S_y(f)$ \cite{psd_to_adev}:
\begin{equation}
    \sigma_A^2(\tau) = 2 \int_0^\infty S_y(f) \frac{\sin^4\left( \pi f \tau \right)}{(\pi f \tau)^2}\,df \label{eqn:psd_to_adev}
\end{equation}

and therefore all processes, that can be observed in the power spectral density can also be seen in the allan deviation. The inverse transform, however, is not always possible as shown by \citeauthor{inverse_adev} \cite{inverse_adev}.

Distinguishing different noise processes using the Allan deviation will be elaborated in the next section.

\subsection{Identifying Noise in Allan Deviation Plots}
It was already mentioned by \citeauthor{adev} in \cite{adev}, that types of noise, whose spectral density follows a power law
\begin{equation}
    S(f) = h_{\alpha} \cdot f^\alpha \label{eqn:power_law}
\end{equation}
can be easily identified in the Allan deviation plot. The constant $h_\alpha$ is called the power (intensity) coefficient. The most common types of noise encountered in experimental data and their representations can be found in table \ref{tab:adev_alpha}, which serves as a summary of this section. Since those types of noise is present in any measurement or electronic device, it warants a further discussion to understand their root causes and ideas to minimize them. While not a type of noise, linear drift can also be easily identified in the Allan deviation plot. It is therefore included in table \ref{tab:adev_alpha} as well.

\begin{table}[ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Amplitude noise type& Power-law coefficient $\alpha$& Allan variance $\sigma_A^2$\\
        \midrule
            White noise & $0$& $\frac 1 2 h_0 \tau^{-1}$ \cite{adev_noise_types}\\
            Flicker noise& $-1$& $2 \ln 2 \, h_{-1} \tau^0$ \cite{adev_noise_types}\\
            Random walk noise& $-2$& $\frac 3 2 \pi^2 h_{-2} \tau^{1}$ \cite{adev_noise_types}\\
            Burst noise& $0 \textrm{ and } -\!2$& $y_{RMS}^2\frac{\bar \tau^2}{\tau^2} \left(4 e^{-\frac{\tau}{\bar \tau}} - e^{-\frac{2 \tau}{\bar \tau}} + 2 \frac{\tau}{\bar \tau} - 3 \right)$\\
            Drift & --& $\frac 1 2 D^2 \tau^2$ \cite{adev_drift}\\
        \bottomrule
    \end{tabular}
    \caption{Power law representations using the Allan variance.}
    \label{tab:adev_alpha}
\end{table}

In order to arrive at a good understanding of the features seen in an Allan deviation plot, this section will provide the reader with examples of each type of noise and the corresponding time domain, power spectral density and Allan deviation plot. Since a complete overview is not available in current literature, all required mathematical descriptions and simulation tools will be discussed here. The simulations were done using Python and the source code is linked to in the discussions.

\clearpage
\subsubsection{White Noise}
White noise is probably the most common type of noise found in measurement data. Johnson noise found in resistors, caused by the random fluctuation of the charge carriers, is one example of mostly white noise up to bandwidth of \qty{100}{\MHz}, from where on quantum corrections are required \cite{nist_johnson_noise}. Amplifiers also tend to have a white noise spectrum at higher frequencies.

For this reason, white noise typically makes up for a considerabe amount of noise in a measurement, unless one works at very low frequencies. White noise is a series of uncorrelated random events and therefore characterised by a uniform power spectral density, which means there is the same power in a given bandwidth at all frequencies up to infinity. White noise therefore has infinite power (variance). In reality a measurement is always limited in bandwidth and hence the above property of a constant power spectral density only holds within that bandwidth. Those bandlimited samples of white noise thus have a finite variance.
Since white noise is so common, a few properties should be mentioned. One such property is, that the variance $\sigma_{x+y}^2$ of two uncorrelated variables $x$ and $y$ adds as:
\begin{equation}
    \sigma_{x+y}^2  = \sigma_x^2 + \sigma_y^2 + \underbrace{2\,\mathrm{Cov}(x,y)}_{\text{uncorrelated}\, =\, 0}\ = \sigma_x^2 + \sigma_y^2 \label{eqn:adding_white_noise}
\end{equation}

This allows simple addition rules of variances from different sources, but it must be stressed here, that this property is only valid for uncorrelated sources like white noise, although it is usually incorrectly applied to all measurements in disreagard of the dominant noise present, which unfortunately obscures rather than clarifies the uncertainties involved.

In order to demonstrate the effect of white noise in Allan deviation plots, it was simulated using the excellent \textit{AllanTools} library \cite{allantools}. The noise generator chosen in the AllanTools library is based on the work of \citeauthor{noise_generation} \cite{noise_generation}. The full Python program code is published online \cite{}. For better comparison, all noise densities are normalized to give an Allan deviation of $\sigma_A(\tau_0)=1$, with $\tau_0$ being the smallest time interval between measurements.

Figure \ref{fig:white_noise_simulated} shows a sample of white noise in three different forms. Figure \ref{fig:white_noise_time} is the time series representation. From this sample, the power spectral density was calculated and is shown in figure \ref{fig:white_noise_psd}. The dashed line shows the expectation value of the power spectral density and the Allan deviation.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/white_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:white_noise_time}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/white_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:white_noise_psd}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/white_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:white_noise_adev}
    \end{subfigure}
    \caption{Different representations of white noise.}
    \label{fig:white_noise_simulated}
\end{figure}

From this simulation, several features can be observed. First of all, the power spectral density is flat and constant with $h_0 = 2$, which is in accordance with table \ref{tab:adev_alpha} and the normalization mentioned earlier. Figure \ref{fig:white_noise_adev} shows the typical $\tau^{-\frac 1 2}$ dependence of white noise in the Allan deviation plot. This immediately explains, why filtering white noise scales with $\frac{1}{\sqrt{n}}$ with $n$ being the number of samples averaged.

\clearpage
\subsubsection{Burst Noise}
\label{sec:theory_burst_noise}
Burst noise, popcorn noise, or sometimes referred to as random telegraph signal is a random bi-stable change in a signal and is caused by a generation recombination processes. This, for example, happens in semiconductors if there is a site, that can trap an electrons for a prologned period of time and then randomly release it. Imporities causing lattice defects are discussed in this context \cite{kay2012operational,burst_noise_psd,popcorn_noise_orgin,technote_ti_popcorn_noise}. Such latttice defects can also be introduced by ion implantation during doping. Fortunately, this type of noise has become less prevalent in modern manufacturing processes, because the quality of the semiconductors has improved. But if a trap site is located very close to an important structure, for example a high precision Zener diode, its effect might be so strong, that it can be clearly seen.

The discussion is split into two parts. First the power spectral density is calculated and then the Allan variance is caclulated using that result.

The spectral density of burst noise caused by a single trap site was derived in \cite{burst_noise_wiener_khinchin} by \citeauthor{burst_noise_wiener_khinchin}. The author used the autocorrelation function of the burst noise signal and applied the Wiener-Khinchin (Wiener-Хи́нчин) theorem, which connects the autocorrelation function with the power spectral density. A more detailed derivation can be found in \cite{fundamentals_of_noise_processes}, in this paper the preconditions, like stationarity of the process, are also discussed. The burst noise signal consists of two energie levels, called $0$ and $1$, split by $\Delta y$. Multiple burst noise signals can be superimposed in a real device. This would then result in mutiple levels, but they can be treated separately. The measurement interval over an even number of transitions, so that one ends in the same state as the measurement has started, is the time $T$. The mean lifetime of the levels is called $\bar \tau_0$ and $\bar \tau_1$:
\begin{equation}
    \bar \tau_{0} \approx \frac 1 N \sum_{i}^N \tau_{0,i} \qquad \bar \tau_{1} \approx \frac 1 N \sum_{i}^N \tau_{1,i}
\end{equation}

Figure \ref{fig:burst_noise} shows a burst noise signal along with the definitions above.

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{burst_noise.tex}
    } % scalebox
    \caption{A random burst noise signal.}
    \label{fig:burst_noise}
\end{figure}

Using these definitions, one can then derive \cite{burst_noise_wiener_khinchin}:
\begin{align}
    R_{xx} (T) &= \Delta y^2 \cdot \frac{\bar \tau_1 \bar \tau_0 e^{-\left(\frac{1}{\bar \tau_1}+\frac{1}{\bar \tau_0}\right)T}}{\left(\bar \tau_1 + \bar \tau_0\right)^2} \quad \text{and} \label{eqn:burst_noise_correlation}\\
    S(\omega) &= 4 R_{xx}(0) \frac{\frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0}}{\left(\frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0}\right)^2 + \omega^2} \qquad \omega > 0 . \label{eqn:burst_noise_psd}
\end{align}
Note, that the power spectral density is the one-sided version, hence an additional factor of $2$ is included. The d.c. term was ommitted here and can usually be neglected, because it is not relevant for calculating the power spectral density as it only contributes a single peak at $\omega=0$. Using the following definitions of the average time constant and the duty cycle

\begin{align}
    \frac{1}{\bar \tau} &= \frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0} \quad \mathrm{and} \label{eqn:definition_bar_tau}\\
    D_i &= \frac{\bar \tau_i}{\bar \tau_1 + \bar \tau_0} \quad i \in \{0 ; 1\}
\end{align}

equations \ref{eqn:burst_noise_correlation} and \ref{eqn:burst_noise_psd} can be rewritten to give a more intuitive form:

\begin{align}
    R_{xx} (T) &= \Delta y^2 D_1 D_0 \, e^{-\left(\frac{1}{\bar \tau_1}+\frac{1}{\bar \tau_0}\right)T}\\
    S(\omega) &= 4 R_{xx}(0) \frac{\bar \tau}{1 + \omega^2 \bar \tau^2} \label{eqn:burst_noise_lorentzian}
\end{align}

The special case $\bar \tau_0 = \bar \tau_1$ with $D_i=\frac 1 2$ is the previously mentioned case of random telegraph noise.

$R_{xx} (0)$ can be identified as the mean squared value of $y$:
\begin{equation}
    y_{RMS} = \sqrt{R_{xx}(0)} \,.
\end{equation}

Equation \ref{eqn:burst_noise_lorentzian} is a Lorentzian function and from this it can be easily seen, that a single trap site has a power spectral density, which is proportional to $\frac{1}{f^2}$ at high frequencies and is flat at low frequencies.

With the spectral density in hand, it is now possible to calculate the Allan variance as it was done by \citeauthor{allen_dev_flicker} in \cite{allen_dev_flicker} for the classic example of random telegraph noise where $\bar \tau_1 = \bar \tau_0$. Do note, that table I given by \citeauthor{allen_dev_flicker} shows the total number of events instead of the instantationous number of events typically given. Hence, their notation must be multiplied by $\frac{1}{\tau^2}$ (or $\frac{1}{T^2}$ in their notation). For the generic case with $\bar \tau_1$, $\bar \tau_0$ and the definition of $\bar \tau$ given in equation \ref{eqn:definition_bar_tau} one finds for the Allan variance of burst noise:
\begin{equation}
    \sigma^2_A(\tau) = R_{xx}(0) \frac{\bar \tau^2}{\tau^2} \left(4 e^{-\frac{\tau}{\bar \tau}} - e^{-\frac{2 \tau}{\bar \tau}} + 2 \frac{\tau}{\bar \tau} - 3 \right) \label{eqn:burst_noise_avar}
\end{equation}

Having arrived at equations \ref{eqn:burst_noise_lorentzian} and \ref{eqn:burst_noise_avar} of the power spectral density and Allan variance, it it now possible to model it. For this purpose, parts of the Python library \textit{qtt} \cite{qtt} was used. The algorithm written by \citeauthor{qtt} implements continous-time Markov chains to simulate the burst noise signal. The result can be see in figure \ref{fig:burst_noise_simulated}. For these simulations one time constant, namely the lifetime of the lower state $\bar \tau_0$ was held constant, while the lifetime of the upper state was varied to show the effect of different $\bar \tau$. By looking at the time domain in figure \ref{fig:burst_noise_time} it can be seen, that the maximum average number of state changes can be observed, when $\bar \tau_1 = \bar \tau_0$. If $\bar \tau_1 > \bar \tau_0$ the system will favour the upper, while if $\bar \tau_1 < \bar \tau_0$ it will favour the lower state instead. This explaines why the noise is strongest for random telegraph noise when $\bar \tau_1 = \bar \tau_0$, which can also be seen in power spectral density in figure \ref{fig:burst_noise_psd}. Looking at the Allan deviation in figure \ref{fig:burst_noise_adev} confirms this, but also shows another interesting implication as it shows an obvious maximum. If the application allows a choice over the sampling interval $\tau$, the effect of the burst noise can mitigated by staying well clear of the maximum.

The small deviation from the analytical solution in figure \ref{fig:burst_noise_adev}  at large $\tau$ is a typical so called end-of-data error. As it was discussed above, the Allan deviation can only be estimated given a limited number of samples using equation \ref{eqn:adev_estimator} and going to longer $\tau$ means there are fewer samples to average over.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.8\linewidth}
        \centering
        \scalebox{1}{%
            \input{images/burst_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:burst_noise_time}
    \end{subfigure}
    \begin{subfigure}{0.8\linewidth}
        \centering
        \scalebox{1}{%
            \input{images/burst_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:burst_noise_psd}
    \end{subfigure}
    \begin{subfigure}{0.8\linewidth}
        \centering
        \scalebox{1}{%
            \input{images/burst_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:burst_noise_adev}
    \end{subfigure}
    \caption{Different representations of burst noise for different $\bar \tau_1$ and fixed $\bar \tau_0 = \qty{1}{\s}$.}
    \label{fig:burst_noise_simulated}
\end{figure}

The burst noise equations can used to gain further insight into other types of noise. The first one is Shot noise, which is commonly found in photodetectors and lasers. Here, electrons or photons are created at discrete intervals resulting in an instantationous signal. This means, that the lifetime of the upper level is very short in comparison to the lower level ($\tau_1 \ll \tau_0$) equation \ref{eqn:burst_noise_psd} becomes:
\begin{align}
    S_{Shot}(\omega) = S_{\tau_1 \ll \tau_0}(\omega) &= 4 \Delta y^2 \frac{\tau_1}{\tau_0} \frac{\frac{1}{\bar \tau_1}}{\left(\frac{1}{\bar \tau_1}\right)^2 + \omega^2}\nonumber\\
    &= 4 \Delta y^2 \frac{1}{\tau_0} \frac{1}{\frac{1}{\tau_1^2}+\omega^2}\\
    \overset{\omega \ll 1/\tau_0}&{\approx} 4 \Delta y^2 \frac{\tau_1^2}{\tau_0} = \text{const.}
\end{align}

For the typical case, a very large number of such events happen. When not counting single events, but rather a stream, the relation $\omega \ll 1/\tau_0$ is valid and hence the results is a white spectrum as $S_{Shot}(\omega)$ is constant with respect to $\omega$ --- just as observed in photodetectors and lasers.

The other interesting case is a case, where many trap sites with different time constants are contributing to the noise. This can change the shape of the spectrum from $f^{-2}$ to $f^{-1}$ and is discussed in the next section.

\clearpage
\subsubsection{Flicker Noise}
\label{sec:flicker_noise}
Flicker noise is also called $\frac 1 f$-noise and it can be observed in many naturally occuring phenomenen. Its origin is not clear, although there have been many explanations. An overview can be found in \cite{flicker_noise_overview, flicker_noise_overview2, origins_1_f_noise}. This work concentrates on flicker noise in electronic devices. In thick-film resistors, for example, it was shown to extend over at least 6 decades without any visible flattening \cite{1_f_noise_thick_film}. In transistors, flicker noise is caused by the existance of generation-recombination noise or burst noise discussed in the previous section \cite{origins_1_f_noise}. If there are many uncorrelated trap sites, that contribute to the total noise, the envelope of the noise spectral density changes from $\frac{1}{f^2}$ to $\frac{1}{f^1}$ as shown in figure \ref{fig:flicker_noise_evelope}

\begin{figure}[hb]
    \centering
    \input{images/flicker_noise_envelope.pgf}
    \caption{Multiple overlapping Lorentzian noise sources forming a $\frac 1 f$-like shape.}
    \label{fig:flicker_noise_evelope}
\end{figure}

Given that no trap site can store an electron indefinetely, the number of trap sites $N$ with a certain time constant $\frac 1 2 \bar \tau = \bar \tau_0 = \bar \tau_1$ must decline for longer time scales. Assuming $N$ is inversely proportional to the time constant $\bar \tau$
\begin{equation}
    N(\tau) \propto \frac{1}{\bar \tau}\,, \label{eqn:flicker_noise_weight_function}
\end{equation}

which can be motivated if the trapping process is thermally activated \cite{1_f_noise_motivation} and using equation \ref{eqn:burst_noise_lorentzian} from the previous section, multiplying the weight function \ref{eqn:flicker_noise_weight_function} and integrating over all possible storage times gives:

\begin{align}
    S(\omega) &= \lim_{t \to \infty} \int_0^t N(\bar \tau) \, 4 R_{xx}(0) \frac{\bar \tau}{1 + \omega^2 \bar \tau^2} \, d\bar\tau \nonumber\\
    \overset{\bar \tau_0 = \bar \tau_1}&{=} 4 R_{xx}(0)\, C_N \lim_{t \to \infty} \int_0^t \frac{1}{1 + \omega^2 \bar\tau^2} \, d\bar\tau \nonumber\\
    &= \frac{4 R_{xx}(0)\, C_N}{\omega} \lim_{t \to \infty}  \arctan{\bar\tau \omega} \Big|_{\bar\tau=0}^t \nonumber\\
    &= \frac{4 R_{xx}(0)\, C_N}{\omega} \cdot \frac{\pi}{2} \nonumber\\
    &= \frac{2 \pi R_{xx}(0)\, C_N}{\omega}\\
    S(f) &= h_{-1} f^{-1}
\end{align}

$C_N$ is the proportionality constant of \ref{eqn:flicker_noise_weight_function} and $h_{-1}$ is the power coefficient introduced in \ref{eqn:power_law}. This shows, that for a large number of distributed trap sites, a noise spectrum of $f^{-1}$ is found.

Using equation \ref{eqn:psd_to_adev}, the Allan variance can be calculated from the power spectral density:
\begin{align}
    \sigma_A^2(\tau) &= 2 h_{-1} \int_0^\infty \frac{1}{f} \frac{\sin^4\left( \pi f \tau \right)}{(\pi f \tau)^2}\,df \nonumber\\
    &=2 \ln 2 \, h_{-1}
\end{align}

Again, using the \textit{AllanTools} library \cite{allantools}, flicker noise was simulated to give an impresion of its properties.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/flicker_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:flicker_noise_time}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/flicker_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:flicker_noise_psd}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/flicker_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:flicker_noise_adev}
    \end{subfigure}
    \caption{Different representations of flicker noise.}
    \label{fig:flicker_noise_simulated}
\end{figure}

While it is not immediately evident from the power spectral density, the Allan deviation plot explains very well, why additional filtering does not affect flicker noise. No matter how long the integration time, the variance will the same.

The small wiggles at longer $\tau$ are typical end-of-data errors caused by spectral leakage, because there are insufficient samples to average over \cite{adev_long_tau}. As it was discussed above, the Allan deviation can only be estimated using equation \ref{eqn:adev_estimator} given a limited number of samples. Therefore, at $\frac{\tau}{2}$ there are only $2$ samples left, so there is no averaging possible to improve the estimate of the Allan deviation, which causes the oscillations at low frequencies or large $\tau$.

As a last remark, a commonly used definition in combination with flicker noise is the corner frequency $f_c$. The corner frequency appears in situations, where there is both flicker and white noise present. It is the crossover point in frequency, where the flicker noise is equal compared to the white noise.
\begin{equation}
    f_c = \frac{h_{-1}}{h_0} \label{eqn:corner_frequency}
\end{equation}
It can be graphically extracted from the power spectral density plot by drawing a line trough the flicker noise and the white noise and finding the intersection. This can be seen in figure \ref{fig:adev_example_psd} on page \pageref{fig:adev_example_psd}. The corner frequency can be found where the horizontal dashed blue and green line meet.

\clearpage
\subsubsection{Random Walk}
Random walk noise can be attributed to environmental factors such as temperature \cite{random_walk_fm} and diffusion processes, the latter contributing to the ageing effect seen in semiconductors.
It is a process, where in each time step the change is randomly determined to be either a positve or negative step with equal probability and a fixed step size. Its mean is
\begin{equation}
    \langle y_n \rangle = \langle e_1 + e_2 + \dots e_n \rangle = \underbrace{\langle e_1 \rangle}_{=\,0} + \langle e_2 \rangle + \dots + \langle e_n \rangle = 0 \, ,
\end{equation}
but its variance
\begin{equation}
    \sigma_y^2 = \langle y_n^2 \rangle - \underbrace{\langle y_n \rangle}_{=\,0} = \sigma_{e_1}^2 + \sigma_{e_2}^2 + \dots \sigma_{e_n}^2 = n \sigma_e^2
\end{equation}
goes with $n$ (or $t$). It therefore not a stationary process as can also be seen in figure \ref{fig:random_walk_adev}.

The power spectral density can be calculated \cite{psd_to_adev,noise_generation} to
\begin{equation}
    S(f) = h_{-2} \frac{1}{f^2}
\end{equation}
and the Allan deviation can again be calculated from the spectral density
\begin{align}
    \sigma_A^2(\tau) &= 2 h_{-2} \int_0^\infty \frac{1}{f^2} \frac{\sin^4\left( \pi f \tau \right)}{(\pi f \tau)^2}\,df \nonumber\\
    &=\frac{2}{3} \pi^2 h_{-2}\, \tau
\end{align}

The \textit{AllanTools} library \cite{allantools} can then be used to simulate the random walk.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/random_walk_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:random_walk_time}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/random_walk_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:random_walk_psd}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/random_walk_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:random_walk_adev}
    \end{subfigure}
    \caption{Different representations of random walk noise.}
    \label{fig:random_walk_noise_simulated}
\end{figure}


\clearpage
\subsubsection{Drift}
Finally, the last feature of the Allan deviation plot, that needs to be discussed is drift. Drift happens at very long time scales and descriped a linear dependence of measurand on the time. This is also part of the ageing effect. \citeauthor{adev_drift} discussed the effect of drift \cite{adev_drift} on the Allan variance and found the following relationship:
\begin{align}
    \sigma_A^2(\tau) = \frac{D^2}{2} \tau^2
\end{align}
with slope of the drift $D$.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/drift_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:drift_time}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/drift_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:drift_adev}
    \end{subfigure}
    \caption{Different representations of linear drift.}
    \label{fig:drift_noise_simulated}
\end{figure}

\clearpage
\subsubsection{Dead Time}
\label{sec:dead_time}
The coefficients given here were derived using the assumption, that all sampling in a measurement are continous and the dead time $\theta = 0$. Unfortunately, measurement sometime have a dead time, that is non negligible. This problem was extensively discussed by \citeauthor{psd_to_adev} \cite{psd_to_adev}. \citeauthor{adev_frequency_counter} even developed special models to account for the algorithms of modern frequency counters \cite{adev_frequency_counter}. While some frequency counters support gapless measurements, the situation is entirely different for digitizers and digital multimeters. Several settings commonly used affect the dead time, which can be considerable. It is therefore important to discuss typical measurement settings for voltmeters to estimate the errors that arise from those settings. The focus of this discussion lies on the dead time introduced by digital multimeters, but the application is not limited to this field.

The most commonly used settings, that affect the dead time of a voltmeter are auto-zeroing and line synchronization. Auto-zeroing is done by adding additional measurements to the normal input integration cycle. To correct for the zero offset drift a zero measurement is added where the adc is switched to the low terminal. Additionally, some devices add a reading of the reference voltage to correct for gain errors. The implementation details and type of measurements are manufacturer dependent and must be determinded for every multimeter used.

The other setting, that can be enabled in voltmeters, is the line synchronization to increase the noise rejection of the instrument. This setting synchronizes the start of a measurement to the zero crossing of the power line. Depending on the instrument, this might cause a delay of one power line cycle (PLC) after each measurment if the instrument is not capable of processing the previous measurement while at the same time recording another one.

A simple measurement with dead time is shown in figure \ref{fig:allan_variance_definitions} on page \pageref{fig:allan_variance_definitions}. The model assumes, that the dead time is constant and is always added after the actual integration time $\tau$. This is rarely true for real measurement data as many devices and even ADCs use internal averaging and auto-zeroing to produce a measurement. The actual dead time is therefore spread over the whole measurement and not limited to the end of the measurement. An example is the Keysight \device{3458A} DMM, which automatically switches to averaging when selecting integration times greater than \qty{10}{\plc}. The reason is simple, for longer integration times, more and more flicker noise starts contributing to the measurement. The measurement is therefore split into single measurements of \qty{10}{\plc} and using auto-zeroing the flicker noise is suppressed. This is discussed in more detail as an example in section \ref{sec:autozero}. The mathematical problem of a distributed dead time was already noted by \citeauthor{adev_noise_types} \cite{adev_noise_types} and it is distinctively different from the calculations made by \citeauthor{psd_to_adev} for a single dead time at the end of the measurement. The exact mathematical treatment is complex and is beyond the scope of this work, especially considering, that auto-zeroing does a lot more than just adding dead time at the end of the measurement. Fortunately using a few assumptions the problem can be greatly simplified.

An interesting observation can be made for white noise. Since it is uncorrelated, it makes no difference whether it is sampled in full, or only partially, therefore the Allan deviation for a white noise process with or without dead time is the same:
\begin{equation}
    \sigma^2(N,T, \tau) = \sigma^2(N=2,T=\tau, \tau) = \sigma_A^2(\tau) \frac 1 2 h_0 \tau^{-1}
\end{equation}

Consequently, if the dead time is added at a frequency high enough, so that the input amplifier output is dominated by white noise, the dead time will have no influence on the Allan variance.

Finally, \citeauthor{psd_to_adev} \cite{psd_to_adev} notes that for measurement durations or averaging times $T \gg T_0$, the Allan variance with respect to $T$ shows an asymptotic behaviour of $\sigma_A^2(T) \to \sigma_A^2(\tau)$.

\clearpage
\subsection{Example}
\label{sec:noise_example}
Using the results from the previous sections, it is possible to simulate a typical measurement sample containing white noise, flicker noise and random walk behaviour. The simulation was written in Python using the \textit{AllanTools} library \cite{allantools} to generate the time domain data, which was then converted to a power spectrum using the algorithm of \citeauthor{welch} \cite{welch}. The Allan deviation was calculated using the \textit{AllanTools}. The full Python source is available at \cite{}. The time domain data shown here was downsampled from $2^{25}$ data points to \num{2000} points for faster plotting, using the Largest-Triangle-Three-Buckets (LTTB) algorithm created by \citeauthor{lttb} \cite{lttb}. The downsampling algorithm chosen is optimal for this application, because it aims to visually keep the result the same by favouring parts of the data, where there is more change. The only difference noticable to the author is, that the edges of the white noise plot are a little rougher. The full data set can be obtained using the source code given above if one desires. The power spectrum and the Allan deviation were always calculated from the full dataset. The data of the power spectrum was additionally binned to be evenly spaced on a logarithmic scale. This considerably reduced the high frequency noise and made the plot easier while not negatively impacting the shape.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/example_white_time.pgf}
        } % scalebox
        \caption{White noise}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/example_flicker_time.pgf}
        } % scalebox
        \caption{Flicker noise}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/example_rw_time.pgf}
        } % scalebox
        \caption{Random walk}
    \end{subfigure}
    \caption{Three sperate noise components, that were summed together to simulate a typical noise source.}
    \label{fig:adev_example_noise_types}
\end{figure}

The three time series shown in figure \ref{fig:adev_example_noise_types} were sequentially generated using a fixed seed for the random number generator to ensure repeatability as long as the order of creation is kept the same. For generting the noise, the algorithm presented by \citeauthor{noise_generation} \cite{noise_generation} and implemented in the \textit{AllanTools} library was used. The noise strength parameters were deliberately chosen in such a way, that both the white noise and the random walk part have more noise power than the flicker noise. This allows to distinguish them in the plots at both extremes of the frequency scale. Finally, the three types of noise data were summed together to give the combined signal. The time series is shown in figure \ref{fig:adev_example_time}, again downsampled using LTTB. The summed series clearly shows the white noise content and it is possible to deduce some flicker or random walk noise, but it is highly obscured due to the amount of white noise. Using only the time domain plot makes it very hard to distinguish the type of noise present, let alone estimate the individual noise power of the three sources. Therefore, a different analysis tool is called for.

\begin{figure}[ht]
    \centering
    \input{images/example_time.pgf}
    \caption{A simulated time series containing white noise, flicker noise and random walk behaviour.}
    \label{fig:adev_example_time}
\end{figure}

A common approach to identify noise sources is the power spectrum. It is easily accessible, even in real-time, using spectrum analyzers and, utilizing the computational power of modern computers, large time-domain data sets can be converted making this the method of choice in the lab. The power spectrum of figure \ref{fig:adev_example_time} is shown in figure \ref{fig:adev_example_psd}. It allows to clearly separate the white noise part from the other $f^{\alpha}$ components. The dashed lines representing the individual components were plotted using the $h_\alpha$ values calculated from the input parameters of the simulation. The noise spectral density $h_0$ of the white noise signal can be easily extracted even by hand without resorting to a fit. This yields $h_{0} = \qty{2e-3}{\per \Hz} $. $h_{-1}$ and $h_{-2}$ can be exctracted as well using a fit to
\begin{equation}
    S(f) = \sum_{\alpha = -2}^0 h_\alpha f^\alpha \, .
\end{equation}
The noise corner frequency $f_c$ can either be calculated from $h_0$ and $h_{-1}$ using equation \ref{eqn:corner_frequency} or determined graphically by contructing a tangent with a slope of $-1$ to the spectral density. From the intersection of the blue $h_0$ line and the green $h_{-1}$ line the corner frequncy is found to be $f_c \approx \qty{1.8}{\kHz}$.

\begin{figure}[hb]
    \centering
    \input{images/example_psd.pgf}
    \caption{A simulated power spectrum containing white noise, flicker noise and random walk behaviour.}
    \label{fig:adev_example_psd}
\end{figure}

To get an even better representation of the individual noise contributions, the Allan variance or Allan deviation can be used. The Allan deviation plot shown in figure \ref{fig:adev_example_adev} gives very clean results and all noise components can be clearly identified. The individual components were plotted using dashed lines as well.

\begin{figure}[hb]
    \centering
    \input{images/example_adev.pgf}
    \caption{A simulated Allan deviation containing white noise, flicker noise and random walk behaviour.}
    \label{fig:adev_example_adev}
\end{figure}

The Allan variance was calculated using the overlapping Allan variance algorithm \cite{oadev_definition} and only Allan deviation values for frequency vales of $(1, 2, 4)$ per decade were plotted. The overlapping Allan variance gives a better confidence at longer intervals or lower frequencies, allowing to identify very low frequency noise like the random walk shown here. Reference \cite{oadev_definition} also gives a very good comparison of other algorithms to identify even more noise types in data sets, like phase noise. Plotting only three values per decade improves the clarity of the plot, because at longer taus, even though the overlapping Allan variance is used, some oscillations inevitably show up. Using fewer values of $\tau$ causes less distractions in this case.
From the figure \ref{fig:adev_example_adev}, the Allan deviation of the flicker noise can be estimated from the flat minimum to be around $2.3$ or $\sqrt{5}$. Using table \ref{tab:adev_alpha} the Allan variance can be converted to
\begin{equation}
    h_{-1} = \frac{5}{2 \ln 2} \approx 3.6 \nonumber
\end{equation}

Using the previously found $h_0$, this corner frequency is calculated using equation \ref{eqn:corner_frequency} to be:
\begin{equation}
    f_c = \frac{5}{\qty{2e-3}{\per \Hz} \cdot 2 \ln 2} \approx \qty{1.8}{\kHz} \nonumber
\end{equation}

This is obviously the same result as the one from the geometric approach above.

\clearpage
\section{Autozeroing}
\label{sec:autozero}
Autozeroing (AZ), sometimes called zero-drift or dynamic offset compensation, is such an important concept, that it must be discussed in its own right. The need for autozeroing comes from the typical behaviour of amplifiers. Every amplifier has some offset, be it small or large, and especially at high gains, this offset becomes a problem for high precision measurements. To make matters worse, this offset is not stable over time and drifts with both time and temperature. It can therefore not be calibrated out once, it must be permanently adjusted during operation, depending on environmental conditions. This procedure is called autozeroing.

There are many different ways to implement autozeroing and regarding operational amplifiers a good overview can be found in \cite{horowitz1989}. As an example, the autozero cycle for the Keithley \device{Model 2002} and the Keysight \device{3458A} Mulimeter is shown in figure \ref{fig:dmm_autozero_comparison}. Keithley uses a more complex and slower algorithm, while HP implemented a simpler, but faster algorithm. The most simple (digital) approach is to regularly switch the input from the signal to zero, take a reading, then subtract this reading from all subsequent readings until a new zero reading is taken. The other second approach adds another measurement of the reference voltage to apply a gain correction. This is done by the Keithley \device{Model 2002} and works very well to suppress gain drift in the input amplifier due to temperature changes, but increases the time between samples by another \qty{50}{\percent}. This is the reason, why on the \device{Model 2002}, there is another mode to strech the time between autozero cycles, but the price to pay is a non-uniform sampling rate. This makes post-processing a lot more complicated as most alorithms assume a (near) constant sampling rate. The Keysight \device{3458A} calculates gain corrections only during the manual autocalibration routine to maintain a higher throughput.

\begin{figure}[hb]
    \centering
    %\resizebox {0.8\textwidth} {!} {
        \import{figures/}{dmm_autozero.tex}
    %} % resizebox
    \caption{Auto-zero phases of the Keysight \device{3458A} and Keithley \device{Model 2002}.}
    \label{fig:dmm_autozero_comparison}
\end{figure}

\subsection{Offset-Nulling}
Offset-nulling is the most basic approach to autozeroing. It aims to remove the offset drift of an amplifier. Especially at high gains, the offset, which is multiplied by the gain, can be substantial. In order to explain how offset-nulling works and how it shapes the spectrum, it is best to discuss it based on an example. While this technique can also be found in many integrated circuits, it is more noticable in DMMs, because is a switchable option. Therefore, the example data set simulated is based on the parameters of the aforementioned Keysight \device{3458A} multimeter. The corner frequency and the white noise floor is modeled after the \qty{10}{\V} range of the \device{3458A} \cite{3458A_noise_floor, sampling_with_3458A} with the values given below. Do note that both references \cite{3458A_noise_floor, sampling_with_3458A} contain a typographical error. The corner frequency of the noise floor is erroneously given as \qty{0.5}{\Hz}, but should be \qty{1.5}{\Hz}. This can be seen in figure 2.35 in \cite{sampling_with_3458A}, where the noise spectral density is plotted and it was also confirmed with the author \cite{lapuh_email_corner_frequency}. The sample is generated using the Python \textit{AllanTools} library \cite{allantools}.

\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{offset_nulling_definitions.tex}
    } % scalebox
    \caption{Integration sequences of the offset-nulling algorithm. Solid lines denote sampled data. Red is the input signal, green is the zero reading and blue is the dead time required for switching inputs.}
    \label{fig:dmm_autozer_offset_nulling}
\end{figure}

For this simulation, a noise-free and arbitrarily chosen \qty{10}{\V} input is assumed to be sampled by the device at a sampling rate of \qty{10}{\plc} at \qty{50}{\Hz}, the same rate discussed previously on page \pageref{sec:dead_time}. As it will be shown, the actual mean value of the input signal has no bearing on the outcome of the calculation when considering offset-nulling, but its value must be considered for other types of autozeroing as discussed in section \ref{sec:autozero_gain} and is included here only for the sake of completeness.

Figure \ref{fig:dmm_autozer_offset_nulling} shows the individual sequences of the offset-nulling algorithm. First, the source is sampled for $\tau_s = \qty{10}{\plc}$, then the input is switched to the LO terminal. While this operation is very fast and takes less than \qty{1}{\ms} \cite{article_3458A_input_mpedance}, if the instrument is synchronized to the line frequency the zero measurement will nontheless be delayed until the next zero crossing, hence the dead-time $\theta = \qty{1}{\plc}$. Finally, the zero reference is measured for another $\tau_r = \qty{10}{\plc}$ and then the instrument switches back to the HI terminal.

The data is simulated in the following way. First, two sets of noise data are generated, a white noise spectrum with a noise spectral density of \qty[power-half-as-sqrt, per-mode=symbol]{165}{\nV \Hz\tothe{-0.5}} and a flicker noise spectrum with an intesity scaled to result in a final spectrum with a corner frequency of \qty{1.5}{\Hz}. The required flicker noise intensity is calculated using equation \ref{eqn:corner_frequency}. To get a good low frequency estimate, $2^{20} \approx 10^{6}$ values were generated. Finally, the two noise data sets are summed with the noise-free input source to give the final result. Other effects, such as power-line hum are neglected in this simple simulation, because it would needlessly overcomplicate the example and limit the educational value. The same goes for higher order random-walk $f^{-2}$ noise components, which can be introduced by temperature fluctions and other environmental effects and would be present in a real measurement.

\begin{figure}[ht]
    \centering
    \input{images/autozero_raw_time.pgf}
    \caption{Time series data with white noise and flicker noise.}
    \label{fig:autozero_raw_time}
\end{figure}

The time domain plot of the simulation is shown in figure \ref{fig:autozero_raw_time}. The white noise component is clearly visible, while the $f^{-1}$ flicker noise can be recognized, but its strength can hardly be estimated. It was already shown in section \ref{sec:noise_example}, different types of noise have different frequency components and can be be distinguished in the frequency domain, which leads to the next approach.

The noise power spectral density shown in figure \ref{fig:autozero_raw_psd} is calculated from the time series and confirms the flicker and white noise content. The theoretical white noise floor is shown as a horizontal dashed blue line and the flicker noise as a dashed green line. The \qty{1.5}{\Hz} corner frequency, which is defined as the intersection between the $f^{-1}$ noise and the white noise floor easily identified using the those lines. It is evident, that the \qty{5}{\Hz} sampling frequency with a \qty{2.5}{\Hz} bandwidth does not allow the spectral density to fully settle to the noise floor.

\begin{figure}[hb]
    \centering
    \input{images/autozero_raw_psd.pgf}
    \caption{Simulated power spectrum of a Keysight \device{3458A} containing white noise and flicker noise.}
    \label{fig:autozero_raw_psd}
\end{figure}

From the power spectral density is can be seeen, that higher frequencies have a significantly lower noise spectral density than the low frequencies. It is therefore most beneficial, to so measurements a higher frequencies. To discuss the optimal measurement interval, the Allan deviation is an excellent tool.

\begin{figure}[ht]
    \centering
    \input{images/autozero_raw_adev.pgf}
    \caption{Simulated Allan deviation of the input amplifier of a Keysight \device{3458A} containing white noise and flicker noise.}
    \label{fig:autozero_raw_adev}
\end{figure}

The Allan deviation it plotted in figure \ref{fig:autozero_raw_adev} and shows two distinct regions. Short $\tau$ display an asymptotic behaviour towards white noise with a $\tau^{−0.5}$ dependence and at longer $\tau$ the constant flicker noise region can be identified. At very long $\tau$ typical end-of-data oscillations can be seen, which are the result of the limited confidence of the Allan deviation estimator as previously discussed and can be safely ignored. The Allan deviation clearly demonstrates the performance of the device at longer integration times and it is obvious, that an beyond integration time of about \qty{1}{\second} or \qty{50}{\plc} no additional information can be extracted from the measurement and the variance is constant. This leads to the need to autozeroing to remove the flicker noise. It can be shown \cite{autozero_with_dead_time}, that subtracting a reference measurement from the actual measurement data removes all correlated effects. Since flicker noise is autocorrelated, it can be removed by subtracting a zero measurement.

To demonstrate autozeroing, two cases will be discussed. Going back to figure \ref{fig:dmm_autozer_offset_nulling} it can be seen, that between switching inputs, a dead time $\theta$ is added. For a first discussion, this dead time neglected and then the effect of adding a dead time is discussed.

Using figure \ref{fig:autozero_raw_adev} it was shown, that integrating over flicker noise, does not reduce the variance. In order to have as little flicker noise content in the final measurement value it is clear that the autozeroing should be done as fast possible to keep the flicker noise content out. This allows to calculate the expected variance of the autozeroed measurement. The noise of the input measurement $x$ and the reference measurement $y$ are the same, because in this model the only noise source comes from the input amplifier, as the input signal is assumed to be noise-free. The zero level is, by definition, noise-free. As dicussed above, the autozero interval is chosen, so that its variance is dominated by white noise The variance $\sigma^2$ of the combined measurement of $x-y$ can then be calculated using equation \ref{eqn:adding_white_noise}:
\begin{equation}
    \sigma_{x-y}^2 = \sigma_x^2 + \sigma_y^2 \label{eqn:autozeroing}
\end{equation}

By subtracting the zero reading, the amplifier noise is effectively added twice to the final result, once for the input measurement and once for the zero measurement. Additional noise from the input signal noise would simply be added to this as it is uncorrelated as well.

Do note, that the number of samples is now half the number before applying autozeroing. This leads to an interesting effect. Imagine a data set containing only white noise with a variance $\sigma^2$. Removing half the samples, obviously does not change the variance as white noise is not correlated, but subtracting the samples is effectively decimating the data set and since the sampling rate is halfed, the Nyquist band is halfed as well. Unfortunately the input noise bandwdith stays the same. The second Nyquist band is then folded back into the first, thus doubling the noise power density.

To conclude, it is expected, that the variance doubles and the power spectral density quadruples!

These consideration can be compared to the simulated data. Applying the autozeroing algorithm to the simulated data set, the constant \qty{10}{\V} input signal was nulled for every odd value and then the residual noise was subtracted from the signal value. The result in the time domain is shown in figure \ref{fig:autozero_time}.

\begin{figure}[hb]
    \centering
    \input{images/autozero_time.pgf}
    \caption{Simulated measurement with auto-zeroing applied.}
    \label{fig:autozero_time}
\end{figure}

When comparing to figure \ref{fig:autozero_raw_time} it is immediately evident, that the $f^{-1}$ component is no longer present. The difference in white noise strength is difficult to compare and it must be turned gain to the power spectral density. When calculating the spectral density it is important to remember, that the sampling rate is now halved. The result is shown in figure \ref{fig:autozero_psd} along with dashed lines showing the noise content prior to applying the autozero algorithm as before in figure \ref{fig:autozero_raw_psd}.

\begin{figure}[ht]
    \centering
    \input{images/autozero_psd.pgf}
    \caption{Simulated power spectrum of a Keysight \device{3458A} with autozeroing applied. The dashed lines denote the noise present prior to applying the autozero algorithm.}
    \label{fig:autozero_psd}
\end{figure}

The power spectral density in figure \ref{fig:autozero_psd} confirms an increase in the white noise power as discussed above and it can be determined that the white noise power $\sqrt{h_{-1}}$ has increased from \qty[power-half-as-sqrt, per-mode=symbol]{165}{\nV \Hz\tothe{-0.5}} to \qty[power-half-as-sqrt, per-mode=symbol]{489}{\nV \Hz\tothe{-0.5}}, an increase by a factor of $\sqrt{8.8}$, which is more than estimated by \ref{eqn:autozeroing}, including the factor of $2$ for the decimation, which gauged the increase of $\sqrt{h_{-1}}$ to be $\sqrt{4}$ . The reason for the additional noise was already mentioned above. There is still some substantial $f^{-1}$ noise present at the autozero frequency of \qty{5}{\Hz}. This type of noise is not uncorrelated and therefore the covariance is not zero, hence equation \ref{eqn:adding_white_noise} does not strictly hold. The hypothesis can be confirmed, by moving the corner frequency in the simulation from \qty{1.5}{\Hz} a decade lower to \qty{0.15}{\Hz}. The white noise floor would then only increase by a factor of $\sqrt{4.5}$.

Nonetheless, down to very low frequencies the $f^{-1}$ noise is effectively suppressed and the spectral density is almost perfectly flat.

The Allan deviation plot in figure \ref{fig:autozero_adev} also confirms that white noise is the only component and shows a $\tau^{-\frac 1 2}$ dependence for the full range of integration times.

\begin{figure}[hb]
    \centering
    \input{images/autozero_adev.pgf}
    \caption{Simulated Allan deviation of a Keysight \device{3458A} with autozeroing applied. The dashed lines denote the deviation prior to applying the autozero algorithm.}
    \label{fig:autozero_adev}
\end{figure}

From this plot it can be seen, that for measurement times longer than about \qty{2}{\s} or \qty{100}{\plc} autozeroing has a clear benefit over a measurement without autozeroing. It must be noted though, that judging from this simulation, the device would reach a noise floor of \qty[per-mode = symbol]{0.01}{\V \per \V} only at integration times of slighly more than \qty{10}{\s}, while the datasheet claims \qty{2}{\s}. It is therefore likely, that the noise parameters of a real device are be better than the numbers used in the simulation. Additionally, the datasheet likely refers to an instrument, that is synced to a \qty{60}{\Hz} power line frequency which shifts the sampling frequency up by \qty{20}{\percent} and, as discussed, reduces the noise floor, because more noise content is white noise at the autozero interval. In this simulation the \qty{0.01}{\V \per \V} noise level would be reached at exactly \qty{10}{\s} when using a line frequency of \qty{60}{\Hz}. For the purpose of demonstrating the autozeroing algorithms these subtleties are irrelavant.

For the comparison of different intregation times before applying autozeroing figure \ref{fig:autozero_nplcs_adev} can be consulted. Using the Allan deviation makes it is very simple to compare noise figures for identical measurement times $\tau$, but different integration times before autozeroing is applied.

\begin{figure}[ht]
    \centering
    \input{images/autozero_nplcs_adev.pgf}
    \caption{Allan deviation for different integration times before applying the AZ algorithm. Deadtime $\theta = 0$. The dashed line denotes the Allan variance without AZ.}
    \label{fig:autozero_nplcs_adev}
\end{figure}

It can be seen, that with increasing integration times before applying the AZ algorithm more uncertainty is accumulated due to the $f^{-1}$ content, which cannot be filtered. As a result, after removing the $f^{-1}$ content using autozeroing more time is required for filtering until the same Allan deviation can be reached. From these simulations it can be said, that when there is negligible dead time $\theta$ involved when switching the inputs it is advantageous to switch to switch early, while white noise is still dominating the noise.

Finally, the case of a non-negligible dead time shall be treated. When the dead time has to be considered, it is clear, that the autozero frequency cannot be arbitrarily increased, because the proportion of sampling time lost, in comparison to the time spent sampling, increases. This effective loss in sampling time then increases the noise spectral density due to aliasing as discussed above. To show this effect the simulation above is modified to include a dead time of \qty{1}{\plc} as detailed in figure \ref{fig:dmm_autozer_offset_nulling}. The measurement sequence now includes a dead time after switching each input. \citeauthor{autozero_with_dead_time} proposes \cite{autozero_with_dead_time} splitting the measurement interval in two and instead of measuring HI-LO-HI-LO, to measure HI-LO-LO-HI. This scheme is a mixed bag, because the $f^{-1}$ noise is correlated and its autocorrelation function decays with $e^{-t}$, therefore constantly changing the order of subtracted samples is not as efficient in removing the noise as the normal autozero procedure. Only when the dead time is large in comparison to the measurement time, this method yields an advantage. Therefore only the simplest case of a HI-LO-HI-LO measurement is treated here. The Allan deviation for different integration times is evaluated in same way as in figure \ref{fig:autozero_nplcs_adev}. The results are shown in figure \ref{fig:autozero_deadtime_nplcs_adev}.

Figure \ref{fig:autozero_deadtime_nplcs_adev} demonstrates, that the effectiveness of the AZ scheme no longer increases with an increasing switching frequency and there is an optimal autozero interval. For the parameters chosen for this simulation ($f_c = \qty{1.5}{\Hz}$ and \qty[power-half-as-sqrt, per-mode=symbol]{165}{\nV \Hz\tothe{-0.5}}), \qty{5}{\plc} is the optimal interval. If the corner frequency is shifted to a lower frequency, the optimum shifts more towards \qty{10}{\plc}. The same goes for a higher line frequency of \qty{60}{\Hz}. This explains, why HP chose \qty{10}{\plc} as the maximum integration time. For integration times higher than that, software averaging is used, therefore delivering the perfomance shown in figure \ref{fig:autozero_deadtime_nplcs_adev} along the \qty{10}{\plc} line.

\begin{figure}[ht]
    \centering
    \input{images/autozero_deadtime_nplcs_adev.pgf}
    \caption{Allan deviation for different integration times before applying the AZ algorithm. Deadtime $\theta = \qty{1}{\s}$. The dashed line denotes the Allan variance without AZ.}
    \label{fig:autozero_deadtime_nplcs_adev}
\end{figure}

It should be stressed here, that the dead time is not be the only factor to consider when chossing the autozero interval. For example, in case of an amplifier, switching the input also adds an error current due to the charge injection of the switching transistors. This may negatively impact the measurement of a high impedance source. These additional drawbacks are implementation specific and must considered during the design phase.

\clearpage
\subsection{Gain Correction}
\label{sec:autozero_gain}
The effect of the gain correction, where the input value $x$ is scaled by a scaling factor $y$ to adjust the changing, can be calculated, assuming white noise, as follows:
\begin{align}
    \sigma_{x \cdot y}^2 &= \langle x^2 y^2 \rangle - \langle x y \rangle^2 \nonumber\\
    &= \langle x^2 \rangle \langle y^2 \rangle + \underbrace{2\,\mathrm{Cov}\left(x^2,y^2\right)}_{\text{uncorrelated} \, = \, 0} - \left( \langle x \rangle \langle y \rangle + \underbrace{2\,\mathrm{Cov}\left(x,y\right)}_{=\, 0} \right)^2 \nonumber\\
    &= \left(\sigma_x^2 + \langle x \rangle^2\right) \cdot \left(\sigma_y^2 + \langle y \rangle^2\right) - \langle x \rangle \langle y \rangle \nonumber\\
    &= \sigma_x^2 \sigma_y^2 + \sigma_x^2 \langle y \rangle^2 + \sigma_y^2 \langle x \rangle^2 \label{eqn:variance_multiplied}\\
\end{align}

With respect to the gain correction, equation \ref{eqn:variance_multiplied} can be further reduced. The scaling factor is derived from the reference voltage $V_{ref}$ and normalized using $\frac{V_{ref, meas}}{V_{ref}}$. The expected value, therefore is $\langle y \rangle \approx 1$, as the ADC should not drift much from its calibrated value. Furthermore, $\sigma_y^2$ is scaled by the constant $1/V_{ref}$ and $\sigma_x^2 \sigma_y^2 \ll \sigma_x^2$. The latter should be true for any measurement of significance.

\begin{equation}
    \sigma_{x \cdot y}^2 \approx \sigma_x^2 + \sigma_y^2 \langle x \rangle^2
\end{equation}

The gain correction noise therefore behaves similar to the offset correction case, except, that it scales with the input voltage and has no effect with a shorted input, while fully introducing its noise for a full scale input.


% check \cite{psd_to_adev} Appendix II for details on dead time
% Compare PSD in Generation-Recombination Noise, Allan Variance, and Low-Frequency Gain Instabilities in Microwave Amplifiers to our controller. The hump look similar. Due to popcorn noise

\clearpage
\section{Current Sources}
% TODO: The FET Constant-Current Source/Limiter
% TODO: Cable choice https://www.gore.com/resources/search?f[]=product:24241&f[]=content_type:6&f[]=language:en or http://www.jenving.com/products/view/hd5-hdmi-dvi-dp-blue-b75-1001000064
Throughout this work the concept of current sources is widely used, for example section \ref{sec:laser_current_driver} discusses a current source to drive laser diodes and the temperature controller discussed in section \ref{sec:temperature_controller} uses a current source to measure the resistance of a temperature sensitive resistor. While there are many more use cases, this section will limit the discussion to a few examples used by the devices presented in this work. Namely, this is a unidirectional transconductance amplifier with an operational-amplifier and a field-effect transistor and a bidirectional Howland current pump invented by Bradford Howland in 1962 and first published in 1964 by \citeauthor{howland_current_source} \cite{howland_current_source}. The discussion will start with the properties of the ideal current source and, based on that, develop a more accurate model. The models developed typically represent the static, time-independent case unless explicitely stated. First, the unidirection current source is treated, then the bidirectional Howland current pump is discussed.

\subsection{Current Sink and Current Source}
%\begin{chapquote}{Adapted from William Shakespeare, \textit{Hamlet}}
%``To sink, or not to sink, that is the question.''
%\end{chapquote}

The question whether to use a current source or a current sink is elemental for the design of a laser driver. Figure \ref{fig:current_sink_source} shows different configuration of current sinks and sources with respect to the laser diode.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.225\linewidth}
        \centering
        \import{figures/}{current_source_high.tex}
        \caption{Source with\protect\\grounded LD.}
        \label{fig:current_source_high}
    \end{subfigure}
    \begin{subfigure}{0.225\linewidth}
        \centering
        \import{figures/}{current_source_low.tex}
        \caption{Source with\protect\\floating LD.}
        \label{fig:current_source_low}
    \end{subfigure}
    \begin{subfigure}{0.225\linewidth}
        \centering
        \import{figures/}{current_sink_high.tex}
        \caption{Sink with\protect\\grounded LD.}
        \label{fig:current_sink_high}
    \end{subfigure}
    \begin{subfigure}{0.225\linewidth}
        \centering
        \import{figures/}{current_sink_low.tex}
        \caption{Sink with\protect\\floating LD.}
        \label{fig:current_sink_low}
    \end{subfigure}
    \caption{Different configuration of current sinks and sources with respect to the laser diode. A green checkmark denotes a fail-safe configuration when accidentally shorting one or more pins of the diode to the laser chassis, illustrated by a dashed connection.}
    \label{fig:current_sink_source}
\end{figure}

The most practical configuration depends on the laser diode and safety aspects in terms of protecting the laser diode. The protection of the laser diode is discussed first. The laser resonator is assumed grounded in our setup. While not intended, there are numerous ways to accidently short the diode to ground and since there are no immediate consequences arrising from it, when the controller is disconnected, it might easily be overlooked. This blunder should not bear the risk of destroying an expensive laser diode. To ensure this, a configuration where the laser diode is shorted out instead of the current source or sink must be chosen. That way, the laser diode is automatically removed from the circuit in case of an error condition.
Choosing between a current sink and a current source is more subtle. If the can of the laser diode is connected to anode, a current sink can be considered, to keep the can at ground potential. This is not an issue with our laser design though, because the laser diode mount is floating. Another aspect is the electronics side. A current source is typically implemented using p-channel field-effect transistors, while current sinks are using n-channel transistors and additionally the input of a current source is referenced to the positive supply, while the sink is referenced to the negative supply. Using the negative supply as a reference for control signals brings more challenges than vice versa, because typically integrated components like digital-to-analog converters prefer working with positive voltages and would need additional support to be floated to a negative reference. This makes a current source simpler to implement in this scenario and this work focusses on the current source, but in principle all methods derived can be applied to a current sink as well.

\subsection{Ideal Current Source}
The ideal current source as shown in figure \ref{fig:ideal_current_source} has two major properties besides the output current $I_{out}$, the output impedance and the compliance voltage, which are best understood when looking at the two equivalent representations of a current source separately. On the left in figure \ref{fig:ideal_current_source_norton}, the Norton representation can be seen. Norton's theorem reduces any linear circuit to a current source, shown in green, with a parallel resistance $R_{out}$, usually called output resistance or impedance. On the right, the Thévenin representation can be see, which simplifies a circuit as a voltage source, also shown in green, with a series resistance.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{current_source_norton.tex}
        \caption{Norton representation.}
        \label{fig:ideal_current_source_norton}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{current_source_thevenin.tex}
        \caption{Thévenin representation.}
        \label{fig:ideal_current_source_thevenin}
    \end{subfigure}
    \caption{An ideal current source with output impedance $R_{out}$ and noise $e_n$.}
    \label{fig:ideal_current_source}
\end{figure}

First, the output impedance is discussed. Ideally, $R_{out}$ is infinite and all current is forced to flow through the load. Given a finite output impedance leads to a decreased accuracy of $I_{out}$, because it is influenced by the load impedance as
\begin{equation}
    I_{out} = I_{set} \cdot \frac{R_{out}}{R_{load} + R_{out}} \, .
\end{equation}

In addition to a decreased accuracy, inserting a noise voltage source between the current source and the load as shown in figure \ref{fig:ideal_current_source} in orange, has the same effect as a changing load resistance and due to the finite output impedance $R_{out}$, any voltage noise $e_n$ translates to current noise $i_n$ through the load as
\begin{equation}
    i_n = \frac{e_n}{R_{load} + R_{out}} \approx \frac{e_n}{R_{out}} \, ,
\end{equation}

again making a high output impedance desirable to supress noise sources between the current source and the load.

Going to figure \ref{fig:ideal_current_source_thevenin} of a current source in Thévenin representation allows to discuss the compliance voltage property. As it was said above, the output impedance of an ideal current source is infinite and so is the maximum output voltage of said current source. A finite output impedance immediately implies a finite supply voltage to keep the current to a finite limit, which dictates a maximum output voltage. This is called the compliance voltage.

\subsection{The Field-Effect Transistor Current Source}
\label{sec:mosfet_current_source}
% Good slides can be found here: https://www.ittc.ku.edu/~jstiles/312/handouts/
Given the limited supply voltage of a real current source drives the need for a resistive element that has a finite resitance and infinite, or very high, frequency dependent dynamic impedance to react to load changes. One such pass element, having these properties, is a field-effect-transisistor (FET). A junction-gate field-effect transistor (JFET) or metal–oxide–semiconductor field-effect transistor (MOSFET) can be used either as a current source or sink, depending on its doping. A p-channel FET, which uses a positve doping of the channel, is a current source, while an n-channel FET works as a current sink. This discussin is focussing on the p-channel FET with MOSFETs at its center, because it covers the bulk of the laser current driver design in section \ref{sec:laser_current_driver}.

\begin{figure}[hb]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{p-channel_jfet.tex}
        \caption{P-Channel JFET.}
        \label{fig:pjfet}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{p-channel_mosfet.tex}
        \caption{P-Channel MOSFET.}
        \label{fig:pmos}
    \end{subfigure}
    \caption{The simplified semiconductor structure of a JFET and a MOSFET.}
    \label{fig:FETs}
\end{figure}

The difference between a JFET and a MOSFET, is the gate structure as illustrated in figure \ref{fig:FETs}. While a MOSFET has an insulated gate, the JFET does not. This reduces the gate leakage current, typically by about three orders of magnitude, and allows to forward bias the device since there is no diode, resulting in larger current handling capacity. So for low currents up to a few \unit{\mA} or low noise applications, JFETS are preferred, while MOSFETs can handle several hundred ampere. The same mathematical approach can be applied to both types of FETs though. The other difference between a JFET and a MOSFET is the fact that JFETs are only available as depletion-mode (normally-on) devices, while MOSFETs are available as both depletion and enhancement (normally-off) devices. The reason is the gate structure as mentioned above. An enhancement-mode device does not conduct, when the gate-to-source voltage $V_{GS} = \qty{0}{\V}$, so $|V_{GS}|$ must be increased or enhanced for the device to allow conduction. This is not possible with an uninsulated gate like a simple p-n junction of a JFET, which would then start conducting start leaking. A p-channel depletion-mode device on the other hand conducts at $V_{GS} = \qty{0}{\V}$ and $|V_{GS}|$ must be decreased or depleted to reduce the current, which is not possible with the uninsulated gate, because the p-n junction is reverse biased. The annotated circuit symbol and the quantities used to discuss the device properties are shown in figure \ref{fig:fet_symbols}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{jfet_pins.tex}
        \caption{P-channel JFET.}
        \label{fig:fet_symbols_jfet}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{pmos_pins.tex}
        \caption{P-channel MOSFET.}
        \label{fig:fet_symbols_mosfet}
    \end{subfigure}
    \caption{Basic p-channel FET circuit.}
    \label{fig:fet_symbols}
\end{figure}

A p-channel FET has its source (S) connected to the positive supply and the drain (D) is connected to a more negative voltage, typically the load. For the MOSFET the gate (G) is biased below the source to allow conduction. The source is is usually connected to the substrate for solitary devices as shown in figure \ref{fig:pmos}. This will be assumed in all further discussions and the consequences of a substrate, that is biased differently are omitted here. The interested reader may look up these details in \cite{mosfet_details}.

As it was mentioned above, if appropriately biased, a FET can be considered a voltage controlled current source. This property can be seen in figure \ref{fig:fet_curret_gate_bias}.

\begin{figure}[hb]
    \centering
    \input{images/mosfet_current_gate_bias.pgf}
    \caption{Simulated drain current for different gate bias voltages of an \device{IRF9610} p-channel MOSFET.}
    \label{fig:fet_curret_gate_bias}
\end{figure}

Figure \ref{fig:fet_curret_gate_bias} shows the current $I_D$ flowing out of the drain of a p-channel MOSFET over the drain-to-source voltage $V_{DS}$ that is applied accross the FET. For illustrative purposes an example p-channel MOSFET was chosen and its \textit{Simulation Program with Integrated Circuit Emphasis} (SPICE) model \cite{irf9610_spice,irf9610_spice_better} was used to generate the data, yet the overall shape is the same for all FETs. For more information on modelling MOSFETs in SPICE, \citep[p. 442]{spice_mosfets} can be consulted. There are two regions, the first region, where $V_{DS} > V_{GS} - V_{th}$, demonstrates an almost linear linear correlation of the channel current and the voltage across the device. This is called the ohmic region, where the MOSFET behaves much like a (gate-) voltage controlled resistor and can be described \cite{shockley_fet_equations} as
\begin{equation}
    I_{D,ohmic} = \underbrace{\kappa (V_{GS} - V_{th}) V_{DS}}_{\text{ohmic}} - \underbrace{\frac 1 2 \kappa V_{DS}^2}_{\text{pinch off}} \, .
\end{equation}
For small voltages $V_{DS}$ the output current is proportional to the applied voltage $V_{DS}$ accross the channel, just like a normal resistor, hence calling its name: ohmic region. As the voltage increases further $I_D$ starts leveling off, because $V_{DS}$ starts affecting the channel conductivity. The channel is slowly getting pinched off at one end and becomes tapered. The reason is, that the voltage $V_{DS}$ is dropped accross the length of the channel. This voltage drop is linear with $V_{DS}$, resulting in a $-V_{DS}^2$ dependency of the current, reducing the conductivity of the channel. $V_{th}$ is called the threshold voltage of a MOSFET or pinch-off voltage $V_p$ in case of a JFET and is the voltage at which a current starts flowing.

The parameter $\kappa$ is a device specific parameter and depends on process parameters and the geometry of the device.
\begin{equation}
    \kappa = \kappa' \frac W L = \mu C_{ox} \frac W L
\end{equation}
$\mu$ is the electron mobility, which is about \qty{1350}{\square \cm \per \V} for n-channel MOSFETs and about \qty{540}{\square \cm \per \V} for p-channel MOSFETs \cite{fet_equations}. $C_{ox}$ is the gate-oxide capacitance per unit area and determined by the thickness $t_{ox}$ of the silicon dioxide layer of the gate
\begin{equation}
    C_{ox} = \frac{\epsilon_{ox}}{t_{ox}} \approx \frac{3.9 \cdot \epsilon_{0}}{t_{ox}} \approx \frac{\qty{3.45e-11}{\F \per \m}}{t_{ox}}\,,
\end{equation}
W is the width of the channel, and L is the length of the channel.

The letter $\kappa$ is used here instead of the usual $k$ as it is used by \citeauthor{fet_equations} \cite{fet_equations} to avoid confusion with the Boltzmann constant $k_B$. Unfortunately, $\kappa$ is not well controlled \cite{horowitz1989}, because it is not just determined by the size, but also the doping of the material. While the size of the structure can be well controlled to within a few \unit{\nm} using lithography masks, the doping is a matter of temperature and time in a diffusion furnace. The ohmic mode of operation is, for example, used in linear voltage regulators to control the output voltage of the regulator, forming a low impedance voltage source, not the desired current source. This brings up the next region to discuss.

Once the voltage $V_{DS}$ has reached $V_{GS} - V_{th}$, the channel is fully pinched off, any further increase in $V_{DS}$ will not lead to an increase in $I_D$, in other words the output resistance becomes infinite. The MOSFET is said to be pinched-off or in saturation. In practice there still is a small influence of $V_{DS}$ on the channel. While the depth can no longer decrease as its length is \num{0} at one end already, the channel will retract a small amount in length with increasing $V_{DS}$. This is taken into account by the factor $\lambda$, called channel-length modulation. The drain current in saturation can now be described \cite{shockley_fet_equations} as
\begin{equation}
    I_{D,sat} = \underbrace{\frac 1 2 \kappa \left(V_{GS} - V_{th} \right)^2}_{\text{ideal FET}} (1 + \lambda V_{DS}) \, . \label{eqn:mosfet_saturation}
\end{equation}

The parameter $\lambda$ is the first order Taylor exapansion of the length dependence of $\kappa$ and typically is small and on the order of \qtyrange[per-mode=power]{0.01}{0.05}{\per \volt} for p-channel MOSFETs \citep[p. 23]{mosfet_flicker_noise}. It mainly depends on the length of the channel to which it is inversely proportional, since the channel length defines the slope of the tapered channel. Sometimes the value $\frac{1}{\lambda}$ is also referred to as the Early voltage $V_A$. It is noteworthy, that more modern processes choose a smaller channel length to reduce the on-state resistance of the MOSFET, because the main application of a MOSFET nowadays is as a switch. The reduced channel length makes the MOSFET more susceptible to the channel length modulation effect. This will be discussed in more detail in section \ref{sec:component_selection}, when choosing a suitable MOSFET.

Going back to figure \ref{fig:fet_curret_gate_bias} the effect of the channel-length modulation can be seen as a small slope of $I_D$ in the saturation region.

Combining the previous equations, the FET drain current behaviour can be summed up as
\begin{equation}
    I_D = \begin{cases}
        0 & \text{if } V_{GS} - V_{th} < 0\\
        \kappa (V_{GS} - V_{th}) V_{DS} - \frac 1 2 \kappa V_{DS}^2 & \text{if } V_{GS} - V_{th} >= 0 \text{ and } V_{DS} < V_{GS} - V_{th}\\
        \frac 1 2 \kappa \left(V_{GS} - V_{th} \right)^2 (1 + \lambda V_{DS}) & \text{if } V_{GS} - V_{th} >= 0 \text{ and } V_{DS} \geq V_{GS} - V_{th}
    \end{cases}
    \label{eqn:mosfet_id_large_signal}
\end{equation}

The saturation region is the region of interest for building a high output impedance current source, because for a wide range of $V_{DS}$, the current remains almost constant and can be adjusted using the gate voltage $V_{GS}$. As a reminder, for the p-channel MOSFET, all voltages are reversed. $V_{GS}$, $V_{th}$, $V_{DS}$, $\kappa$ and $I_D$ are negative. Some datasheets therefore only give the magnitude of those quantities. The important aspect to remember, is that for the p-channel enhancement-mode MOSFET the gate must be biased negative with respect to the source pin by a least the threshold voltage ($V_{GS} < V_{th}$ or $|V_{GS}| > |V_{th}|$) to turn the transistor on and allow current to flow.

Before proceeding to the precision current source in section \ref{sec:precision_current_source}, the concept of conductance and transconductance must be explored. The transconductance describes the relationship of the input voltage with the output currrent. The conductance is a measure for how well current flows from input to output. The transconductance $g_m$ and the channel conductance $g_{DS}$ are defined as
\begin{align}
    g_{m, sat} &\coloneqq \left. \frac{\partial I_{D,sat}}{\partial V_{GS}} \right|_{V_{DS} = const} = \kappa \left(V_{GS} - V_{th} \right) (1 + \lambda V_{DS}) \, , \label{eqn:mosfet_gm}\\
    &= \sqrt{2 \kappa I_D \left(1+ \lambda V_{DS}\right)} \approx \sqrt{2 \kappa I_D} \label{eqn:mosfet_gm_approximation} \\
    g_{DS, sat} &\coloneqq \left. \frac{\partial I_{D,sat}}{\partial V_{DS}} \right|_{V_{GS} = const} = \frac{1}{2} \kappa \left(V_{GS} - V_{th} \right)^2 \lambda\\
    &= \frac{I_D}{\frac{1}{\lambda} + V_{DS}} = \frac{1}{R_o} \approx I_D \lambda \label{eqn:mosfet_gds}\,.
\end{align}
The transconductance $g_m$, as a measure of the current gain with respect to the gate-source voltage of the MOSFET, is proportional to the square root of the drain current $I_D$. The inverse of the channel conductance $g_{DS}$ is called output resistance $R_o$ and discussed below. Typically the $V_{DS}$ term in the denominator of the output resistance in equation \ref{eqn:mosfet_gds} can be neglected.

The meaning of $g_{m}$ and $g_{GS}$ can be best understood, when looking at a mathematical model of the MOSFET. These models come in varying complexity and either as a large-signal or small-signal model. Only the latter is used here. The small-signal model, is a first-order Taylor approximation around the working point, for a constant gate-source voltage $V_{GS}$ and constant drain-source $V_{DS}$, hence both $g_{m}$ and $g_{GS}$ are constants.
\begin{align}
    I_D &\approx \frac{\partial I_D}{\partial V_{GS}} \Delta V_{GS} + \frac{\partial I_D}{\partial V_{DS}} \Delta V_{DS}\\
    &= g_{m} \Delta V_{GS} + g_{DS} \Delta V_{DS}\\
    &= g_{m} v_{GS} + \frac{1}{R_o} v_{DS} = i_D \label{eqn:mosfet_id_small_signal}
\end{align}
The lower case letters denote the variables of the small-signal model as they only change very little compared to the working point parameters.
From \ref{eqn:mosfet_id_small_signal} it can be seen, that the $g_{DS}$ term adds to the output current and is proportional to $v_{DS}$. Comparing with figure \ref{fig:ideal_current_source_norton}, this the proportionality constant can be identified as $\frac{1}{R_o}$ like proposed above. Just like the ideal current source in figure \ref{fig:ideal_current_source}, the model can be given in the Norton or Thévenin representation both shown in figure \ref{fig:mostfet_small_signa_model}.

\begin{figure}[hb]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{mosfet_small_signal.tex}
        \caption{Small signal model of a saturated MOSFET including the output resistance. The output resistance models the channel-length modulation as given by equation \ref{eqn:mosfet_id_small_signal}.}
        \label{fig:mostfet_small_signa_model_model_norton}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \centering
        \import{figures/}{mosfet_small_signal_t-model.tex}
        \caption{MOSFET model in Thévenin representation.}
        \label{fig:mostfet_small_signa_model_thevenin}
    \end{subfigure}
    \caption{Equivalent MOSFET models in Norton and Thévenin representations.}
    \label{fig:mostfet_small_signa_model}
\end{figure}

A detailed graphic derivation of the Thévenin representation can be found in \cite{fet_equations}. The Thévenin representation will prove especially valuable, when treating circuits with a resistance in the source leg.
The small-signal model now shows, that the output impedance is dependent on the channel-length modulation $\lambda$ and $v_{DS}$. Typically, $\frac{1}{\lambda} \gg v_{DS}$, so $\lambda$ is the most important factor governing the output impedance of a MOSFET.

To give an example of the output impedance of a MOSFET, parameters were taken from the aforementioned SPICE model of the \device{IRF9610}. Do note, that these parameters of a model are tuned to match certain operating conditions by their creatros and only present an estimation of the real MOSFET. Using the example parameters from \ref{tab:current_source_parameters} $I_D=\qty{250}{\mA}$, $\lambda = \qty[per-mode=power]{4}{\per \milli \volt}$, $V_{DS}=\qty{3.5}{\V}$ yields
\begin{equation}
    R_{out} = R_{o}\left(I_D=\qty{250}{\mA}, \lambda = \qty[per-mode=power]{4}{\per \milli \volt}\right) = \qty{1014}{\ohm} \overset{V_{DS} = 0}{\approx} \qty{1}{\kilo \ohm} \, , \label{eqn:mosfet_rout_irf9610}
\end{equation}
which is not very convincing as a current source. The small impact of $V_{DS}$ on the output impedance can be seen when dropping the $V_{GS}$ term, which leads to an output impedance of \qty{1}{\kilo \ohm}. Usually, in textbooks, this dependence is therefore neglected. To improve $R_{out}$, the focus thus lies on the $\lambda$ dependence. The model derived from equation \ref{eqn:mosfet_id_small_signal} can be used to do so, leading to the precision current source presented next.

% This can be demonstrated building a simple current source and then cascoding it. A very simple current source can be built using a JFET. As mentioned above, a JFET is a depletion-mode device and is already turned on at $V_{GS} = \qty{0}{\V}$. To turn it off the gate voltage must be increased above the source leg. For an illustration, refer to figure \ref{fig:jfet_curret_gate_bias}, which is very similar to the MOSFET behaviour.
%
% \begin{figure}[ht]
%     \centering
%     \input{images/jfet_current_gate_bias.pgf}
%     \caption{Simulated drain current for different gate bias voltages of a \device{2N5460} p-channel JSFET.}
%     \label{fig:jfet_curret_gate_bias}
% \end{figure}
%
% The topmost curve of figure \ref{fig:jfet_curret_gate_bias} is the case with a direct connection of the gate to the source. Above about $V_{DS}=\qty{4}{\V}$, the JFET works as a current source, although the effect of the Early voltage given in equation \ref{eqn:mosfet_saturation} can be clearly seen with a slight dependence of the output current on $V_{DS}$. With increasing $V_{GS}$, it can be observed, that the slope of $I_D$ flattens.
%
%
% An example for a cascode is shown in figure \ref{fig:current_source_jfet_cascode}.
%
% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[t]{0.3\linewidth}
%         \centering
%         \import{figures/}{current_source_fet_no_bias.tex}
%         \caption{JFET current source.}
%         \label{fig:current_source_jfet_no_bias}
%     \end{subfigure}%
%     %\hfill%
%     \begin{subfigure}[t]{0.3\linewidth}
%         \centering
%         \import{figures/}{current_source_fet_bias.tex}
%         \caption{Self-biased JFET current source.}
%         \label{fig:current_source_jfet_bias}
%     \end{subfigure}%
%     %\hfill%
%     \begin{subfigure}[t]{0.3\linewidth}
%         \centering
%         \import{figures/}{current_source_fet_cascode.tex}
%         \caption{Cascoded JFET current source.}
%         \label{fig:current_source_jfet_cascode}
%     \end{subfigure}
%     \caption{Different types of JFET current sources with increasing output impedance.}
%     \label{fig:current_source_jfet}
% \end{figure}

\clearpage
\subsection{Precision Current Source}
\label{sec:precision_current_source}
In the previous section \ref{sec:mosfet_current_source} it was shown in equation \ref{eqn:mosfet_id_small_signal}, that the output impedance of a MOSFET depends on the channel-length modulation $\lambda$ and is too low for practical purposes. On the quest to improve the output impedance of the MOSFET circuit \ref{fig:mostfet_small_signa_model_model_norton}, the most obvious solution would be to simply add a source resistor $R_S$ into the circuit as shown in in figure \ref{fig:pmos_current_source_resistor}. At first glance, this may seem to only add a series resistance to $R_o$, but the attempt is more interesting and will lead to an even better solution.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \import{figures/}{current_source_resistor.tex}
        \caption{P-channel MOSFET with source resistor $R_S$ to improve the output impedance $R_{out}$.}
        \label{fig:pmos_current_source_resistor}
    \end{subfigure}%
    \begin{subfigure}[t]{0.45\linewidth}
         \centering
         \import{figures/}{pmos_small_signal_resistor.tex}
         \caption{Equivalent small-signal Thévenin model.}
         \label{fig:pmos_current_source_resistor_small_signal}
     \end{subfigure}%
     \caption{Circuit of a MOSFET with source degeneration resistor and equivalent Thévenin model.}
\end{figure}

Before calculating the output impedance, we shall have a look at $v_{GS}$ and the input signal $v_i$ derived from it. With the introduction of the sense resistor $R_S$, $v_i$ no longer equals $v_{GS}$, because $\frac{1}{g_m}$ now forms a voltage divider with $R_S$ and it follows
\begin{equation}
    v_{GS} = v_i \frac{\frac{1}{g_m}}{R_S + \frac{1}{g_m}} = v_i \frac{1}{1 + g_m R_S} \,.
\end{equation}
This implies a reduction in gain, by the factor $\frac{1}{1 + R_S g_m}$ compared to the previously discussed approach. The cause of this reduction is negative feedback. To understand this, imagine, that with a constant $v_i$ and hence a constant current $I_D$ flowing, a changing load resistance is trying to modulate $I_D$. Any increase in $I_D$ will cause the voltage across $R_S$ to rise, reducing $v_{GS}$, because $v_i$ is still constant. The decreasing $v_{GS}$ will then reduce $I_D$, thus introducing negative feedback. Having realized there is negative feedback present, it can be postulated, that the reduction in input sensitivity, or effective transconductance, will passed on to the output impedance. This very interesting relationship will now be derived.

To calculate the output impedance, figure \ref{fig:pmos_current_source_resistor_small_signal} can be simplified by grounding $v_i$, because there is no AC component as there is no current flowing through the insulated MOSFET gate and is not modulated. The load $R_{load}$ resistance must is replaced by an AC test voltage $v_{load}$ to modulate $I_D$. These changes result in the small signal model shown in figure \ref{fig:pmos_common_gate_amplifier}. As a sidenote, this configuration is also called a common-gate amplifier.

\begin{figure}[ht]
    \centering
    \import{figures/}{mosfet_small_signal_cg.tex}
    \caption{Small signal model of the common-gate amplifier with source resistance $R_S$.}
    \label{fig:pmos_common_gate_amplifier}
\end{figure}

The (dynamic) output impedance is given by
\begin{equation}
    R_{out,cg} = \frac{v_{load}}{i_D}\,, \label{eqn:mosfet_rout}
\end{equation}
with $i_D = i_S$, since there is no gate current. $v_{load}$ can easily be calculated by looking at figure \ref{fig:pmos_common_gate_amplifier} and is the total voltage across $R_o$ and $R_S$. $v_{GS}$ can also be found, because the gate is grounded. With the resistance $\frac{1}{g_m}$ at one end, the voltage at the source pin must be $-v_{GS}$.
\begin{align}
    v_{load} &= \left(i_D - i\right) R_o + i_S R_S \nonumber\\
    &= \left(i_D - g_m v_{gs}\right) R_o + i_D R_S \nonumber\\
    &= \left(i_D + g_m i_D R_S\right) R_o + i_D R_S \label{eqn:mosfet_cg_vout}
\end{align}
Using equations \ref{eqn:mosfet_rout} and \ref{eqn:mosfet_cg_vout} gives
\begin{equation}
    R_{out,cg} = \left(1 + g_m R_S\right) R_o + R_S \label{eqn:mosfet_cg_rout}
\end{equation}
for the output impedance.

This result is interesting, as it can be be immediately seen, that the output impedance scales very quickly with the transconductance $g_m$ and $R_S$. As it was already speculated above, the reduction in the transconductance $\frac{1}{1 + g_m R_S}$ of the MOSFET is transfered to the output impedance, which is increasing by the inverse of the loss in transconductance.

Going back to the quest for increased output impedance, it is appararent, that increasing $R_S$ quickly raises the output impedance, as it scales with $gm_m R_o$, but it would come at the cost of a significantly reduced compliance voltage. So, other means need to be explored. As we have seen, the scale factor $gm_m R_o$ is explained by feedback and this brings up another solution. The amount of feedback can be increased further using an operational amplifier (op-amp) as shown in figure \ref{fig:precision_current_source}.

\begin{figure}[ht]
    \centering
    \import{figures/}{precision_current_source.tex}
    \caption{Transconductance amplifier with a p-channel MOSFET.}
    \label{fig:precision_current_source}
\end{figure}

The output impedance of this transconductance amplifier is amplified by the open-loop gain of the op-amp as shown in appendix \ref{sec:transfer_function_transconductance}, while the transfer function greatly simplified and found to be
\begin{align}
    R_{out} &\approx A_{ol} \left(g_m R_o R_S + R_o + R_S \right) \nonumber\\
    I_{out} &\approx \frac{V_{ref}}{R_S} \label{eqn:current_source_transfer_function}
\end{align}

In addition to the increased output impedance, the current $I_D = I_{out}$ can now steered by adjusting $V_{ref}$ and is, given sufficient loop gain of the op-amp, no longer dependent on the MOSFET, but rather only on the sense resistor $R_S$.

This has the added benefit, that it is possible to leverage the tight accuracy and precision of a resistor, over the poor specifications of a MOSFET. Resistors can be manufactured with tolerances of less than \qty{100}{\micro \ohm \per \ohm}, which is orders of magnitude better than FETs, which can be matched to low \unit{\percent} values with patience.

Using the example parameters from table \ref{tab:current_source_parameters}, the output impedance in staturation can now be calculated again for $I_{out}=\qty{250}{\mA}$ and the ideal \device{IRF9610} model with the addition of an idealized \device{AD797} op-amp using the worst-case specifications.
\begin{equation}
    R_{out} \approx \qty[per-mode=power]{2}{\volt \per \uV} \left(\qty{0.64}{\siemens}\cdot \qty{1014}{\ohm} \cdot \qty{30}{\ohm} + \qty{1014}{\ohm} + \qty{30}{\ohm} \right) \approx \qty{40}{\giga\ohm}
\end{equation}

From these consideration, it can be seen, that the open-loop gain and the unity-gain bandwidth of the op-amp essentially determine the properties of the current source, given that $R_{id} \gg R_S$ and $R_o \gg R_S$. This will be important for selecting an operational amplifier later.

The next section will focus on the MOSFET and discuss the compliance voltage of the current source, which was only briefly touched during the introdution. It will give rise to criteria for selecting a MOSFET for the precision current source.

\clearpage
\subsection{Compliance Voltage}
\label{sec:compliance_voltage}
The compliance voltage of a current source is the maximum voltage it can output to maintain the requested output current. For an ideal current source, the compliance voltage is infinite, but obviously limited in the physical world.

The precision current source discussed in section \ref{sec:precision_current_source} has several limiting factors of the compliance voltage, which shall be discussed now. The compliance voltage is taxed most at the maximum output current $I_{out,max}$. So for the following discussion, the output is always treated as set to maximum.

Looking at figure \ref{fig:precision_current_source} of the precision current source it is immediately evident, that the output voltage can be calculated by subtracting the voltage accross the source resistor $V_{R_S}$ and the MOSFET $V_{DS}$ from the supply voltage $V_{sup}$
\begin{equation}
    V_{out} = V_{sup} - V_{R_S} - V_{DS} = V_{sup} - V_{ref} - V_{DS} \nonumber\,.
\end{equation}

The voltage $V_{R_S}$ is given by equation \ref{eqn:current_source_transfer_function} and equal to the setpoint voltage and hence given by the system parameters. This leads to the question of the minimum working point voltage $V_{DS}$ at $I_{out,max}$. As a reminder, from equation \ref{eqn:mosfet_id_large_signal} and figure \ref{fig:fet_curret_gate_bias} one can see, that the drain current is almost constant over $V_{DS}$ in the saturation region and in the ohmic region is proportional to $V_{DS}$. The transition point from the ohmic region to the saturation region is at $V_{DS} = V_{GS} - V_{th}$ and putting this into equation \ref{eqn:mosfet_id_large_signal} yields for the drain current
\begin{align}
    I_D &= \frac{1}{2} \kappa V_{DS}^2 \left(1+ \lambda V_{DS}\right) \nonumber\\
    \Rightarrow V_{DS} &\approx \sqrt{\frac{2 I_D}{\kappa}}\\
    &\approx \qty{784}{\mV}
\end{align}

The latter result was calculated using the example parameters from table \ref{tab:current_source_parameters}. At this point it can already be postulated, that the MOSFET will severly change in its function as a current source for $V_{DS} < \qty{0.78}{\V}$. To quantify this, one has to look at the output impedance of the transconductance amplifier once again. In the last section, the output impedance was only treated for the staturation region, but this time, $R_{out}$ must must be considered over a wide range of $V_{DS}$, thus not only in the saturation region, but also in the ohmic region. Instead of using the small-signal model as before, which assumed only small changes of $V_{DS}$, a large-signal model must applied, which also includes the non-linear nature of the piece-wise defined equation \ref{eqn:mosfet_id_large_signal} of the drain current.

For the sake of simplicity, a SPICE simulation of figure \ref{fig:precision_current_source} was carried out in LTSpice \cite{ltspice}. Solving this analytically, bears no educational value over the numerical solution shown below as will be seen. Additionally, the SPICE simulation also offers the opportunity to add additional, parasitic elements to the model to evaluate their effect, for example, the capacitive nature of the MOSFET gate.

The simulation itself is numerically fairly challenging and the typical approaches will lead to the limits of the numerical precision. To make simulation possible, the large-signal model is broken down into several small segements. For each these segments, the small-signal models at its respective working point is evaluated and then the result joined back together to reconstruct the large-signal model sought. How this is done in detail, is shown in appendix \ref{sec:ltspice_current_source} as it is beyond the scope of this section. The final result was calculated for two different frequencies, one frequency was deliberately chosen so low (\qty{1}{\micro\Hz}), that it is well below the dominant pole of the op-amp, meaning, that the full open-loop gain applies and the other frequency chosen was \qty{1}{\MHz}, were the gain had dropped to \qty[per-mode=power]{10}{\V \per V}. This is shown in figure \ref{fig:ltspice_output_impedance_simulation}.

\begin{figure}[ht]
    \centering
    \input{images/ltspice_output_impedance_simulation.pgf}
    \caption{Simulated output impedance for the precision current source from figure \ref{fig:precision_current_source} at DC and \qty{1}{\MHz} over the drain-source voltage.}
    \label{fig:ltspice_output_impedance_simulation}
\end{figure}

Looking at figure \ref{fig:ltspice_output_impedance_simulation} clearly shows the effect of entering the ohmic region of the MOSFET. Over a range of about \qty{100}{\mV} below the \qty{0.78}{\V} calculated above, the output impedance drops by two orders of magnitude and then keeps dropping at an exponential rate with decreasing $V_{DS}$. The same effect applies to the output impedance at \qty{1}{\MHz}, although the starting value is around \qty{200}{\kilo\ohm} due to the reduced gain from the op-amp at \qty{1}{\MHz}. It can also be seen, that $R_{out}$ levels off at \qty{30}{\ohm}, the value of the sense resistor.

This overall effect of leaving the saturation region is so drastic, that the compliance voltage must be defined in such a way, that the MOSFET remains in saturation and this leads to
\begin{equation}
    V_{comp} = V_{sup} - V_{ref} - \sqrt{\frac{2 I_D}{\kappa}} \,.
\end{equation}

Now turning to the supply voltage, it is limited by the op-amp which must drive the gate of the MOSFET all the way up to the supply to turn off the current source. The reference voltage is, unless one divides it down, which is a delicate matter, dictated by the reference chosen. This, unfortunately, leaves only little room for the MOSFET and it must be carefully chosen not limit the compliance voltage too much.

At this point a fallacy, the author has observed multiple times must addressed. In order to address the limited compliance voltage, one may be tempted to use multiple MOSFETs in parallel to divide the current between the MOSFETs and thereby reduce the voltage that needs to be dropped across the FET proportional to $\frac{1}{\sqrt{N}}$, where $N$ is the number of MOSFETs paralleled.

Imagine the following modified circuit of the precision current source shown in figure \ref{fig:precision_current_source_two_mosfets} with two MOSFETs in parallel. For clarity the gate resistors required are not included.

\begin{figure}[ht]
    \centering
    \import{figures/}{precision_current_source_2fets.tex}
    \caption{Transconductance amplifier with two p-channel MOSFETs in parallel.}
    \label{fig:precision_current_source_two_mosfets}
\end{figure}

While at first, this seems like a solution to the limited $V_{DS}$, it is not that easy and a bad idea for number of reasons given here. The first reason is, MOSFET specifactions are very loose, notably the threshold voltage $V_{th}$, the transconductance $g_m$ and the capacitances, but the latter is of little concern here. Paralleling MOSFETs works well under certain conditions, when using the MOSFETs as a switch, not as a current source. It seems to be a common misunderstanding, that MOSFETs are imune to thermal runaway. This is true, when using them as a switch fully turned on and in the ohmic region. In this case, there a two effects occuring, the first is, that the (absolute) value of $V_{th}$ decreases with temperature, thus increasing $I_D$ and the second effect is, $R_{DS,on}$ is rising with temperature \cite{mosfet_thermal_runaway}. But here, the MOSFET is operating in pinch-off and not the ohmic region, $R_{DS,on}$ has no influence on the current, therefore, the only effect at work is the decreasing $V_{th}$, so depending on how bad the imbalance in $V_{th}$ of the paralled MOSFETs is, one MOSFET will gobble up most of the current and power. Adding source resistors, can compensate for this by pushing down the source voltage as the current goes up. This will then reduce $V_{GS}$. The size of the resistor depends on the transconductance $g_m$ and the temperature coefficent of $V_{GS}$, which is around \qtyrange[range-units = single]{1.5}{2}{\mV \per \K} \cite{mosfet_vgs_tempco}. Unfortunately, \qty{1}{\ohm} or \qty{2}{\ohm}, will already eat up, most of the benefits gained in compliance voltage as will be shown below. A detailed analyis of paralleling MOSFETs can be found in \cite{paralleling_mosfets}.

The second reaon, why paralleling MOSFETs is a bad idea can be found, when
remembering equation \ref{eqn:mosfet_id_large_signal}. We know that the transition from the undesirable ohmic region to the saturation region is
\begin{equation}
    V_{DS} \geq V_{GS} − V_{th}
\end{equation}

Looking at \ref{fig:precision_current_source_two_mosfets}, we see that $V_{GS}$ is set by the op-amp and is the same for both MOSFETs, but $V_{th}$ is device specific and according to the datasheet of our example \device{IRF9610} \cite{datasheet_IRF9610} $V_{th}$ values can show a spread of as much as \qtyrange[range-units = single]{-2}{-4}{\V}, although \cite{appnote_mosfet_parameter_spread} suggests, that MOSFETs from the same reel show a spread of only \qty{\pm 125}{\mV} of $V_{th}$  within the same batch for consecutive devices. The \qty{125}{\mV} was found for the \device{BUK7S1R5-40H} \cite{datasheet_BUK7S1R5}, which was sampled in this report. The number given in the report is for $3\sigma$ and assuming the datasheet values are also referring to $3\sigma$, the value found in the report is about twice as good as the datasheet value of \qtyrange[range-units = single]{2.4}{3.6}{\V}. Assuming similar numbers for \device{IRF9610} MOSFET used in our examples, this leads to \qty{\pm 208}{\mV} for the \device{IRF9610}, again applying $3\sigma$. Using this number, a Monte Carlo simulation (not quite, because the dice were biased to yield a Gaussian distribution) was run using LTSpice, simulating the circuit shown in figure \ref{fig:precision_current_source_two_mosfets} and also the original circuit using only one MOSFET. The current source was set to \qty{250}{\mA} as per table \ref{tab:current_source_parameters}. The load voltage was set to
\begin{equation}
    V_{DS, parallel} = \sqrt{\frac{2 \frac{I_d}{2}}{\kappa}} \approx \qty{555}{\mV}\,,
    V_{DS, single} = \sqrt{\frac{2 I_d}{\kappa}} \approx \qty{784}{\mV} \,. \nonumber
\end{equation}

$\frac{I_D}{2}$ was used for the parallel configuration to show the effect assuming perfect current sharing between the MOSFETs. Additionally, $V_{DS, parallel} + 1\sigma$ was also investigated. \num{4000} samples were drawn and the spread of the output impedance was calculated for each circuit. The results are shown as a histogram in figure \ref{fig:ltpsice_mosfet_mc_output_impedance}. The counts give the number of cases for each bin of the output imdedance.

\begin{figure}[ht]
    \centering
    \input{images/ltspice_mosfet_mc_output_impedance.pgf}
    \caption{Results of a Monte Carlo simulation of the output impedance for different configurations of MOSFETs.}
    \label{fig:ltpsice_mosfet_mc_output_impedance}
\end{figure}

Unsurprisingly, there is no variance of the output impedance in the single MOSFET case in accordance to what we have learnt in appendix \ref{sec:transfer_function_transconductance}. The op-amp gain simply supresses all device properties of the MOSFET. The slight variation of $g_m$ for different samples was not simulated, because this variation stems from the variation of $\kappa$ and goes as $\frac{1}{\sqrt{\kappa}}$, so its effect is not as pronounced as the threshold.

In case of two mosfets, the output impedance varies over an order of magnitude from about \qtyrange[range-units = single]{1.8}{52}{\giga \ohm}. Even when increasing the drain-source voltage by $1 \sigma = \qty{70}{\mV}$ to \qty{625}{\mV} on average, the spread is still an order of magnitude. Only when increasing $V_{DS}$ to around \qty{700}{\mV}, the situation stabilizes, but then the net gain from this measure has shrunk to a meager \qty{84}{\mV}. We can see from this simuation, that the system-to-system spread becomes very unstable in tough situations. This instability can also be brought into the system by temperature effects as $V_{th}$ is temperature dependent as discussed above. This is a designers nightmare, because these devices are no longer interchangeable in situations of high load currents and load impedances. Additionally, they may suffer from thermal runaway if each individual MOSFET is not layed to carry the full current. As a final remark: Do not parallel MOSFETs in saturation, ever.

\clearpage
\subsection{Noise Sources}
\label{sec:current_source_noise}
The fundamentals of different types of noise were already introduced in section \ref{sec:allan_deviation}. Here, a subset of these noise types is treated. It is expected, that the dominant noise observed in this circuit is $\frac{1}{f}$-noise at low frequencies and white wideband-noise. All noise components will be converted to the so-called input referred notation to make the noise sources comparable. This can be easily understood, when looking at two amplifiers with different gain. If both of them add a fixed amount of noise to the output signal, the absolute amount of noise may be the same, but the signal to noise ratio shows a different picture. To compare these amplifiers it is useful to divide the noise by the transfer function (gain) of the amplifier. This is called input-referred noise, since it treats the noise in relation to the input signal. Additionally, when calculating noise figures, the noise bandwidth is always considered to be \qty{1}{\Hz}.

\begin{figure}[ht]
    \centering
    \import{figures/}{precision_current_source.tex}
    \caption{Transconductance amplifier with a p-channel MOSFET. Repeated from page \pageref{fig:precision_current_source}.}
    \label{fig:precision_current_source_noise}
\end{figure}

Noise sources are ubiquitous in the circuit in figure \ref{fig:precision_current_source} on page \pageref{fig:precision_current_source}, repeated here as figure \ref{fig:precision_current_source_noise} for clarity. The resistor $R_S$, the MOSFET, the op-amp, the setpoint voltage $V_{ref}$ and the supply voltage $V_{sup}$ can all contribute noise to the output current. Fortunately, some of those noise contributions are either very small or are well suppressed in this design, so each component must be briefly discussed.

Starting with the supply voltage $V_{sup}$, it can be seen, that any change of this voltage affects the string $R_S$-$Q$-$R_{load}$. From equation \ref{eqn:transconductance_amplifier_transfer_function}, we know that if the op-amp gain is high, that is, within the bandwidth of the op-amp, all disturbances of the voltage accross $R_S$ will be suppressed and the output current is only defined by the reference input and $R_S$. Looking closer, the supply noise is present at the inverting and non-inverting input of the op-amp with the same magnitude. If there is no current flowing into the op-amp pins, which is true for low frequecies, the noise is affecting both pins equally and it will be suppressed by the common-mode-rejection ratio (CMRR) of the device. Fortunately, this is a strong quality of precision op-amps and values of more than \qty[per-mode=power]{1}{\uV \per \volt} are not uncommon. The op-amp will therefore take care of the supply noise at low frequencies. At high frequencies the parasitic capacitance of the input pins and the reduced gain and CMRR come into play, reducing the CMRR and the gain also drop at high frequencies. To take care of this, it is therefore prudent to filter the supply for high frequency noise.

The next noise source is the reference voltage. The reference is directly connected to the input and its noise dictates most of the circuit noise. While the high-frequency noise can again be filtered to some extend, the low frequency noise, which is mostly $\frac{1}{f}$-noise can not be filtered as was shown in section \ref{sec:flicker_noise}, so it must be kept low from the start and the reference selected for low flicker noise.

The MOSFET as a noise source is considered in appendix \ref{sec:mosfet_noise} and the interested reader may find the derivation of the MOSFET noise within our circuit there. The two types of noise that need to be considered are the flicker noise of the MOSFET and its wideband thermal noise as calculated in equation \ref{eqn:current_noise_mosfet}
\begin{equation}
    i_{n} = \sqrt{\underbrace{4 k_B T \frac{2}{3} g_m}_{\text{thermal}} + \underbrace{\frac{K_f I_D}{C_{ox} L^2} \frac{1}{f}}_{\text{flicker}}} \,.\nonumber
\end{equation}

To calculate the input referred noise and show that the MOSFET noise will be suppressed by the op-amp, the current noise needs to be divided by the open-loop gain derived as equation \ref{eqn:transconductance_amplifier_open_loop_gain}
\begin{equation}
    e_{n,FET} = \frac{i_n}{A_f} = \frac{\sqrt{4 k_B T \frac{2}{3} g_m + \frac{K_f I_D}{C_{ox} L^2} \frac{1}{f}}}{\frac{A_{op}}{R_S} \frac{g_m \left(R_o || R_S || R_{id}\right)}{g_m \left(R_o || R_S || R_{id}\right) + 1}} \,.
\end{equation}

Looking at the parameters from table \ref{tab:current_source_parameters}, we find $\left(R_o || R_S || R_{id}\right) \approx R_S$ and $e_n$ can be simplified to
\begin{align}
    e_{n,FET} &\approx \frac{\sqrt{4 k_B T \frac{2}{3} g_m + \frac{K_f I_D}{C_{ox} L^2} \frac{1}{f}}}{A_1 \frac{1}{R_S + \frac{1}{g_m}}} \nonumber\\
    &\approx \frac{R_S + \frac{1}{g_m}}{A_1} \sqrt{4 k_B T \frac{2}{3} g_m + \frac{K_f I_D}{C_{ox} L^2} \frac{1}{f}} \nonumber\\
    \overset{A_1 \to \infty}&{=} 0\nonumber
\end{align}

Unless the MOSFET transconductance $g_m$ or the gain of the op-amp $A_1$ become very small, the noise of the MOSFET is very well suppressed. This means, that if the wideband thermal noise contribution is small (it is, see \ref{sec:mosfet_noise}) and the flicker noise corner frequency is within the bandwdith of the op-amp, the noise contribution from the MOSFET can be neglected.

The noise contribution from the sense resistor $R_S$ is the (approximated) Johnson–Nyquist noise, which when transformed to its Norton representation can be written as current noise
\begin{equation}
    i_{n,R} = \sqrt{\frac{4 k_B T}{R_S}} \,.
\end{equation}

Additionally, it was shown, that depending on the material of the resistive element, a flicker noise component can also be present. This is especially prevalent in carbon and tick-film resistors \cite{flicker_noise_carbon_film,1_f_noise_thick_film}. While thin-film resistors are less noisy, their performance varies greatly between different models \cite{resistor_current_noise_ligo}, so their make and model must be carefully selected for the application. Foil and wirewound resistors were shown to perform best and have almost no flicker noise \cite{resistor_current_noise_ligo,flicker_noise_foil_resistor_beev}. Using a high quality resistor the flicker noise can be neglected and only the thermal noise must be taken into account.

The sense resistor is part of the feedback network and therefore it contributes fully to the noise of the transimpedance amplifier. Input referred, the current noise must be divided by the closed-loop gain $A_f$ given by \ref{eqn:transfer_function_closed_loop}.
\begin{equation}
    e_{n,R} = i_{n,R} \cdot \beta \approx i_n \cdot R_S = \sqrt{4 k_B T R_S} \label{eqn:noise_sense_resistor}
\end{equation}

The final component to be discussed is the operational amplifier. Although the op-amp is a rather complex device, its noise can be modeled by a small number of noise sources. This noise model of the op-amp is shown in figure \ref{fig:op-amp_noise_model}.

\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{op-amp_noise_model.tex}
    } % scalebox
    \caption{Noise model of the operational amplifier.}
    \label{fig:op-amp_noise_model}
\end{figure}

In figure \ref{fig:op-amp_noise_model} we can see, that there are three noise sources required to treat the op-amp. The input voltage noise $e_{n}$ and two input current noise sources $i_n$. The current noise noise source are assumed to be mostly uncorrelated. This assumption will lead to an upper bound as can seen from figure \ref{fig:op-amp_input_stage}, which shows the the input differential amplifier, that is the first stage of a typical bipolar op-amp.

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{op-amp_input_stage.tex}
    } % scalebox
    \caption{Bipolar op-amp input stage with noise sources.}
    \label{fig:op-amp_input_stage}
\end{figure}

Of the three noise sources $i_{n,p}$ and $i_{n,n}$ are uncorrelated, because it is the input bias current of the individual transistors, and only the effect of $i_{n,EE}$ is correlated, because the current of the emitter bias current souce is equally distributed between the two input transistors. Since effects of equal magnitude and sign cancel out due to the differential nature of the input stage, correlated effects are suppressed. An equal magnitude can be assumed, because the gain of the two transistors is well matched, due to their close proximity on the semiconductor die. Therefore assuming all noise is uncorrelated, presents an upper bound. A more detailed analysis can be found in \cite{op-amp_noise_correlation}, if interested. Due to the matching of the transistors, the magnitude $i_{n,p}$ and $i_{n,n}$ are also closely matched, hence in our model, they are assumed equal.

As we have done before in this section with referring all noise sources to the input, the same was done manufacturer and they are given in the datasheet. The two current noise sources can not be combined into the input voltage noise, because the depend on the external impedances connected to the op-amp. Given the complete circuit as in figure \ref{fig:precision_current_source_noise} it is possible to calculate the full noise contribution of the op-amp
\begin{equation}
    e_{n,op} = \sqrt{e_n^2 + e_{n,+}^2 + e_{n,-}^2}
\end{equation}
given that the noise sources are uncorrelated. The input referred noise $e_{n,-}$ of the inverting input can be calculated in a similar fashion as $e_{n_R}$ in equation \ref{eqn:noise_sense_resistor}. It is likewise part of the feedback network and must therefore be divided by the closed-loop gain $A_f$ as before.
\begin{equation}
    e_{n,-} \approx i_n \cdot R_S
\end{equation}

The current noise of the input can be translated by looking at the input impedance. This will be determined by the output filter of the reference voltage, which is required to remove the high frequency noise as discussed above. Assuming an RC-filter of first order,
the output impedance can be calculated from the transfer function of the low-pass filter, derived in equation \ref{eqn:first_order_model}
\begin{align}
    R_{out,filt} &= R_{filt} \cdot A = \frac{R_{filt}}{1+sR_{filt}C} \nonumber\\
    \lim_{s \to 0} R_{out,filt} &= R_{filt} \nonumber\\
    \lim_{s \to \infty} R_{out,filt} &= 0 \,.\nonumber\\
     e_{n,-} &\approx \frac{i_n R_{filt}}{1+sR_{filt}C}
\end{align}

As it can be seen, for high frequecies, the output impedance goes to \num{0}, while for low frequencies it is $R_{filt}$. If the filter corner frequency $\omega_0 = \frac{1}{RC}$ is close to or at the flicker noise corner frequency of the reference voltage, it means that there is almost no wideband current noise contribution as well. The only the $\frac{1}{f}$ component of the op-amp current noise multiplied with $R_{filt}$ should be lower than the reference noise to have negligible impact.
This leads to the total noise of the op amp
\begin{equation}
    e_{n,op} = \sqrt{e_n^2 + (i_n R_S)^2 + \left|\frac{i_n R_{filt}}{1+sRC}\right|^2} \,.
\end{equation}

To conclude, table \ref{tab:current_source_noise_contributers} is given as a reference for the noise contributions in the low-frequency and also the wideband domain. From this table, it can be seen, that the only wideband-noise contributors are the reference resistor and the op-amp. The low-frequency contributors are the voltage reference and the op-amp, since they have a strong flicker noise component. A low-noise, precision op-amp typically has far less low frequency noise than a voltage reference and the dominant low frequency contributor is the voltage reference.

\begin{table}[ht]
    \centering
    \begin{tabular}{lll}
        Noise component& Low frequency& Wideband \\
        \midrule
        $V_{sup}$ & $\approx 0$ & $\approx 0$\\
        MOSFET & $\approx 0$ & $\approx 0$\\
        $V_{ref}$ & $\sqrt{e_{n,ref}^2 + 4 k_B T R_{filt}} $ & $\approx 0$\\
        $R_S$ & $\sqrt{4 k_B T R_S}$ & $\sqrt{4 k_B T R_S}$\\
        Op-amp & $\sqrt{e_n^2 + i_n^2 (R_S^2 + R_{filt}^2)}$ & $\sqrt{e_n^2 + i_n^2 R_S^2}$
    \end{tabular}
    \caption{Input referred noise components of the transimpedance amplifier. Multiply by $\frac{1}{R_S}$ to get the output referred current noise.}
    \label{tab:current_source_noise_contributers}
\end{table}

\clearpage
\subsection{Component Selection}
\label{sec:component_selection}
%TODO: Fix link to lst:dgDrive_specs_environment and lst:dgDrive_specs_electrical
This section deals with selecting the right components for the precision current source presented in section \ref{sec:precision_current_source}. The focus lies on the requirements defined in section \ref{sec:laser_current_driver}, notably tables \ref{lst:dgDrive_specs_environment} and \ref{lst:dgDrive_specs_electrical}. Most attention will be on the MOSFET, the operational amplifier and the voltage reference. We will start with the voltage reference, because this will define several parameters down the road. Then, the op-amp is discussed, for which several examples from scientific publications and other alternatives are shown and the best solution is presented. Finally, the selection parameters for the MOSFET will be elaborated. The reader must warned though, that the lineup of p-channel MOSFETs in production is decreasing, with more and more products being discontinued in favor of n-channel MOSFETs and the examples may be outdated.

Numerous laser driver designs can be found in literature \cite{libbrecht_hall, laser_driver_mosfet_noise, laser_driver_digital, laser_driver_digital_update, laser_driver_qcl_space, laser_driver_qcl_taubman, laser_driver_qcl_taubman_multiplexer}. While \citeauthor{libbrecht_hall} where not first to present a design and similar design can already be found in \cite{laser_driver_old}, their design stands out for its simplicity. The design in literature can be divided into two groups. High power drivers for quantum cascade lasers (QCL) typically featuring a compliance voltage of more than \qty{10}{\V} and output currents of up to serveral ampere based on the work of \citeauthor{laser_driver_qcl_taubman} and medium power devices for laser diodes having a lower compliance voltage of around \qty{2}{\V} and able to driver a few hundred \unit{\mA} based on the work of \citeauthor{libbrecht_hall}. Our requirements mostly fall into the latter category, except for the compliance voltage, which is targeted to be $\qty{\ge 8}{\V}$. All these drivers share one common aspect, though, the type of voltage reference. Most laser drivers in literature and commercial products are designed around low-noise, low-drift buried Zener diode voltage references, namely the \device{LM399} \cite{datasheet_LM399} or \device{LTZ1000} \cite{datasheet_LTZ1000}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lllll}
        Component& Voltage& Temperature coefficent & Stability& Package \\
        \midrule
        \device{LT1021} & \qty{7}{\V} & \qtyrange[range-units = single]{2}{5}{\uV \per \V \per \K} & \qty[per-mode = symbol, power-half-as-sqrt]{15}{\uV \per \V \per \kilo\hour\tothe{0.5}} & SO-8\\
        \device{LT1027} & \qty{5}{\V} & \qtyrange[range-units = single]{1}{2}{\uV \per \V \per \K} & not specified & SO-8\\
        \device{LM399} & \qty{7}{\V} & \qtyrange[range-units = single]{0.3}{1}{\uV \per \V \per \K} & \qty[power-half-as-sqrt]{8}{\uV \per \V \per \kilo\hour\tothe{0.5}} & TO-46\\
        \device{ADR1399} & \qty{7}{\V} & \qtyrange[range-units = single]{0.2}{1}{\uV \per \V \per \K} & \qty[power-half-as-sqrt]{7}{\uV \per \V \per \kilo\hour\tothe{0.5}} & TO-46\\
        \device{LTZ1000} & \qty{7.2}{\V} & \qty{0.05}{\uV \per \V \per \K} & \qty[power-half-as-sqrt]{0.3}{\uV \per \V \per \kilo\hour\tothe{0.5}} & TO-99\\
        \device{ADR1000} & \qty{6.6}{\V} & \qty{<0.2}{\uV \per \V \per \K} & \qty[power-half-as-sqrt]{0.2}{\uV \per \V \per \kilo\hour\tothe{0.5}} & TO-99
    \end{tabular}
    \caption{List of buried Zener diodes and selected properties.}
    \label{tab:overview_buried_zener_diodes}
\end{table}

The buried types of voltage references are \textit{Zener} diodes, that are created within the bulk silicon using ion implantation. This reduces noise due to surface contamination \cite{zener_diode_stability}. These diodes are not true Zener diodes, but called Zeners nonetheless and use a mix of Zener and avalanche breakdown to compensate the temperature coefficient. The Zener effect is the tunneling of electrons through the barier from the valence band to conduction band. It  has a negative temperature coefficient, because in increase in temperature reduces the size of the bandgap. Avalanche breakdown, on the other hand describes the mechanism, that free electrons (due to temperature) are accelerated to such energies, that they knock out other electrons, causing a avalanche of electrons. This effect has a positive temperature coefficient, because a higher temperature results in more free carriers, that cause leakage but no avalanche. While the zero temperature coefficient point is around \qty{5}{\V}, this operating point implies a high succeptibility to changes in the reverse current. So typically the Zener voltage is shifted slightly upwards to result in a net positive coefficent, which is then compensated by the negative temperature coefficent of a forward biased diode \cite{zener_diode_stability}. This results in the typical Zener diode voltage of around $\qty{6.2}{\V} + \qty{0.7}{\V} = \qty{6.9}{\V}$. In comparison to other types of diodes, buried Zeners have the best stability and lowest noise. In order to achieve high stability and low noise, $V_{ref} \approx \qty{7}{\V}$ is therefore pretty much set in stone. Table \ref{tab:overview_buried_zener_diodes}, lists some commercially available buried Zener diodes. All diodes are manufactured by Analog Devices as they are the sole manufacturer left to produce these kind of diodes.

Choosing a voltage reference can be done according to table \ref{lst:dgDrive_specs_environment}. A temperature coefficent of \qty{<= 1}{\uA \per \A \per \K} rules out any non-hermetic unheated voltage reference. Using a hermetic package improves the stability against humidity as the epoxy used for an SO-8 package is hydrophilic and swells when exposed to water vapour causing pressure on the die, resulting in a change of the output voltage. The hermetic voltage references can be divided into two groups, the \device{LM399} and the newer \device{ADR1399} in one group and the \device{LTZ1000} and its newer counterpart \device{ADR1000} in aother. While the \device{LM399} requires very few external components, the external circuit for the \device{LTZ1000} is far more elaborate requiring more parts and space. Additionally, the \device{LTZ1000} is more than four times the price of the \device{LM399} in quantities of \num{10} at the time of writing. Last but not least, the stability and temperature coefficent of the \device{LTZ1000} cannot be matched by the performance of the sense resistor, so the sense resistor gives a lower bound of about \qty{0.5}{\uA \per \A \per \K}. Unless the better low frequency noise performance is absolutely required, the \device{LM399} and \device{ADR1399} are the more economical parts. The performance of those two references will be discussed in section \ref{sec:zener_diode_selection}.

With the maximum reference voltage of \qty{7}{\V} known, a sense resitor between \qty{14}{\ohm} (\qty{500}{\mA}) and \qty{28}{\ohm} (\qty{250}{\mA} is required. Combined with the requirement for a low noise output, this limits the choice of op-amps to bipolar low-noise devices or discrete implementations. Table \ref{tab:overview_bipolar_op-amps} lists some choices compiled from the literature sources, which will now be discussed.

\begin{table}[ht]
    \centering
    \begin{tabular}{llll}
        Component& Wideband-noise& Low frequency noise & Temperature coefficent \\
        \midrule
        \device{LT1028} & \qty[power-half-as-sqrt]{0.85}{\nV \per \Hz\tothe{0.5}} & \qty{35}{\nV_{p-p}} & \qty{0.2}{\uV \per \K}\\
        \device{AD797} & \qty[power-half-as-sqrt]{0.9}{\nV \per \Hz\tothe{0.5}} & \qty{50}{\nV_{p-p}} & \qty{0.2}{\uV \per \K}\\
        \device{ADA4898} & \qty[power-half-as-sqrt]{0.9}{\nV \per \Hz\tothe{0.5}} & not specified & \qty{1}{\uV \per \K}\\
        \device{ADA4004} & \qty[power-half-as-sqrt]{1.8}{\nV \per \Hz\tothe{0.5}} & \qty{150}{\nV_{p-p}} & \qty{0.7}{\uV \per \K}\\
        \device{AD8671} & \qty[power-half-as-sqrt]{2.8}{\nV \per \Hz\tothe{0.5}} & \qty{77}{\nV_{p-p}} & \qty{0.3}{\uV \per \K}
    \end{tabular}
    \caption{List of low-noise precision bipolar operational amplifiers with typical performance properties.}
    \label{tab:overview_bipolar_op-amps}
\end{table}

The low value of the sense resistor makes a bipolar op-amp the preferred choice, because they have a very low voltage noise and their current noise and input bias current do not interfere with such a low value resistor. While a discrete solution using matched jfets or bipolar transistors may push the input noise even lower, the temperature stability, circuit complexity and again the size speaks against this option, so the disussion will be limited to integrated solutions only. To find a reference point for the choice of op-amp, the thermal noise of the sense resistor must looked at. The \qty{28}{\ohm} sense resistor has a thermal noise of
\begin{equation}
    e_n\left(\qty{23}{\celsius}\right) = \qty[power-half-as-sqrt]{0.67}{\nV \per \Hz\tothe{0.5}} \,. \nonumber
\end{equation}

This means, that even the lowest noise op-amp from table \ref{tab:overview_bipolar_op-amps} dominates the wideband-noise. The \device{AD8671} chosen by \cite{laser_driver_digital} only makes sense, because they have chosen a very large filter resistor $R_{filt}$ of $2 \times \qty{3}{\kilo\ohm}$. The \device{ADA4004} was used by Moglabs in the \device{DLC-202}, again likely due to the high values of $R_{filt}$ used. The \device{ADA4898} might seem like a good choice at first sight but the very limited (in terms of precision op-amps) open-loop gain of \qty{0.14}{\V \per \uV} makes this op-amp a cheap, but poor choice. The final choice is between the \device{AD797} and the \device{LT1028}, both op-amps have very similar specifications, but there is a peculiarity in the datasheet of the \device{LT1028} \cite{datasheet_LT1028}. While there is a current noise spectrum, there is no voltage noise spectrum to be found in the datasheet. The author assumes a good deal of specsmanship at this point. The publication by \citeauthor{libbrecht_hall} already blames the \device{LT1028} for a noise peak around \qty{400}{\kHz}. This peak is also included in the noise models for the op-amp and was additionally confirmed by the author with a measurement. This peak is the reason why \citeauthor{laser_driver_mosfet_noise} found the AD797 to be higher noise than the \device{AD797}. Additionally to the superior noise performance, the \device{AD797B} has excellent specifications overall. The open-loop gain is between \qtyrange[range-units = single]{2}{20}{\V \per \uV}, the supply rejection is greater than \qty{1}{\uV \per \V}, the bias current is almost constant between \qtyrange[range-units = single]{20}{100}{\celsius} and the unity gain bandwidth is around \qty{10}{\MHz}. Finally it does have a very high output drive capability of \qty{50}{\mA}, which allows to drive fairly large MOSFETs. These features make the \device{AD797} the ideal op-amp for those low-value sense resistors, although it puts limits on the maximum filter resistor to limit the low frequency current noise contribution.

Finally, the choice of MOSFETs can be discussed. As it was shown in section \ref{sec:mosfet_current_source} in equation \ref{eqn:mosfet_saturation}, the channel length modulation playes an important role in increasing the channel conductance $g_{DS}$ and limiting the output impedance. To reduce the channel length modulation a longer channel is preferred. Manufacturers do not give these numbers, nor the manufacturing process. Older technologies like the planar (lateral) FET is better suited for operating in the saturation region than the modern trench (vertical) FET. Trench MOSFETs are geared towards a low on-state resistance $R_{DS,on}$, which is important for MOSFETs in switching applications, but their lower resistance comes from a shorter channel. One of the few planar MOSFETs still available on the market is the HEXFET, which was designed for switching applications, but prooves useful nonetheless as we will see. High voltage MOSFETs also have longer channels than low voltage MOSFETs, so browsing for MOSFETs, that are rated for \qtyrange[range-units = single]{60}{100}{\V} or more can narrow down the candidates. While the output impedance is a factor worth keeping in mind, the most important aspect is, whether the MOSFET can drive the load regarding the compliance voltage. To outline the problem, we can again refer to the example parameters from table \ref{tab:current_source_parameters}.

Assuming a supply voltage of \qty{15}{\V} and the \device{AD797} op-amp, the current source supply voltage $V_{sup}$ is then limited to about \qtyrange[range-units = single]{11}{12}{\V}, because the \device{AD797} is no rail-to-rail op-amp and its output only swings to within \qty{3}{\V} of the rail (minimum) and the input is limited to within \qty{2}{\V} of the rail (minimum). Considering the maximum $V_{ref}$ at full output of \qty{7}{\V} and a load voltage of \qty{3}{\V} in case of the \device{L785H1} \cite{datasheet_thorlabs_780nm} used as an example in this section leaves only
\begin{equation}
    V_{DS,min} = V_{sup} - V_{ref} - V_{load} = \qtyrange[range-units = bracket]{11}{12}{\V} - \qty{7}{\V} - \qty{3}{\V} = \qtyrange[range-units = bracket]{1}{2}{\V} \label{eqn:minimum_mosfet_vds}
\end{equation}
for the MOSFET -- a serious challenge.

To find a suitable MOSFET, one has to consult the \textit{Typical Output Characteristics} graph in the datasheet. Using the maxium output current specifaction it is possible to estimate the minimum drain-source voltage $V_{DS}$ to keep the MOSFET in saturation at the given maxium output current. This again narrows down the list of candidates.

The final aspect is the capacitive nature of the MOSFET gate. This property was brushed in appendix \ref{sec:mosfet_noise} and the parasitic capacitances can be found in figure \ref{fig:mosfet_parasitic_capacitors}. The \device{AD797} can drive fairly large capacitive loads and several hundred \unit{\pF} are possible. It is best to keep the input capacitance $C_{iss}$ below \qty{500}{\pF}. Do remember the output impedance of the \device{AD797}, is about \qty{10}{\ohm} at \qty{1}{\MHz} and rising by an order of magnitude at \qty{10}{\MHz}. The \qty{500}{\pF} results in an impedance of around \qty{300}{\ohm} dropping by an order of magnitude at \qty{10}{\MHz}, so keeping capacitance low, allows a higher bandwidth of the current source.

Using these guidelines, searching a MOSFET across a lot of manufactures can still be tedious, but, for example, the distributor Digikey allows filtering and sorting by voltage and input capacitance. The follwing MOSFETs in table are given as an example and can be chosen for their respective current ranges.

\begin{table}[ht]
    \centering
    \begin{tabular}{llll}
        MOSFET& Maximum $V_{DS}$& Input capacitance $C_{iss}$ & Current range \\
        \midrule
        \device{IRF9610} & \qty{200}{\V}& \qty{170}{\V} & \qtyrange[range-units = single]{100}{250}{\mA}\\
        \device{IRF9Z10} & \qty{50}{\V}& \qty{270}{\V} & \qtyrange[range-units = single]{250}{500}{\mA}\\
        \device{IRF9Z14} & \qty{60}{\V}& \qty{270}{\V} & \qtyrange[range-units = single]{250}{500}{\mA}
    \end{tabular}
    \caption{Example MOSFETs for a current source and recommended current ranges.}
    \label{tab:example_mosfet_selection}
\end{table}

The current range of the MOSFETs in table \ref{tab:example_mosfet_selection} is given based on the datasheet, making sure, that the MOSFET can be biased into saturation for the estimated minimum $V_{DS}$ according to \ref{eqn:minimum_mosfet_vds}. The \device{IRF9Z10} is a lower voltage version of the \device{IRF9Z14} and the \device{IRF9Z14} should be preferred if available. Those MOSFETs starting with \textit{IRF} are all HEXFETs formerly made by International Recitfier, whose MOSFET business was bought by Vishay in 2007.

\clearpage
\subsection{Current Source Example Parameters}
\label{sec:current_source_summary}
Throughout this section, example calculations are performed to give the reader an idea of real-life parameters derived from the theoretical models. These parameters are summarized in table \ref{tab:current_source_parameters}, including their origin.

\begin{table}[ht]
    \centering
    \begin{tabular}{lll}
        Parameter& Value& Source \\
        \midrule
        MOSFET drain current $I_D$ & \qty{250}{\mA} & \device{L785H1} \cite{datasheet_thorlabs_780nm}\\
        MOSFET $\kappa$ & \qty[per-mode=power]{0.813}{\ampere \per \square\volt} & \device{IRF9610} SPICE model \cite{irf9610_spice}\\
        MOSFET channel length modulation $\lambda$ & \qty[per-mode=power]{4}{\per \milli \volt} & \device{IRF9610} SPICE model \cite{irf9610_spice}\\
        MOSFET source voltage & \qtyrange{3.5}{4}{\V} & section \ref{}\\
        Source/Sense Resistor $R_S$ & \qty{30}{\ohm} or \qty{50}{\ohm} & section \ref{}\\
        Op-amp differential input impedance $R_{id}$ & \qty{7.5}{\kilo\ohm} & \device{AD797} \cite{datasheet_AD797}\\
        Op-amp open-loop gain $A_{ol}$ & \qty[per-mode=power]{2}{\volt \per \uV} & \device{AD797} \cite{datasheet_AD797}\\
        Op-amp gain bandwidth product $GBP$ & \qty{10}{\MHz} & \device{AD797} \cite{datasheet_AD797}
    \end{tabular}
    \caption{Parameters used throughout this section and their sources.}
    \label{tab:current_source_parameters}
\end{table}

\clearpage
\section{Temperature Controller}
\label{sec:temperature_controller}

% include Investigation of Long-Term Drift of NTC Temperature Sensors with less than 1 mK Uncertainty

\subsection{Tuning of a PID controller}
The number of empirical algorithms to determine a set of PID parameters ($\mathrm{k_p, k_i, k_d}$) are numerous. In this work only the most common algorithms and a few notable exceptions will be presented.
\subsection{Design}
