\chapter{Preparation}
\section{Grounding and Shielding}
Add parts from "references\\Grounding and Shielding.pdf"
\section{Laser Current Driver}
% Include Emission wavelength dependence of characteristic temperature of InGaN laser diodes
% Check Diode Laser Characteristics
% I-lamda in Wavelength Dependence of InGaN Laser Diode Characteristics
% also Determination of piezoelectric fields in strained GaInN quantum wells using the quantum-confined Stark effect
Laser diodes are current driven devices, because
\begin{equation}
    P_{out} \propto I\,, \nonumber
\end{equation}

and the diode current $I$ approximately follows the Shockley equation \cite{shockley_diode}
\begin{equation}
    I = I_0 \left( e^{\frac{qV_d}{k_B T}} - 1\right) \, .
\end{equation}

$k_B$ is Boltzmann constant, $T$ the temperature, $q$ the electron charge and $V_d$ the diode voltage. The exponential dependence of the current on the the supply voltage calls for a current source to drive a laser diode safely, without risking thermal damage.

The primary function of a laser driver is to provide a stable, but user adjustable, current. The user adjustable current can typically be modulated at frequencies up to serveral \unit{\MHz} to shape the frequency and ampltitude of the laser beam. Additional features like current and voltage limits aid in protecting the expensive laser diodes. This section deals with the design challenges of such a device used for high precission laser spectroscopy. First the design requirements are derived and then technical specifications are developed.

The focus of this work many lies on two types of laser diodes, indium gallium nitride (InGaN) and aluminium gallium arsenide (AlGaAs), but is not limited to those two types. The former material is used for blue laser didoes at around \qty{450}{\nm}, but also cover up to green wavelengths, and the latter for near-infrared laser diodes at \qty{780}{\nm}, both wavelengths used for experiments in this group.

The design requirements are split into four parts, that need to be discussed. The ambient environment, the diode voltage and current requirements, the modulation bandwith and finally the noise specifications.

\clearpage
\subsection{Design Goals: Ambient Environment}
The laser driver is to be used in a clean laboratory environment. Typical lab temperatures are in the range of \qtyrange{20}{25}{\celsius} and ware mostly met in our labs, before improvements were implemented as part of this work. Humidity is only controlled with dehumidifiers and therefore in the range of \qtyrange{15}{60}{\percent rH}. The air is typically filtered using H14 HEPA filters. Figure \ref{fig:lab_temperature_start_of_project} shows a typical \qty{1}{d} span of the lab temperature as it was found at the start of this project.

\begin{figure}[ht]
    \centering
    \input{images/temperature_011_2016.pgf}
    \caption{Temperature in Lab 011 on 2016-11-26.}
    \label{fig:lab_temperature_start_of_project}
\end{figure}

As it can be seen there are strong oszillations of the temperature as a result of the on–off air conditioning temperature controller. The commercial controller used back then was realized using an IMI Heimeier EMO T Valve \cite{datasheet_heimeier_emo_t}, which is a 2 step valve. Altough this solution was later replaced by a custom design described in section \ref{}, these type controllers are found in many other labs and temperature swings of \qty{2}{\kelvin} must therefore be expected.

These expected environmental parameters can now be used to estimate the design requirements for the laser driver. The more demanding laser system is the \qty{450}{\nm} system \cite{thesis_baus} required for the spectroscopy of highly charged ions \cite{thesis_alex} at GSI. This system was found to be more susceptible to changes of the drive current since the wavelength selective filter element was far broader in comparison to a \qty{780}{\nm} system \cite{two_filter_paper}. This laser is stable over regions of tens of \unit{\uA} and requires a maximum drive current of \qty{145}{\mA} \cite{datasheet_osram_pl450b}. From these values, the driver should be able to supply at least \qty{150}{\mA} and stay well within \qty{10}{\uA} over the whole environmental range. Assuming the worst-case specifications a tolerance of $3\sigma$ (\qty{99.7}{\percent}) must be met \cite{worst_case_design}.

The environmental parameters that mostly affect current sources are temperature and humidity. Air pressure is typically a matter of concern for high voltage systems \cite{IPC-2221B} and secondary to consider for this design as it is a low voltage system (\qty{<= 48}{\V}). Air pressure effects are also the most expensive to test for as a pressure chamber is required, therefore this will not be specified. While humidity does affect electronics due to corrosion and also indirectly because the epoxy used in the PCB and component moulding is hygroscopic and the absorbed humidty leads to swelling and mechanical stress \cite{epoxy_humidity}. This effect is very slow at ambient temperature and can easily take days \cite{epoxy_humidity}. This parameter is therefore included in the long-term stability and not specified separately.

it changes  of close to \qty[per-mode = symbol]{1}{\uA \per \A} is required, preferably better than that. The long-term stability should be less than \qty[per-mode = symbol]{10}{\uA \per \A} to allow open-loop adjustment of the laser.

This leads to the following design specifications regarding the environmental conditions:

\begin{center}
    \begin{tcolorbox}[
        colback=red!5!white,
        colframe=red!75!black,
        title=Current controller environmental design specifications,
        width=0.8\linewidth,
        ]
    \begin{itemize}
        \item Temperature range \qtyrange{20}{25}{\celsius}
        \item Temperature coefficient \qty[per-mode = symbol]{<= 1}{\uA \per A}
        \item Humidty (non-condensing) \qty{<= 75}{\percent rH}
        \item Humidty coefficient not specified, but included in the long-term drift
        \item Maximium altitude not specified
        \item Long-term drift \qty[per-mode = symbol]{<= 10}{\uA \per \A}
    \end{itemize}
    \end{tcolorbox}
\end{center}


A basic laser current driver design, that has some of the  can be found in the work of \citeauthor{libbrecht_hall} \cite{libbrecht_hall}. While this design contains all the basic features, like a current source, a modulation input and a voltage limit, there are several shortcomings that have emerged over the years with new generations of laser didoes. The laser driver used by legacy applications in this group is based on the aforementioned paper and has been successfully employed in several projects over the years, but several limitations have come up in recent years. In order to derive the design requirements of a new generation of laser drivers the important design elements need to be identified first. The essential design elements are the bulk current source, the modulation current source, the reference element and output programming. The next 4 sections will deal with each element and outline the design goals that were identified while employing the legacy generation of diode drivers in several experiments.

\subsection{Design Goals: Current Source}
An idealized current source as shown in figure \ref{fig:ideal_current_source} has two properties. The output current $I$ and the output impedance $R_\mathrm{out}$. Ideally, $R_\mathrm{out}$ is infinite and all current is forced to flow through the load.

\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{current_source_norton.tex}
    } % scalebox
    \caption{An ideal current source with output impedance $R_\mathrm{out}$.}
    \label{fig:ideal_current_source}
\end{figure}

\subsubsection{Simulation}
\paragraph{Op Amp Stability}
\subsection{Noise Considerations}
\subsection{Voltage Reference}
\subsection{MOSFET Selection}

\clearpage
\section{LabKraken}
\subsection{Design Goals}
LabKraken is a designed to be a asynchronous, resilient data aquisition suite, that scales to thousands of sensors and accross different networks.
\subsection{Hardware}
\subsection{Software Architecture}
LabKraken needs to scale to thousands of sensors, which need to be served concurrently. This problem is commonly referered to as the C10K problem as dubbed by Dan Kegel back in 1999 \cite{10kProblem} and refers to serving \num{10000} concurrent connections via network sockets. While today millions of concurrent connections can be handled by servers, handling \num{10000} can still be challenging, especially, if the data sources are heterogeneous as is typical for sensor networks of different sensors from different manufacturers.

In order to meet the design goals, an asynchronous architecture was chosen and several different architectures were implemented over time. All in all four complete rewrites of the software were made to arrive at the architecture presented here. The reason for the rewrites is mostly historic and can be explained by the history of the programming language Python, which was used to write the code. The first first version was written for Python 2.6 and exclusively supported sensors made Tinkerforge. In 2015, Python 3.5 was released, which supported a new syntax for asynchronous coroutines. The software was rewritten from scratch to support this new syntax, because it made the code a lot more verbose and easier to follow. With the release of Python 3.7 in 2018 asynchronous generator expressions where mature enough to be used in productions and the programm was again rewritten to use the new syntax. In 2021 a new approach was taken and the programm was once more rewritten with a functional programming style. I will discuss each approach in the next sections to highlight the improvements, that were made over time. Each of these sections discusses the same programm, but written in different styles to show the differences.

\subsubsection{Threaded Design}
The first version of LabKraken used a threaded design approach, because the original libraries of the Tinkerforge sensors are built around threads. The following simplified example shows some code to connect to a temperature sensor over the network and read its data.

\inputpython{source/lab_kraken_threads.py}{1}{26}

\subsubsection{Device Identifiers}
Every sensor network needs device identifiers. Preferably those identifiers should be unique. Typically a device has some kind of internal indetifier. Here are a few examples of the sensors used in our network:

\begin{table}[ht]
\centering
\begin{tabularx}{0.95\textwidth}{|l|p{6.5cm}|X|}
    \hline
    Device Type& Identifiers& Example\\
    \hline
    GPIB (SCPI)& \textit{*IDN?} returns \newline \$manufacturer,\$name,\$serial,\$revision& \\
    \hline
    Tinkerforge& Each sensor has a base58 encoded integer device id& QE9 (163684)\\
    \hline
    Labnode& Universal Unique Identifier (UUID) & cc2f2159-e2fb-4ed9-\newline8021-7771890b37ad\\
    \hline
\end{tabularx}
\end{table}

As it can be seen above, these identifiers do not guarantee to uniquely identify a device within a network. The Tinkerforge id is the weakest, as it is a \qty{32}{\bit} integer (4.294.967.295 options), which might easily collide with another id from a different manufacturer. The tinkerforge id is presented as a base58 encoded string. An encoder/decoder example can be found in the TinkerforgeAsync library \cite{TinkerforgeAsync}.

The id string returned by a SCPI device is slightly better, but again does not guarantee uniqueness. As it is shown in the example the same device might return a different id defpending on its settings. This typically done by manufacturers for compatibility reasons.

The only reasonably unique id is the universal unique identifier (UUID) or globally unique identifier (GUID), as dubbed by Microsoft, used in the Labnodes. Their id can be used for networks with participant numbers going into the millions.

Calculating the probability of a collision between two random UUIDs is called the birthday problem \cite{BirthdayProblem} in probability theory. A randomly generated version 4 UUID of variant 1 as defined in RFC 4122 \cite{RFC-UUID} has \qty{122}{\bit} of entropy, that is out of \qty{128}{\bit}, \qty{4}{\bit} are reserved for the UUID version and \qty{2}{\bit} for the variant. This gives the probability of at least one collision in $n$ devices out of $M = 2^{122}$ possibilities:
\begin{align}
    p(n) &= 1 - 1 \cdot \left(1 - \frac{1}{M}\right) \cdot \left(1 - \frac{2}{M}\right) \dots \left(1 - \frac{n-1}{M}\right) \nonumber\\
    &= 1 - \prod_{k=1}^{n-1} \left(1 - \frac{k}{M} \right)
\end{align}
Using the Taylor series $e^x = 1+x \dots$, assuming $n \ll M$ and approximating we can simplify this to:
\begin{align}
    p(n) &\approx 1 - \left(e^\frac{-1}{M} \cdot e^\frac{-2}{M} \dots e^\frac{-(n-1)}{M} \right) \nonumber\\
    &\approx 1 - \left(e^\frac{-n(n-1)/2}{M} \right) \nonumber\\
    &\approx 1 - \left(1 - \frac{n^2}{2 M} \right) = \frac{n^2}{2 M}
\end{align}
For one million devices, this gives a probability of about \num{2e-25}, which is negligible.

In the Kraken implementation, all devices, except for the Labnodes, will be mapped to UUIDs using the underlying configuration database. It is up to the user to ensure the uniqueness of the non-UUID ids reported by the devices to ensure proper mapping.


\subsubsection{Limitations} % FIXME: Different title
There is one inherent limitation to the ethernet bus for instrumentation. The ethernet bus is inherently asynchronous and multiple controllers can talk to the device at the same time. Not only that, but different processes within the same controller can talk to the same device. This makes deterministic statements about the device state challenging.

While it is impossible to rule out the possibility of multiple controllers on a network, care was taken to synchronize the workers within Kraken.
\subsection{Databases}
\subsubsection{Cardinality}
\begin{itemize}
 \item TimescaleDB vs Influx
 \item Example Sensors vs. Experiment
\end{itemize}

\clearpage
\section{Short Introduction to Control Theory}
This section will give a very brief introduction into some basic concepts of control theory. Many systems require control over one or more process variables. For example, temperature control of a room or a device, or creating a current from a voltage. All of this requires control over a process and is established trough feedback, which allows a controller to sense the state of the system.

The focus of this section lies on the principels feedback and control and will be detailed in the following sections.

\subsection{Transfer Functions}

\subsection{Open and Closed Loop Systems}
To understand feedback, one needs to take a look at dynamical systems. There are two types of systems: open and closed loop systems. A system is called open loop, if the output of a system does not influece its input as in figure \ref{fig:open_loop}. On the other hand, if the output is connected to the input of the system it is called closed loop system, an example is shown in figure \ref{fig:closed_loop}. $G(s)$ is called the transfer function of the system, while $R(s)$ is the input, $Y(s)$ is the output and $s$ the Laplace variable.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \import{figures/}{open_loop.tex}
        \caption{Open loop system.}
        \label{fig:open_loop}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \import{figures/}{closed_loop.tex}
        \caption{Closed loop system.}
        \label{fig:closed_loop}
    \end{subfigure}
    \caption{Block diagram of closed and open loop systems.}
\end{figure}

It is convenient to express the transfer function as its Laplace transform. The unilateral Laplace transform is definded as:
\begin{equation}
    \mathscr{L}\left( f(t) \right) = F(s) = \int_0^\infty f(t) e^{-st}\,dt.
\end{equation}

with $f: \mathbb{R}^+ \to \mathbb{R}$, that is integrable and grows no faster than $e^{s_0t}$ for $s_0 \in \mathbb{R}$. The latter property is important for deriving the rules of differentiation and integration.

To understand the benefits of using the Laplace representation for transfer function a few useful properties must be discussed. First of all the Laplace transform is linear:
\begin{align}
    \mathscr{L}\left(a \cdot f(t) + b \cdot g(t) \right) &= \int_0^\infty (a \cdot f(t) + b \cdot g(t)) e^{-st}\,dt \nonumber\\
    &= a \int_0^\infty f(t) e^{-st}\,dt + b \int_0^\infty g(t) e^{-st}\,dt \nonumber\\
    &= a \mathscr{L}\left(f(t)\right) + b \mathscr{L}\left(g(t)\right)
\end{align}

Another interesting property is the derivative and integral of a function $f$:

\begin{align}
    \mathscr{L}\left(\frac{df}{dt}\right) &= \int_0^\infty \underbracket{f'(t)}_{v'(t)} \underbracket{\vphantom{f'(t)}e^{-st}}_{u(t)}\,dt \nonumber\\
    &= \left[e^{-st} f(t) \right]_0^\infty - \int_0^\infty (-s)f'(t)\,dt \nonumber\\
    &= -f(0) + s \int_0^\infty f'(t)\,dt \nonumber\\
    &= s F(s) - f(0)
\end{align}

\begin{align}
    \mathscr{L} \left( \int_0^t f(\tau)\,d\tau \right) &= \int_0^\infty \left(\int_0^t f(\tau)\,d\tau e^{-st} \right)\,dt \nonumber\\
    &= \int_0^\infty \underbracket{e^{-st}\vphantom{\int_0^t}}_{v'(t)} \underbracket{\int_0^t f(t)\,d\tau}_{u(t)}\,dt \nonumber\\
    &= \left[\frac{-1}{s} e^{-st} \int_0^t f(t)\,d\tau \right]_0^\infty - \int_0^\infty \frac{-1}{s} e^{-s\tau} f(\tau)\,d\tau \nonumber\\
    &= 0 + \frac{1}{s} \int_0^\infty e^{-s\tau} f(\tau)\,d\tau \nonumber\\
    &= \frac{1}{s} F(s) \label{eqn:lapace_integration}
\end{align}

If the initial state $f(0)$ can be chosen to be $0$, the differentiation becomes a simple multiplication by $s$, while the integration becomes a division by $s$. Finally, the most important aspect is, that a simple relation between the input $r(t)$ and the ouput $y(t)$ of a system can be given. The relation between input and the ouput of a system as shown in figure \ref{fig:open_loop} is given by the convolution, see e.g. \cite{pid_basics}. Assuming the system has an initial state of $0$ for $t<0$, hence $r(t<0) = 0$ and $g(t<0) = 0$, one can calculate:

\begin{equation}
    y(t) = (r \ast g)(t) = \int_0^\infty r(\tau) g(t-\tau)\,d\tau
    \label{eqn:convolution}
\end{equation}

Applying the Laplace transformation, greatly simplifies this:
\begin{align}
    Y(s) &= \int_0^\infty e^{-st} y(t)\,dt \nonumber\\
    \overset{\ref{eqn:convolution}}&{=} \int_0^\infty \underbrace{e^{-st}}_{e^{-s(t-\tau)}e^{-s\tau}} \int_0^\infty r(\tau) g(t-\tau)\,d\tau\,dt \nonumber\\
    &= \int_0^\infty \int_0^t e^{-s(t-\tau)} e^{-s\tau} g(t-\tau) r(\tau)\,d\tau\,dt \nonumber\\
    &= \int_0^\infty e^{-s\tau} r(\tau)\,d\tau \int_0^\infty e^{-st} g(t)\,dt \nonumber\\
    &= R(s) \cdot G(s)
\end{align}

This formula is a lot simpler than the convolution of $r(t)$ and $g(t)$, therefore the use of the Laplace transform has become very popular in control theory.

Another property that is heavily used in control theory is the time delay of functions. To show this property, let $f(t-\theta)$ be
\begin{equation}
    g(t) \coloneqq \begin{cases} f(t-\theta), & t \geq \theta \\ 0, & t < \theta \end{cases} \label{eqn:delayed_f}
\end{equation}

The reason for this definition is, that the system must be causal. This means, it is impossible to get data from the future ($t<\theta$). An example is shown in figure \ref{fig:heaviside}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.4\linewidth}
        \scalebox{0.75}{%
            \import{figures/}{laplace_no_delay.tex}
        } % scalebox
        \caption{Original signal $f(t)$.}
        \label{fig:heaviside}
    \end{subfigure}
    \begin{subfigure}{0.4\linewidth}
        \scalebox{0.75}{%
            \import{figures/}{laplace_time_delay.tex}
        } % scalebox
        \caption{Delayed signal $f(t-2)$.}
        \label{fig:heaviside_delayed}
    \end{subfigure}
\end{figure}

The Laplace transform of a delayed signal can be calculated as follows:

\begin{align}
    \mathscr{L}\left( g(t) \right) &= \int_0^\infty f(t-\theta) e^{-st}\,dt \nonumber\\
    \overset{\ref{eqn:delayed_f}}&{=} \int_\theta^\infty f(t-\theta) e^{-st}\,dt \nonumber\\
    \overset{u \coloneqq t-\theta}&{=} \int_0^\infty f(u) e^{-s(u+\theta)}\,du \nonumber\\
    &= e^{-s\theta} \int_0^\infty f(u) e^{-su} \nonumber\\
    &= e^{-s\theta} F(s) \label{eqn:laplace_delayed}
\end{align}

To satisfy the causaulity requirement, the Heaviside function $H(t)$ can be used:
\begin{align}
    \mathscr{L}\left( f(t-\theta) H(t-\theta) \right) = e^{-s\theta} F(s) \label{eqn:laplace_causality}
\end{align}

Lastly, the Laplace transform of $e^{at}$, which is commonly used in differential equations:
\begin{align}
    \mathscr{L}\left(e^{at} \right) &= \int_0^\infty e^{(a-s)t}\,dt = \frac{1}{a-s} \left[e^{(a-s)t} \right]_0^\infty = \frac{1}{s-a} \label{eqn:laplace_exponential}
\end{align}


Using these tools, it is possible calculate the transfer function of a temperature controller. This is done in the next section.

\subsection{A Model for Temperature Control}
\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{first_order_model.tex}
    } % scalebox
    \caption{Simple temperature model of a generic system.}
    \label{fig:first_order_model_room}
\end{figure}

In order to describe a closed-loop system, one has to first create a model for the process and the controller involved. A simple model can be derived from the idea, that the system at temperature $T_{system}$ has a thermal capacitance $C_{system}$, an influx of heat $\dot Q_{load}$ from a thermal load and a controller removing heat from the system through a heat exchanger with a resistance of $R_{force}$. Additionally, there is some leakage through the walls of the system to the ambient environment via $R_{leakage}$. This analogy of thermodynamics with electrondynamics allows to create the model shown in figure \ref{fig:first_order_model_room}. Since this this model is to be used for a room temperature controller, an assumption to simplify it can be made.

Typically, the room temperature is kept constant. Therefore, the controller will keep $T_{system}$ constant and if the outside temperature and the heat load $\dot Q_{load}$ is \textit{reasonably stable}, it is easy to see, that a constant thermal flux must flow through $R$ since it cannot pass through the thermal capacitance $C$. \textit{Reasonably stable} means that those fluxes can be treated as constant with respect to the temperature controller time constants. This will be further discussed in section \ref{} with regards to system stability. If this assumption holds, the thermal flux from the system load will only cause a constant offset of $T_{force}$, since the heat must be removed by the controller through the resistance $R_{force}$, and the model can be simplified further. Here $T_{force}$ and $T_{system}$ was replaced by $T_{in}$ and $T_{out}$ for better readability.:

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{first_order_model_kirchhoff.tex}
    } % scalebox
\end{figure}

This is the classic $RC$ circuit. Now, neglecting the constant thermal flux from the system load and exploiting the analogy of thermodynamics and electrondynamics again, using Kirchhoff's second law, one finds:

\begin{align}
    \sum T_i &= 0 \nonumber\\
    T_{in}(t) - \dot{Q}(t) R - \frac 1 C \int \dot{Q}(t)\,dt &= 0 \label{eqn:first_order_model_kirchhoff}
\end{align}

Taking the Laplace transform, applying equation \ref{eqn:lapace_integration} and using $T_{out} = \frac{1}{sC} \dot Q(s)$ to replace $\dot Q$, equation \ref{eqn:first_order_model_kirchhoff} can be written as:
\begin{align*}
    T_{in}(s) - \dot{Q}(s) R - \frac{1}{sC} \dot{Q}(s) &= 0\\
    \dot{Q}(s) = \frac{T_{in}(s)}{R-\frac{1}{sC}} &= \frac{T_{out}}{\frac{1}{sC}}
\end{align*}

This allows to calculate the transfer function of the process $P$:
\begin{align}
    P(s) &= \frac{T_{out}}{T_{in}} = \frac{\frac{1}{sC}}{R-\frac{1}{sC}} \nonumber\\
    &= \frac{1}{sRC + 1} \nonumber\\
    &= \frac{K}{1 + s\tau} \label{eqn:first_order_model}
\end{align}
with the system gain $K$ and the time constant $\tau$. In case of the $RC$ circuit, the gain is $1$, but other systems may have a gain or attenuation factor of $K \neq 1$ in the sensor.

Equation \ref{eqn:first_order_model} is called the transfer function of a first-order model, because its origin is a differential equation of first order. This model describes homogeneous systems, like a room, very well, as can be seen in section \ref{}, but in order to derive the transfer function including the controller and the sensor some more work is required on the sensor transfer function.

Expanding on figure \ref{fig:closed_loop} and equation \ref{eqn:convolution} the closed-loop transfer function becomes:
\begin{equation}
    G(s) = P(s) \cdot S(s)
\end{equation}

and the block diagram becomes

\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{open_loop_full.tex}
    }% scalebox
    \caption{Open loop system with sensor.}
\end{figure}

The transfer funciton of the sensor can be modeled as a delay line with delay $\theta$ and $f(t-\theta) = H(t-\theta)$. A gain of $1$ is assumed here, because any system gain is already included in the parameter $K$. Using equation \ref{eqn:laplace_delayed} $S(s)$ can be written as
\begin{equation}
    S(s) = e^{-\theta s} .
\end{equation}

The full process model including the time delay is:
\begin{equation}
    G(s) = \frac{K}{1 + s\tau} e^{-\theta s} \label{eqn:first_order_plus_dead_time_model}
\end{equation}

This is called a first-order plus dead-time model (FOPDT) or first-order plus time-delay model (FOPTD). To fit experimental data to this model it is more convenient to transform the transfer function \ref{eqn:first_order_plus_dead_time_model} into the time domain. To calculate the output response an input $U(s)$ is required. In principal any function can do, but a step function is typically used, for example by \citeauthor{ziegler_nichols} \cite{ziegler_nichols} and many others \cite{tuning_rules,pessen_integral,simc,smic2,pid_controllers_for_time_delay_systems,pi_stabilization_of_fopdt_systems, pid_basics}. It is both simple to calculate and apply to a real system. Using equations \ref{eqn:laplace_delayed} and \ref{eqn:laplace_exponential}, the Heaviside $H(t)$ step function transforms as
\begin{equation}
    \mathscr{L} \left(u(t) \right) = U(s) = \mathscr{L} \left( \Delta u H(t) \right) = \frac{\Delta u}{s}
\end{equation}

with the step size $\Delta u$. The output $Y(s)$ can then be calculated analytically.

\begin{align}
    Y(s) &= \frac{\Delta u}{s} \frac{K}{1 + s\tau} e^{-\theta s} \nonumber\\
    &=  K \Delta u \frac{1}{s (1 + s\tau)} e^{-\theta s} \nonumber\\
    &= K \Delta u \left(\frac{1}{s} - \frac{\tau}{s\tau+1} \right) e^{-\theta s} \nonumber\\
    &= K \Delta u \left(\frac{1}{s} - \frac{1}{s+\frac{1}{\tau}} \right) e^{-\theta s}
\end{align}

To derive $y(t)$, the inverse Laplace transform of $Y(s)$ is required. Unfortunately, this is not as simple as the Laplace transform. Fortunately, using \ref{eqn:laplace_exponential} while making sure causaulity is guaranteed as shown in \ref{eqn:laplace_causality}, the simple first order model can easily be transformed back into the time domain.

\begin{align}
    \mathscr{L}^{-1} \left(Y(s)\right) = y(t) &= K \Delta u \mathscr{L}^{-1} \left(\frac{1}{s} e^{-\theta s} \right)  - K \mathscr{L}^{-1} \left( \frac{1}{s+\frac{1}{\tau}} e^{-\theta s} \right) \nonumber\\
    \overset{\ref{eqn:laplace_exponential}}&{=} K \Delta u \cdot 1 \cdot H(t-\theta) - \left(e^{-\frac{t-\theta}{\tau}} \right) H(t-\theta) \nonumber\\
    &= K \Delta u \left(1- e^{-\frac{t-\theta}{\tau}} \right) H(t-\theta)
\end{align}

The time domain solution of the FOPDT model can now be used extract the parameters $\tau$, $\theta$ and $K$ from a real physical system using a fit to the measurement data. The parameter $\Delta u$ is already known, since it is an input parameter. A simulation of the step response of a first-order model with time delay is shown in figure \ref{fig:fopdt}. Here it can be clearly seen, that the output does not change until the time delay $\theta$ has passed and the Heaviside function changes from $0$ to $1$.

\begin{figure}[ht]
    \centering
    \input{images/FOPDT_theory.pgf}
    \caption{Time domain plot of a first-order plus dead time model showing individual components of the model and the composite function $y(t)$. Model parameters used: $K= \Delta u = 1$, $\tau=2$, $\theta=4$.}
    \label{fig:fopdt}
\end{figure}

%\cite{pi_stabilization_of_fopdt_systems}



% https://apmonitor.com/pdc/index.php/Main/FirstOrderPlusDeadTime

\clearpage
\subsection{PID tuning rules}

%\subsubsection{SIMC}
We use $\tau_c = \tau$ as suggested by \cite{simc,smic2} for “\textit{tightest possible subject to maintaining smooth control}“.

\clearpage
\section{Allan Deviation}
The Allan variance \cite{adev} $\sigma_A^2(\tau)$ is a two-sample variance and used as a measure of stability. The Allan deviation $\sigma_A(\tau)$ is the square root of the variance. Originally, the Allan variance was used to quantify the performance of oscillators, namely the frequency stability, but it can be used evaluate any quantity. In order to define the Allan variance, a few terms need to be defined first. A single measurement value of the time series $y(t)$ can be written as
\begin{equation}
    \bar y_k(t) = \frac{1}{\tau} \int_{t_{k}}^{t_{k}+\tau} y(t)\,dt . \label{eqn:allan_variance_measurement}
\end{equation}
This is the $k$-th measurement with a measurement time or integration time $\tau$. The latter term is frequently used for DMMs. $t_k$ is the start of the $k$-th sampling inverval including the dead time $\theta$
\begin{equation}
    t_{k+1} = t_k + T
\end{equation}
with
\begin{equation}
    T \coloneqq \tau + \theta .
\end{equation}

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{allan_variance_definitions.tex}
    }% scalebox
    \caption{Measurement interval according to equation \ref{eqn:allan_variance_measurement}}
    \label{fig:allan_variance_definitions}
\end{figure}

Using this, the deviation over $N$ samples is defined as \cite{adev,psd_to_adev}
\begin{equation}
    \sigma_y^2(N,T,\tau) = \left\langle \frac{1}{N-1} \left(\sum _{k=0}^{N-1}\bar y_k^2(t)-\frac{1}{N}\left(\sum _{k=0}^{N-1} \bar y_k(t)\right)^2\right)\right\rangle
\end{equation}
The $\langle \; \rangle$ denotes the (infinite time) average over all measurands $y_k$ or expected value.

The Allan variance is a special case of this definition with zero dead-time ($\theta=0$) and only 2 samples:
\begin{align}
    \sigma_A^2(\tau) &= \sigma_A^2(N=2,T=\tau,\tau) \label{eqn:allan_coefficients}\\
    &= \left\langle \frac{\left(\bar y_{k+1} - \bar y_k \right)^2}{2} \right\rangle
\end{align}
It can be shown \cite{psd_to_adev}, that \ref{eqn:adev_estimator} is indeed more useful than $\sigma_A^2(N\to\infty,T=\tau,\tau)$, because $\sigma_A^2(N=2,T=\tau,\tau)$ converges for processes, that do not have a convergent $\sigma_A^2(N\to\infty,T=\tau,\tau)$.

In practice, no experiment can take an infinite number of samples, so typically the Allan variance is estimated using a number of samples $m$:
\begin{equation}
    \sigma_A^2(\tau) \approx \frac1 m \sum_{k=1}^m \frac{\left(\bar y_{k+1} - \bar y_{k} \right)^2}{2} \label{eqn:adev_estimator}
\end{equation}
This esitmation can lead to artifacts in the results as discussed later. In order to derive the Allan variance from a set of data points, the different values of $\tau$ are usually obtained by averaging over a number of samples since there is no dead time.

Additionally, the Allan variance is mathematically related to the two-sided power spectral density $S_y(f)$ \cite{psd_to_adev}:
\begin{equation}
    \sigma_A^2(\tau) = 2 \int_0^\infty S_y(f) \frac{\sin^4\left( \pi f \tau \right)}{(\pi f \tau)^2}\,df \label{eqn:psd_to_adev}
\end{equation}

and therefore all processes, that can be observed in the power spectral density can also be seen in the allan deviation. The inverse transform, however, is not always possible as shown by \citeauthor{inverse_adev} \cite{inverse_adev}.

Distinguishing different noise processes using the Allan deviation will be elaborated in the next section.

\subsection{Identifying Noise in Allan Deviation Plots}
It was already mentioned by \citeauthor{adev} in \cite{adev}, that types of noise, whose spectral density follows a power law
\begin{equation}
    S(f) = h_{\alpha} \cdot f^\alpha \label{eqn:power_law}
\end{equation}
can be easily identified in the Allan deviation plot. The constant $h_\alpha$ is called the power (intensity) coefficient. The most common types of noise encountered in experimental data and their representations can be found in table \ref{tab:adev_alpha}, which serves as a summary of this section. Since those types of noise is present in any measurement or electronic device, it warants a further discussion to understand their root causes and ideas to minimize them. While not a type of noise, linear drift can also be easily identified in the Allan deviation plot. It is therefore included in table \ref{tab:adev_alpha} as well.

\begin{table}[ht]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Amplitude noise type& Power-law coefficient $\alpha$& Allan variance $\sigma_A^2$\\
        \midrule
            White noise & $0$& $\frac 1 2 h_0 \tau^{-1}$ \cite{adev_noise_types}\\
            Flicker noise& $-1$& $2 \ln 2 \, h_{-1} \tau^0$ \cite{adev_noise_types}\\
            Random walk noise& $-2$& $\frac 3 2 \pi^2 h_{-2} \tau^{1}$ \cite{adev_noise_types}\\
            Burst noise& $0 \textrm{ and } -\!2$& $y_{RMS}^2\frac{\bar \tau^2}{\tau^2} \left(4 e^{-\frac{\tau}{\bar \tau}} - e^{-\frac{2 \tau}{\bar \tau}} + 2 \frac{\tau}{\bar \tau} - 3 \right)$\\
            Drift & --& $\frac 1 2 D^2 \tau^2$ \cite{adev_drift}\\
        \bottomrule
    \end{tabular}
    \caption{Power law representations using the Allan variance.}
    \label{tab:adev_alpha}
\end{table}

In order to arrive at a good understanding of the features seen in an Allan deviation plot, this section will provide the reader with examples of each type of noise and the corresponding time domain, power spectral density and Allan deviation plot. Since a complete overview is not available in current literature, all required mathematical descriptions and simulation tools will be discussed here. The simulations were done using Python and the source code is linked to in the discussions.

\clearpage
\subsubsection{White Noise}
White noise is probably the most common type of noise found in measurement data. Johnson noise found in resistors, caused by the random fluctuation of the charge carriers, is one example of mostly white noise up to bandwidth of \qty{100}{\MHz}, from where on quantum corrections are required \cite{nist_johnson_noise}. Amplifiers also tend to have a white noise spectrum at higher frequencies.

For this reason, white noise typically makes up for a considerabe amount of noise in a measurement, unless one works at very low frequencies. White noise is a series of uncorrelated random events and therefore characterised by a uniform power spectral density, which means there is the same power in a given bandwidth at all frequencies up to infinity. White noise therefore has infinite power (variance). In reality a measurement is always limited in bandwidth and hence the above property of a constant power spectral density only holds within that bandwidth. Those bandlimited samples of white noise thus have a finite variance.
Since white noise is so common, a few properties should be mentioned. One such property is, that the variance $\sigma_{x+y}^2$ of two uncorrelated variables $x$ and $y$ adds:
\begin{equation}
    \sigma_{x+y}^2  = \sigma_x^2 + \sigma_y^2 + \underbrace{2\,\mathrm{Cov}(x,y)}_{\text{uncorrelated}\, =\, 0}\ = \sigma_x^2 + \sigma_y^2 \label{eqn:adding_white_noise}
\end{equation}

This allows simple addition rules of variances from different sources, but it must be stressed here, that this property is only valid for uncorrelated sources like white noise, although it is usually incorrectly applied to all measurements in disreagard of the dominant noise present, which unfortunately obscures rather than clarifies the uncertainties involved.

In order to demonstrate the effect of white noise in Allan deviation plots, it was simulated using the excellent \textit{AllanTools} library \cite{allantools}. The noise generator chosen in the AllanTools library is based on the work of \citeauthor{noise_generation} \cite{noise_generation}. The full Python program code is published online \cite{}. For better comparison, all noise densities are normalized to give an Allan deviation of $\sigma_A(\tau_0)=1$, with $\tau_0$ being the smallest time interval between measurements.

Figure \ref{fig:white_noise_simulated} shows a sample of white noise in three different forms. Figure \ref{fig:white_noise_time} is the time series representation. From this sample, the power spectral density was calculated and is shown in figure \ref{fig:white_noise_psd}. The dashed line shows the expectation value of the power spectral density and the Allan deviation.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \scalebox{0.75}{%
            \input{images/white_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:white_noise_time}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \scalebox{0.75}{%
            \input{images/white_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:white_noise_psd}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \scalebox{0.75}{%
            \input{images/white_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:white_noise_adev}
    \end{subfigure}
    \caption{Different representations of white noise.}
    \label{fig:white_noise_simulated}
\end{figure}

From this simulation, several features can be observed. First of all, the power spectral density is flat and constant with $h_0 = 2$, which is in accordance with table \ref{tab:adev_alpha} and the normalization mentioned earlier. Figure \ref{fig:white_noise_adev} shows the typical $\tau^{-\frac 1 2}$ dependence of white noise in the Allan deviation plot. This immediately explains, why filtering white noise scales with $\frac{1}{\sqrt{n}}$ with $n$ being the number of samples averaged.

\clearpage
\subsubsection{Burst Noise}
Burst noise, popcorn noise, or sometimes referred to as random telegraph signal is a random bi-stable change in a signal and is caused by a generation recombination processes. This, for example, happens in semiconductors if there is a site, that can trap an electrons for a prologned period of time and then randomly release it. Imporities causing lattice defects are discussed in this context \cite{kay2012operational,burst_noise_psd,popcorn_noise_orgin,technote_ti_popcorn_noise}. Such latttice defects can also be introduced by ion implantation during doping. Fortunately, this type of noise has become less prevalent in modern manufacturing processes, because the quality of the semiconductors has improved. But if a trap site is located very close to an important structure, for example a high precision Zener diode, its effect might be so strong, that it can be clearly seen.

The discussion is split into two parts. First the power spectral density is calculated and then the Allan variance is caclulated using that result.

The spectral density of burst noise caused by a single trap site was derived in \cite{burst_noise_wiener_khinchin} by \citeauthor{burst_noise_wiener_khinchin}. The author used the autocorrelation function of the burst noise signal and applied the Wiener-Khinchin (Wiener-Хи́нчин) theorem, which connects the autocorrelation function with the power spectral density. A more detailed derivation can be found in \cite{fundamentals_of_noise_processes}, in this paper the preconditions, like stationarity of the process, are also discussed. The burst noise signal consists of two energie levels, called $0$ and $1$, split by $\Delta y$. Multiple burst noise signals can be superimposed in a real device. This would then result in mutiple levels, but they can be treated separately. The measurement interval over an even number of transitions, so that one ends in the same state as the measurement has started, is the time $T$. The mean lifetime of the levels is called $\bar \tau_0$ and $\bar \tau_1$:
\begin{equation}
    \bar \tau_{0} \approx \frac 1 N \sum_{i}^N \tau_{0,i} \qquad \bar \tau_{1} \approx \frac 1 N \sum_{i}^N \tau_{1,i}
\end{equation}

Figure \ref{fig:burst_noise} shows a burst noise signal along with the definitions above.

\begin{figure}[hb]
    \centering
    \scalebox{1}{%
        \import{figures/}{burst_noise.tex}
    } % scalebox
    \caption{A random burst noise signal.}
    \label{fig:burst_noise}
\end{figure}

Using these definitions, one can then derive \cite{burst_noise_wiener_khinchin}:
\begin{align}
    R_{xx} (T) &= \Delta y^2 \cdot \frac{\bar \tau_1 \bar \tau_0 e^{-\left(\frac{1}{\bar \tau_1}+\frac{1}{\bar \tau_0}\right)T}}{\left(\bar \tau_1 + \bar \tau_0\right)^2} \quad \text{and} \label{eqn:burst_noise_correlation}\\
    S(\omega) &= 4 R_{xx}(0) \frac{\frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0}}{\left(\frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0}\right)^2 + \omega^2} \qquad \omega > 0 . \label{eqn:burst_noise_psd}
\end{align}
Note, that the power spectral density is the one-sided version, hence an additional factor of $2$ is included. The d.c. term was ommitted here and can usually be neglected, because it is not relevant for calculating the power spectral density as it only contributes a single peak at $\omega=0$. Using the following definitions of the average time constant and the duty cycle

\begin{align}
    \frac{1}{\bar \tau} &= \frac{1}{\bar \tau_1} + \frac{1}{\bar \tau_0} \quad \mathrm{and} \label{eqn:definition_bar_tau}\\
    D_i &= \frac{\bar \tau_i}{\bar \tau_1 + \bar \tau_0} \quad i \in \{0 ; 1\}
\end{align}

equations \ref{eqn:burst_noise_correlation} and \ref{eqn:burst_noise_psd} can be rewritten to give a more intuitive form:

\begin{align}
    R_{xx} (T) &= \Delta y^2 D_1 D_0 \, e^{-\left(\frac{1}{\bar \tau_1}+\frac{1}{\bar \tau_0}\right)T}\\
    S(\omega) &= 4 R_{xx}(0) \frac{\bar \tau}{1 + \omega^2 \bar \tau^2} \label{eqn:burst_noise_lorentzian}
\end{align}

The special case $\bar \tau_0 = \bar \tau_1$ with $D_i=\frac 1 2$ is the previously mentioned case of random telegraph noise.

$R_{xx} (0)$ can be identified as the mean squared value of $y$:
\begin{equation}
    y_{RMS} = \sqrt{R_{xx}(0)} \,.
\end{equation}

Equation \ref{eqn:burst_noise_lorentzian} is a Lorentzian function and from this it can be easily seen, that a single trap site has a power spectral density, which is proportional to $\frac{1}{f^2}$ at high frequencies and is flat at low frequencies.

With the spectral density in hand, it is now possible to calculate the Allan variance as it was done by \citeauthor{allen_dev_flicker} in \cite{allen_dev_flicker} for the classic example of random telegraph noise where $\bar \tau_1 = \bar \tau_0$. Do note, that table I given by \citeauthor{allen_dev_flicker} shows the total number of events instead of the instantationous number of events typically given. Hence, their notation must be multiplied by $\frac{1}{\tau^2}$ (or $\frac{1}{T^2}$ in their notation). For the generic case with $\bar \tau_1$, $\bar \tau_0$ and the definition of $\bar \tau$ given in equation \ref{eqn:definition_bar_tau} one finds for the Allan variance of burst noise:
\begin{equation}
    \sigma^2_A(\tau) = R_{xx}(0) \frac{\bar \tau^2}{\tau^2} \left(4 e^{-\frac{\tau}{\bar \tau}} - e^{-\frac{2 \tau}{\bar \tau}} + 2 \frac{\tau}{\bar \tau} - 3 \right) \label{eqn:burst_noise_avar}
\end{equation}

Having arrived at equations \ref{eqn:burst_noise_lorentzian} and \ref{eqn:burst_noise_avar} of the power spectral density and Allan variance, it it now possible to model it. For this purpose, parts of the Python library \textit{qtt} \cite{qtt} was used. The algorithm written by \citeauthor{qtt} implements continous-time Markov chains to simulate the burst noise signal. The result can be see in figure \ref{fig:burst_noise_simulated}. For these simulations one time constant, namely the lifetime of the lower state $\bar \tau_0$ was held constant, while the lifetime of the upper state was varied to show the effect of different $\bar \tau$. By looking at the time domain in figure \ref{fig:burst_noise_time} it can be seen, that the maximum average number of state changes can be observed, when $\bar \tau_1 = \bar \tau_0$. If $\bar \tau_1 > \bar \tau_0$ the system will favour the upper, while if $\bar \tau_1 < \bar \tau_0$ it will favour the lower state instead. This explaines why the noise is strongest for random telegraph noise when $\bar \tau_1 = \bar \tau_0$, which can also be seen in power spectral density in figure \ref{fig:burst_noise_psd}. Looking at the Allan deviation in figure \ref{fig:burst_noise_adev} confirms this, but also shows another interesting implication as it shows an obvious maximum. If the application allows a choice over the sampling interval $\tau$, the effect of the burst noise can mitigated by staying well clear of the maximum.

The small deviation from the analytical solution in figure \ref{fig:burst_noise_adev}  at large $\tau$ is a typical so called end-of-data error. As it was discussed above, the Allan deviation can only be estimated given a limited number of samples using equation \ref{eqn:adev_estimator} and going to longer $\tau$ means there are fewer samples to average over.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.8\linewidth}
        \centering
        \scalebox{1}{%
            \input{images/burst_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:burst_noise_time}
    \end{subfigure}
    \begin{subfigure}{0.8\linewidth}
        \centering
        \scalebox{1}{%
            \input{images/burst_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:burst_noise_psd}
    \end{subfigure}
    \begin{subfigure}{0.8\linewidth}
        \centering
        \scalebox{1}{%
            \input{images/burst_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:burst_noise_adev}
    \end{subfigure}
    \caption{Different representations of burst noise for different $\bar \tau_1$ and fixed $\bar \tau_0 = \qty{1}{\s}$.}
    \label{fig:burst_noise_simulated}
\end{figure}

The burst noise equations can used to gain further insight into other types of noise. The first one is Shot noise, which is commonly found in photodetectors and lasers. Here, electrons or photons are created at discrete intervals resulting in an instantationous signal. This means, that the lifetime of the upper level is very short in comparison to the lower level ($\tau_1 \ll \tau_0$) equation \ref{eqn:burst_noise_psd} becomes:
\begin{align}
    S_{Shot}(\omega) = S_{\tau_1 \ll \tau_0}(\omega) &= 4 \Delta y^2 \frac{\tau_1}{\tau_0} \frac{\frac{1}{\bar \tau_1}}{\left(\frac{1}{\bar \tau_1}\right)^2 + \omega^2}\nonumber\\
    &= 4 \Delta y^2 \frac{1}{\tau_0} \frac{1}{\frac{1}{\tau_1^2}+\omega^2}\\
    \overset{\omega \ll 1/\tau_0}&{\approx} 4 \Delta y^2 \frac{\tau_1^2}{\tau_0} = \text{const.}
\end{align}

For the typical case, a very large number of such events happen. When not counting single events, but rather a stream, the relation $\omega \ll 1/\tau_0$ is valid and hence the results is a white spectrum as $S_{Shot}(\omega)$ is constant with respect to $\omega$ --- just as observed in photodetectors and lasers.

The other interesting case is a case, where many trap sites with different time constants are contributing to the noise. This can change the shape of the spectrum from $f^{-2}$ to $f^{-1}$ and is discussed in the next section.

\clearpage
\subsubsection{Flicker Noise}
Flicker noise is also called $\frac 1 f$-noise and it can be observed in many naturally occuring phenomenen. Its origin is not clear, although there have been many explanations. An overview can be found in \cite{flicker_noise_overview, flicker_noise_overview2, origins_1_f_noise}. This work concentrates on flicker noise in electronic devices. In thick-film resistors, for example, it was shown to extend over at least 6 decades without any visible flattening \cite{1_f_noise_thick_film}. In transistors, flicker noise is caused by the existance of generation-recombination noise or burst noise discussed in the previous section \cite{origins_1_f_noise}. If there are many uncorrelated trap sites, that contribute to the total noise, the envelope of the noise spectral density changes from $\frac{1}{f^2}$ to $\frac{1}{f^1}$ as shown in figure \ref{fig:flicker_noise_evelope}

\begin{figure}[hb]
    \centering
    \input{images/flicker_noise_envelope.pgf}
    \caption{Multiple overlapping Lorentzian noise sources forming a $\frac 1 f$-like shape.}
    \label{fig:flicker_noise_evelope}
\end{figure}

Given that no trap site can store an electron indefinetely, the number of trap sites $N$ with a certain time constant $\frac 1 2 \bar \tau = \bar \tau_0 = \bar \tau_1$ must decline for longer time scales. Assuming $N$ is inversely proportional to the time constant $\bar \tau$
\begin{equation}
    N(\tau) \propto \frac{1}{\bar \tau}\,, \label{eqn:flicker_noise_weight_function}
\end{equation}

which can be motivated if the trapping process is thermally activated \cite{1_f_noise_motivation} and using equation \ref{eqn:burst_noise_lorentzian} from the previous section, multiplying the weight function \ref{eqn:flicker_noise_weight_function} and integrating over all possible storage times gives:

\begin{align}
    S(\omega) &= \lim_{t \to \infty} \int_0^t N(\bar \tau) \, 4 R_{xx}(0) \frac{\bar \tau}{1 + \omega^2 \bar \tau^2} \, d\bar\tau \nonumber\\
    \overset{\bar \tau_0 = \bar \tau_1}&{=} 4 R_{xx}(0)\, C_N \lim_{t \to \infty} \int_0^t \frac{1}{1 + \omega^2 \bar\tau^2} \, d\bar\tau \nonumber\\
    &= \frac{4 R_{xx}(0)\, C_N}{\omega} \lim_{t \to \infty}  \arctan{\bar\tau \omega} \Big|_{\bar\tau=0}^t \nonumber\\
    &= \frac{4 R_{xx}(0)\, C_N}{\omega} \cdot \frac{\pi}{2} \nonumber\\
    &= \frac{2 \pi R_{xx}(0)\, C_N}{\omega}\\
    S(f) &= h_{-1} f^{-1}
\end{align}

$C_N$ is the proportionality constant of \ref{eqn:flicker_noise_weight_function} and $h_{-1}$ is the power coefficient introduced in \ref{eqn:power_law}. This shows, that for a large number of distributed trap sites, a noise spectrum of $f^{-1}$ is found.

Using equation \ref{eqn:psd_to_adev}, the Allan variance can be calculated from the power spectral density:
\begin{align}
    \sigma_A^2(\tau) &= 2 h_{-1} \int_0^\infty \frac{1}{f} \frac{\sin^4\left( \pi f \tau \right)}{(\pi f \tau)^2}\,df \nonumber\\
    &=2 \ln 2 \, h_{-1}
\end{align}


Again, using the \textit{AllanTools} library \cite{allantools}, flicker noise was simulated to give an impresion of its properties.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/flicker_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:flicker_noise_time}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/flicker_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:flicker_noise_psd}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/flicker_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:flicker_noise_adev}
    \end{subfigure}
    \caption{Different representations of flicker noise.}
    \label{fig:flicker_noise_simulated}
\end{figure}

While it is not immediately evident from the power spectral density, the Allan deviation plot explains very well, why additional filtering does not affect flicker noise. No matter how long the integration time, the variance will the same.

The small wiggles at longer $\tau$ are typical end-of-data errors caused by spectral leakage, because there are insufficient samples to average over \cite{adev_long_tau}. As it was discussed above, the Allan deviation can only be estimated using equation \ref{eqn:adev_estimator} given a limited number of samples. Therefore, at $\frac{\tau}{2}$ there are only $2$ samples left, so there is no averaging possible to improve the estimate of the Allan deviation, which causes the oscillations at low frequencies or large $\tau$.

As a last remark, a commonly used definition in combination with flicker noise is the corner frequency $f_c$. The corner frequency appears in situations, where there is both flicker and white noise present. It is the crossover point in frequency, where the flicker noise is equal compared to the white noise.
\begin{equation}
    f_c = \frac{h_{-1}}{h_0} \label{eqn:corner_frequency}
\end{equation}
It can be graphically extracted from the power spectral density plot by drawing a line trough the the flicker noise and the white noise and finding the intersection. This can be seen in figure \ref{fig:adev_example_psd} on page \pageref{fig:adev_example_psd}. The corner frequency can be found where the horizontal dashed blue and green line meet.

\clearpage
\subsubsection{Random Walk}
Random walk noise can be attributed to environmental factors such as temperature \cite{random_walk_fm} and diffusion processes, the latter contributing to the ageing effect seen in semiconductors.
It is a process, where in each time step the change is randomly determined to be either a positve or negative step with equal probability and a fixed step size. Its mean is
\begin{equation}
    \langle y_n \rangle = \langle e_1 + e_2 + \dots e_n \rangle = \underbrace{\langle e_1 \rangle}_{=\,0} + \langle e_2 \rangle + \dots + \langle e_n \rangle = 0 \, ,
\end{equation}
but its variance
\begin{equation}
    \sigma_y^2 = \langle y_n^2 \rangle - \underbrace{\langle y_n \rangle}_{=\,0} = \sigma_{e_1}^2 + \sigma_{e_2}^2 + \dots \sigma_{e_n}^2 = n \sigma_e^2
\end{equation}
goes with $n$ (or $t$). It therefore not a stationary process as can also be seen in figure \ref{fig:random_walk_adev}.

The power spectral density can be calculated \cite{psd_to_adev,noise_generation} to
\begin{equation}
    S(f) = h_{-2} \frac{1}{f^2}
\end{equation}
and the Allan deviation can again be calculated from the spectral density
\begin{align}
    \sigma_A^2(\tau) &= 2 h_{-2} \int_0^\infty \frac{1}{f^2} \frac{\sin^4\left( \pi f \tau \right)}{(\pi f \tau)^2}\,df \nonumber\\
    &=\frac{2}{3} \pi^2 h_{-2}\, \tau
\end{align}

The \textit{AllanTools} library \cite{allantools} can then be used to simulate the random walk.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/random_walk_noise_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:random_walk_time}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/random_walk_noise_psd.pgf}
        } % scalebox
        \caption{Power spectral density}
        \label{fig:random_walk_psd}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/random_walk_noise_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:random_walk_adev}
    \end{subfigure}
    \caption{Different representations of random walk noise.}
    \label{fig:random_walk_noise_simulated}
\end{figure}


\clearpage
\subsubsection{Drift}
Finally, the last feature of the Allan deviation plot, that needs to be discussed is drift. Drift happens at very long time scales and descriped a linear dependence of measurand on the time. This is also part of the ageing effect. \citeauthor{adev_drift} discussed the effect of drift \cite{adev_drift} on the Allan variance and found the following relationship:
\begin{align}
    \sigma_A^2(\tau) = \frac{D^2}{2} \tau^2
\end{align}
with slope of the drift $D$.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/drift_time.pgf}
        } % scalebox
        \caption{Time domain}
        \label{fig:drift_time}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/drift_adev.pgf}
        } % scalebox
        \caption{Allan deviation}
        \label{fig:drift_adev}
    \end{subfigure}
    \caption{Different representations of linear drift.}
    \label{fig:drift_noise_simulated}
\end{figure}

\clearpage
\subsubsection{Dead Time}
\label{sec:dead_time}
The coefficients given here were derived using the assumption, that all sampling in a measurement are continous and the dead time $\theta = 0$. Unfortunately, measurement sometime have a dead time, that is non negligible. This problem was extensively discussed by \citeauthor{psd_to_adev} \cite{psd_to_adev}. \citeauthor{adev_frequency_counter} even developed special models to account for the algorithms of modern frequency counters \cite{adev_frequency_counter}. While some frequency counters support gapless measurements, the situation is entirely different for digitizers and digital multimeters. Several settings commonly used affect the dead time, which can be considerable. It is therefore important to discuss typical measurement settings for voltmeters to estimate the errors that arise from those settings. The focus of this discussion lies on the dead time introduced by digital multimeters, but the application is not limited to this field.

The most commonly used settings, that affect the dead time of a voltmeter are auto-zeroing and line synchronization. Auto-zeroing is done by adding additional measurements to the normal input integration cycle. To correct for the zero offset drift a zero measurement is added where the adc is switched to the low terminal. Additionally, some devices add a reading of the reference voltage to correct for gain errors. The implementation details and type of measurements are manufacturer dependant and must be determinded for every multimeter used.

The other setting, that can be enabled in voltmeters, is the line synchronization to increase the noise rejection of the instrument. This setting synchronizes the start of a measurement to the zero crossing of the power line. Depending on the instrument, this might cause a delay of one power line cycle (PLC) after each measurment if the instrument is not capable of processing the previous measurement while at the same time recording another one.

A simple measurement with dead time is shown in figure \ref{fig:allan_variance_definitions} on page \pageref{fig:allan_variance_definitions}. The model assumes, that the dead time is constant and is always added after the actual integration time $\tau$. This is rarely true for real measurement data as many devices and even ADCs use internal averaging and auto-zeroing to produce a measurement. The actual dead time is therefore spread over the whole measurement and not limited to the end of the measurement. An example is the Keysight \device{3458A} DMM, which automatically switches to averaging when selecting integration times greater than \qty{10}{\plc}. The reason is simple, for longer integration times, more and more flicker noise starts contributing to the measurement. The measurement is therefore split into single measurements of \qty{10}{\plc} and using auto-zeroing the flicker noise is suppressed. This is discussed in more detail as an example in section \ref{sec:autozero}. The mathematical problem of a distributed dead time was already noted by \citeauthor{adev_noise_types} \cite{adev_noise_types} and it is distinctively different from the calculations made by \citeauthor{psd_to_adev} for a single dead time at the end of the measurement. The exact mathematical treatment is complex and is beyond the scope of this work, especially considering, that auto-zeroing does a lot more than just adding dead time at the end of the measurement. Fortunately using a few assumptions the problem can be greatly simplified.

An interesting observation can be made for white noise. Since it is uncorrelated, it makes no difference whether it is sampled in full, or only partially, therefore the Allan deviation for a white noise process with or without dead time is the same:
\begin{equation}
    \sigma^2(N,T, \tau) = \sigma^2(N=2,T=\tau, \tau) = \sigma_A^2(\tau) \frac 1 2 h_0 \tau^{-1}
\end{equation}

Consequently, if the dead time is added at a frequency high enough, so that the input amplifier output is dominated by white noise, the dead time will have no influence on the Allan variance.

Finally, \citeauthor{psd_to_adev} \cite{psd_to_adev} notes that for measurement durations or averaging times $T \gg T_0$, the Allan variance with respect to $T$ shows an asymptotic behaviour of $\sigma_A^2(T) \to \sigma_A^2(\tau)$.

\clearpage
\subsection{Example}
\label{sec:noise_example}
Using the results from the previous sections, it is possible to simulate a typical measurement sample containing white noise, flicker noise and random walk behaviour. The simulation was written in Python using the \textit{AllanTools} library \cite{allantools} to generate the time domain data, which was then converted to a power spectrum using the algorithm of \citeauthor{welch} \cite{welch}. The Allan deviation was calculated using the \textit{AllanTools}. The full Python source is available at \cite{}. The time domain data shown here was downsampled from $2^{25}$ data points to $2000$ points for faster plotting, using the Largest-Triangle-Three-Buckets (LTTB) algorithm created by \citeauthor{lttb} \cite{lttb}. The downsampling algorithm chosen is optimal for this application, because it aims to visually keep the result the same by favouring parts of the data, where there is more change. The only difference noticable to the author is, that the edges of the white noise plot are a little rougher. The full data set can be obtained using the source code given above if one desires. The power spectrum and the Allan deviation were always calculated from the full dataset. The data of the power spectrum was additionally binned to be evenly spaced on a logarithmic scale. This considerably reduced the high frequency noise and made the plot easier while not negatively impacting the shape.

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/example_white_time.pgf}
        } % scalebox
        \caption{White noise}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/example_flicker_time.pgf}
        } % scalebox
        \caption{Flicker noise}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \centering
        \scalebox{0.75}{%
            \input{images/example_rw_time.pgf}
        } % scalebox
        \caption{Random walk}
    \end{subfigure}
    \caption{Three sperate noise components, that were summed together to simulate a typical noise source.}
    \label{fig:adev_example_noise_types}
\end{figure}

The the three time series shown in figure \ref{fig:adev_example_noise_types} were sequentially generated using a fixed seed for the random number generator to ensure repeatability as long as the order of creation is kept the same. For generting the noise, the algorithm presented by \citeauthor{noise_generation} \cite{noise_generation} and implemented in the \textit{AllanTools} library was used. The noise strength parameters were deliberately chosen in such a way, that both the white noise and the random walk part have more noise power than the flicker noise. This allows to distinguish them in the plots at both extremes of the frequency scale. Finally, the three types of noise data were summed together to give the combined signal. The time series is shown in figure \ref{fig:adev_example_time}, again downsampled using LTTB. The summed series clearly shows the white noise content and it is possible to deduce some flicker or random walk noise, but it is highly obscured due to the amount of white noise. Using only the time domain plot makes it very hard to distinguish the type of noise present, let alone estimate the individual noise power of the three sources. Therefore, a different analysis tool is called for.

\begin{figure}[ht]
    \centering
    \input{images/example_time.pgf}
    \caption{A simulated time series containing white noise, flicker noise and random walk behaviour.}
    \label{fig:adev_example_time}
\end{figure}

A common approach to identify noise sources is the power spectrum. It is easily accessible, even in real-time, using spectrum analyzers and, utilizing the computational power of modern computers, large time-domain data sets can be converted making this the method of choice in the lab. The power spectrum of figure \ref{fig:adev_example_time} is shown in figure \ref{fig:adev_example_psd}. It allows to clearly separate the white noise part from the other $f^{\alpha}$ components. The dashed lines representing the individual components were plotted using the $h_\alpha$ values calculated from the input parameters of the simulation. The noise spectral density $h_0$ of the white noise signal can be easily extracted even by hand without resorting to a fit. This yields $h_{0} = \qty{2e-3}{\per \Hz} $. $h_{-1}$ and $h_{-2}$ can be exctracted as well using a fit to
\begin{equation}
    S(f) = \sum_{\alpha = -2}^0 h_\alpha f^\alpha \, .
\end{equation}
The noise corner frequency $f_c$ can either be calculated from $h_0$ and $h_{-1}$ using equation \ref{eqn:corner_frequency} or determined graphically by contructing a tangent with a slope of $-1$ to the spectral density. From the intersection of the blue $h_0$ line and the green $h_{-1}$ line the corner frequncy is found to be $f_c \approx \qty{1.8}{\kHz}$.

\begin{figure}[hb]
    \centering
    \input{images/example_psd.pgf}
    \caption{A simulated power spectrum containing white noise, flicker noise and random walk behaviour.}
    \label{fig:adev_example_psd}
\end{figure}

To get an even better representation of the individual noise contributions, the Allan variance or Allan deviation can be used. The Allan deviation plot shown in figure \ref{fig:adev_example_adev} gives very clean results and all noise components can be clearly identified. The individual components were plotted using dashed lines as well.

\begin{figure}[hb]
    \centering
    \input{images/example_adev.pgf}
    \caption{A simulated Allan deviation containing white noise, flicker noise and random walk behaviour.}
    \label{fig:adev_example_adev}
\end{figure}

The Allan variance was calculated using the overlapping Allan variance algorithm \cite{oadev_definition} and only Allan deviation values for frequency vales of $(1, 2, 4)$ per decade were plotted. The overlapping Allan variance gives a better confidence at longer intervals or lower frequencies, allowing to identify very low frequency noise like the random walk shown here. Reference \cite{oadev_definition} also gives a very good comparison of other algorithms to identify even more noise types in data sets, like phase noise. Plotting only three values per decade improves the clarity of the plot, because at longer taus, even though the overlapping Allan variance is used, some oscillations inevitably show up. Using fewer values of $\tau$ causes less distractions in this case.
From the figure \ref{fig:adev_example_adev}, the Allan deviation of the flicker noise can be estimated from the flat minimum to be around $2.3$ or $\sqrt{5}$. Using table \ref{tab:adev_alpha} the Allan variance can be converted to
\begin{equation}
    h_{-1} = \frac{5}{2 \ln 2} \approx 3.6 \nonumber
\end{equation}

Using the previously found $h_0$, this corner frequency is calculated using equation \ref{eqn:corner_frequency} to be:
\begin{equation}
    f_c = \frac{5}{\qty{2e-3}{\per \Hz} \cdot 2 \ln 2} \approx \qty{1.8}{\kHz} \nonumber
\end{equation}

This is obviously the same result as the one from the geometric approach above.

\clearpage
\section{Autozeroing}
\label{sec:autozero}
Autozeroing (AZ), sometimes called zero-drift or dynamic offset compensation, is such an important concept, that it must be discussed in its own right. The need for autozeroing comes from the typical behaviour of amplifiers. Every amplifier has some offset, be it small or large, and especially at high gains, this offset becomes a problem for high precision measurements. To make matters worse, this offset is not stable over time and drifts with both time and temperature. It can therefore not be calibrated out once, it must be permanently adjusted during operation, depending on environmental conditions. This procedure is called autozeroing.

There are many different ways to implement autozeroing and regarding operational amplifiers a good overview can be found in \cite{horowitz1989}. As an example, the autozero cycle for the Keithley \device{Model 2002} and the Keysight \device{3458A} Mulimeter is shown in figure \ref{fig:dmm_autozero_comparison}. Keithley uses a more complex and slower algorithm, while HP implemented a simpler, but faster algorithm. The most simple (digital) approach is to regularly switch the input from the signal to zero, take a reading, then subtract this reading from all subsequent readings until a new zero reading is taken. The other second approach adds another measurement of the reference voltage to apply a gain correction. This is done by the Keithley \device{Model 2002} and works very well to suppress gain drift in the input amplifier due to temperature changes, but increases the time between samples by another \qty{50}{\percent}. This is the reason, why on the \device{Model 2002}, there is another mode to strech the time between autozero cycles, but the price to pay is a non-uniform sampling rate. This makes post-processing a lot more complicated as most alorithms assume a (near) constant sampling rate. The Keysight \device{3458A} calculates gain corrections only during the manual autocalibration routine to maintain a higher throughput.

\begin{figure}[hb]
    \centering
    %\resizebox {0.8\textwidth} {!} {
        \import{figures/}{dmm_autozero.tex}
    %} % resizebox
    \caption{Auto-zero phases of the Keysight \device{3458A} and Keithley \device{Model 2002}.}
    \label{fig:dmm_autozero_comparison}
\end{figure}

\subsection{Offset-Nulling}
Offset-nulling is the most basic approach to autozeroing. It aims to remove the offset drift of an amplifier. Especially at high gains, the offset, which is multiplied by the gain, can be substantial. In order to explain how offset-nulling works and how it shapes the spectrum, it is best to discuss it based on an example. While this technique can also be found in many integrated circuits, it is more noticable in DMMs, because is a switchable option. Therefore, the example data set simulated is based on the parameters of the aforementioned Keysight \device{3458A} multimeter. The corner frequency and the white noise floor is modeled after the \qty{10}{\V} range of the \device{3458A} \cite{3458A_noise_floor, sampling_with_3458A} with the values given below. Do note that both references \cite{3458A_noise_floor, sampling_with_3458A} contain a typographical error. The corner frequency of the noise floor is erroneously given as \qty{0.5}{\Hz}, but should be \qty{1.5}{\Hz}. This can be seen in figure 2.35 in \cite{sampling_with_3458A}, where the noise spectral density is plotted and it was also confirmed with the author \cite{lapuh_email_corner_frequency}. The sample is generated using the Python \textit{AllanTools} library \cite{allantools}.

\begin{figure}[ht]
    \centering
    \scalebox{1}{%
        \import{figures/}{offset_nulling_definitions.tex}
    } % scalebox
    \caption{Integration sequences of the offset-nulling algorithm. Solid lines denote sampled data. Red is the input signal, green is the zero reading and blue is the dead time required for switching inputs.}
    \label{fig:dmm_autozer_offset_nulling}
\end{figure}

For this simulation, a noise-free and arbitrarily chosen \qty{10}{\V} input is assumed to be sampled by the device at a sampling rate of \qty{10}{\plc} at \qty{50}{\Hz}, the same rate discussed previously on page \pageref{sec:dead_time}. As it will be shown, the actual mean value of the input signal has no bearing on the outcome of the calculation when considering offset-nulling, but its value must be considered for other types of autozeroing as discussed in section \ref{sec:autozero_gain} and is included here only for the sake of completeness.

Figure \ref{fig:dmm_autozer_offset_nulling} shows the individual sequences of the offset-nulling algorithm. First, the source is sampled for $\tau_s = \qty{10}{\plc}$, then the input is switched to the LO terminal. While this operation is very fast and takes less than \qty{1}{\ms} \cite{article_3458A_input_mpedance}, if the instrument is synchronized to the line frequency the zero measurement will nontheless be delayed until the next zero crossing, hence the dead-time $\theta = \qty{1}{\plc}$. Finally, the zero reference is measured for another $\tau_r = \qty{10}{\plc}$ and then the instrument switches back to the HI terminal.

The data is simulated in the following way. First, two sets of noise data are generated, a white noise spectrum with a noise spectral density of $\qty[power-half-as-sqrt, per-mode=symbol]{165}{\nV \Hz\tothe{-0.5}}$ and a flicker noise spectrum with an intesity scaled to result in a final spectrum with a corner frequency of \qty{1.5}{\Hz}. The required flicker noise intensity is calculated using equation \ref{eqn:corner_frequency}. To get a good low frequency estimate, $2^{20} \approx 10^{6}$ values were generated. Finally, the two noise data sets are summed with the noise-free input source to give the final result. Other effects, such as power-line hum are neglected in this simple simulation, because it would needlessly overcomplicate the example and limit the educational value. The same goes for higher order random-walk $f^{-2}$ noise components, which can be introduced by temperature fluctions and other environmental effects and would be present in a real measurement.

\begin{figure}[ht]
    \centering
    \input{images/autozero_raw_time.pgf}
    \caption{Time series data with white noise and flicker noise.}
    \label{fig:autozero_raw_time}
\end{figure}

The time domain plot of the simulation is shown in figure \ref{fig:autozero_raw_time}. The white noise component is clearly visible, while the $f^{-1}$ flicker noise can be recognized, but its strength can hardly be estimated. It was already shown in section \ref{sec:noise_example}, different types of noise have different frequency components and can be be distinguished in the frequency domain, which leads to the next approach.

The noise power spectral density shown in figure \ref{fig:autozero_raw_psd} is calculated from the time series and confirms the flicker and white noise content. The theoretical white noise floor is shown as a horizontal dashed blue line and the flicker noise as a dashed green line. The \qty{1.5}{\Hz} corner frequency, which is defined as the intersection between the $f^{-1}$ noise and the white noise floor easily identified using the those lines. It is evident, that the \qty{5}{\Hz} sampling frequency with a \qty{2.5}{\Hz} bandwidth does not allow the spectral density to fully settle to the noise floor.

\begin{figure}[hb]
    \centering
    \input{images/autozero_raw_psd.pgf}
    \caption{Simulated power spectrum of a Keysight \device{3458A} containing white noise and flicker noise.}
    \label{fig:autozero_raw_psd}
\end{figure}

From the power spectral density is can be seeen, that higher frequencies have a significantly lower noise spectral density than the low frequencies. It is therefore most beneficial, to so measurements a higher frequencies. To discuss the optimal measurement interval, the Allan deviation is an excellent tool.

\begin{figure}[ht]
    \centering
    \input{images/autozero_raw_adev.pgf}
    \caption{Simulated Allan deviation of the input amplifier of a Keysight \device{3458A} containing white noise and flicker noise.}
    \label{fig:autozero_raw_adev}
\end{figure}

The Allan deviation it plotted in figure \ref{fig:autozero_raw_adev} and shows two distinct regions. Short $\tau$ display an asymptotic behaviour towards white noise with a $\tau^{−0.5}$ dependence and at longer $\tau$ the constant flicker noise region can be identified. At very long $\tau$ typical end-of-data oscillations can be seen, which are the result of the limited confidence of the Allan deviation estimator as previously discussed and can be safely ignored. The Allan deviation clearly demonstrates the performance of the device at longer integration times and it is obvious, that an beyond integration time of about \qty{1}{\second} or \qty{50}{\plc} no additional information can be extracted from the measurement and the variance is constant. This leads to the need to autozeroing to remove the flicker noise. It can be shown \cite{autozero_with_dead_time}, that subtracting a reference measurement from the actual measurement data removes all correlated effects. Since flicker noise is autocorrelated, it can be removed by subtracting a zero measurement.

To demonstrate autozeroing, two cases will be discussed. Going back to figure \ref{fig:dmm_autozer_offset_nulling} it can be seen, that between switching inputs, a dead time $\theta$ is added. For a first discussion, this dead time neglected and then the effect of adding a dead time is discussed.

Using figure \ref{fig:autozero_raw_adev} it was shown, that integrating over flicker noise, does not reduce the variance. In order to have as little flicker noise content in the final measurement value it is clear that the autozeroing should be done as fast possible to keep the flicker noise content out. This allows to calculate the expected variance of the autozeroed measurement. The noise of the input measurement $x$ and the reference measurement $y$ are the same, because in this model the only noise source comes from the input amplifier, as the input signal is assumed to be noise-free. The zero level is, by definition, noise-free. As dicussed above, the autozero interval is chosen, so that its variance is dominated by white noise The variance $\sigma^2$ of the combined measurement of $x-y$ can then be calculated using equation \ref{eqn:adding_white_noise}:
\begin{equation}
    \sigma_{x-y}^2 = \sigma_x^2 + \sigma_y^2 \label{eqn:autozeroing}
\end{equation}

By subtracting the zero reading, the amplifier noise is effectively added twice to the final result, once for the input measurement and once for the zero measurement. Additional noise from the input signal noise would simply be added to this as it is uncorrelated as well.

Do note, that the number of samples is now half the number before applying autozeroing. This leads to an interesting effect. Imagine a data set containing only white noise with a variance $\sigma^2$. Removing half the samples, obviously does not change the variance as white noise is not correlated, but subtracting the samples is effectively decimating the data set and since the sampling rate is halfed, the Nyquist band is halfed as well. Unfortunately the input noise bandwdith stays the same. The second Nyquist band is then folded back into the first, thus doubling the noise power density.

To conclude, it is expected, that the variance doubles and the power spectral density quadruples!

These consideration can be compared to the simulated data. Applying the autozeroing algorithm to the simulated data set, the constant \qty{10}{\V} input signal was nulled for every odd value and then the residual noise was subtracted from the signal value. The result in the time domain is shown in figure \ref{fig:autozero_time}.

\begin{figure}[hb]
    \centering
    \input{images/autozero_time.pgf}
    \caption{Simulated measurement with auto-zeroing applied.}
    \label{fig:autozero_time}
\end{figure}

When comparing to figure \ref{fig:autozero_raw_time} it is immediately evident, that the $f^{-1}$ component is no longer present. The difference in white noise strength is difficult to compare and it must be turned gain to the power spectral density. When calculating the the spectral density it is important to remember, that the sampling rate is now halved. The result is shown in figure \ref{fig:autozero_psd} along with dashed lines showing the noise content prior to applying the autozero algorithm as before in figure \ref{fig:autozero_raw_psd}.

\begin{figure}[ht]
    \centering
    \input{images/autozero_psd.pgf}
    \caption{Simulated power spectrum of a Keysight \device{3458A} with autozeroing applied. The dashed lines denote the noise present prior to applying the autozero algorithm.}
    \label{fig:autozero_psd}
\end{figure}

The power spectral density in figure \ref{fig:autozero_psd} confirms an increase in the white noise power as discussed above and it can be determined that the white noise power $\sqrt{h_{-1}}$ has increased from $\qty[power-half-as-sqrt, per-mode=symbol]{165}{\nV \Hz\tothe{-0.5}}$ to $\qty[power-half-as-sqrt, per-mode=symbol]{489}{\nV \Hz\tothe{-0.5}}$, an increase by a factor of $\sqrt{8.8}$, which is more than estimated by \ref{eqn:autozeroing}, including the factor of $2$ for the decimation, which gauged the increase of $\sqrt{h_{-1}}$ to be $\sqrt{4}$ . The reason for the additional noise was already mentioned above. There is still some substantial $f^{-1}$ noise present at the autozero frequency of \qty{5}{\Hz}. This type of noise is not uncorrelated and therefore the covariance is not zero, hence equation \ref{eqn:adding_white_noise} does not strictly hold. The hypothesis can be confirmed, by moving the corner frequency in the simulation from \qty{1.5}{\Hz} a decade lower to \qty{0.15}{\Hz}. The white noise floor would then only increase by a factor of $\sqrt{4.5}$.

Nonetheless, down to very low frequencies the $f^{-1}$ noise is effectively suppressed and the spectral density is almost perfectly flat.

The Allan deviation plot in figure \ref{fig:autozero_adev} also confirms that white noise is the only component and shows a $\tau^{-\frac 1 2}$ dependence for the full range of integration times.

\begin{figure}[hb]
    \centering
    \input{images/autozero_adev.pgf}
    \caption{Simulated Allan deviation of a Keysight \device{3458A} with autozeroing applied. The dashed lines denote the deviation prior to applying the autozero algorithm.}
    \label{fig:autozero_adev}
\end{figure}

From this plot it can be seen, that for measurement times longer than about \qty{2}{\s} or \qty{100}{\plc} autozeroing has a clear benefit over a measurement without autozeroing. It must be noted though, that judging from this simulation, the device would reach a noise floor of \qty[per-mode = symbol]{0.01}{\V \per \V} only at integration times of slighly more than \qty{10}{\s}, while the datasheet claims \qty{2}{\s}. It is therefore likely, that the noise parameters of a real device are be better than the numbers used in the simulation. Additionally, the datasheet likely refers to an instrument, that is synced to a \qty{60}{\Hz} power line frequency which shifts the sampling frequency up by \qty{20}{\percent} and, as discussed, reduces the noise floor, because more noise content is white noise at the autozero interval. In this simulation the \qty{0.01}{\V \per \V} noise level would be reached at exactly \qty{10}{\s} when using a line frequency of \qty{60}{\Hz}. For the purpose of demonstrating the autozeroing algorithms these subtleties are irrelavant.

For the comparison of different intregation times before applying autozeroing figure \ref{fig:autozero_nplcs_adev} can be consulted. Using the Allan deviation makes it is very simple to compare noise figures for identical measurement times $\tau$, but different integration times before autozeroing is applied.

\begin{figure}[ht]
    \centering
    \input{images/autozero_nplcs_adev.pgf}
    \caption{Allan deviation for different integration times before applying the AZ algorithm. Deadtime $\theta = 0$. The dashed line denotes the Allan variance without AZ.}
    \label{fig:autozero_nplcs_adev}
\end{figure}

It can be seen, that with increasing integration times before applying the AZ algorithm more uncertainty is accumulated due to the $f^{-1}$ content, which cannot be filtered. As a result, after removing the $f^{-1}$ content using autozeroing more time is required for filtering until the the same Allan deviation can be reached. From these simulations it can be said, that when there is negligible dead time $\theta$ involved when switching the inputs it is advantageous to switch to switch early, while white noise is still dominating the noise.

Finally, the case of a non-negligible dead time shall be treated. When the dead time has to be considered, it is clear, that the autozero frequency cannot be arbitrarily increased, because the proportion of sampling time lost, in comparison to the time spent sampling, increases. This effective loss in sampling time then increases the noise spectral density due to aliasing as discussed above. To show this effect the simulation above is modified to include a dead time of \qty{1}{\plc} as detailed in figure \ref{fig:dmm_autozer_offset_nulling}. The measurement sequence now includes a dead time after switching each input. \citeauthor{autozero_with_dead_time} proposes \cite{autozero_with_dead_time} splitting the measurement interval in two and instead of measuring HI-LO-HI-LO, to measure HI-LO-LO-HI. This scheme is a mixed bag, because the $f^{-1}$ noise is correlated and its autocorrelation function decays with $e^{-t}$, therefore constantly changing the order of subtracted samples is not as efficient in removing the noise as the normal autozero procedure. Only when the dead time is large in comparison to the measurement time, this method yields an advantage. Therefore only the simplest case of a HI-LO-HI-LO measurement is treated here. The Allan deviation for different integration times is evaluated in same way as in figure \ref{fig:autozero_nplcs_adev}. The results are shown in figure \ref{fig:autozero_deadtime_nplcs_adev}.

Figure \ref{fig:autozero_deadtime_nplcs_adev} demonstrates, that the effectiveness of the AZ scheme no longer increases with an increasing switching frequency and there is an optimal autozero interval. For the parameters chosen for this simulation ($f_c = \qty{1.5}{\Hz}$ and \qty[power-half-as-sqrt, per-mode=symbol]{165}{\nV \Hz\tothe{-0.5}}), \qty{5}{\plc} is the optimal interval. If the corner frequency is shifted to a lower frequency, the optimum shifts more towards \qty{10}{\plc}. The same goes for a higher line frequency of \qty{60}{\Hz}. This explains, why HP chose \qty{10}{\plc} as the maximum integration time. For integration times higher than that, software averaging is used, therefore delivering the perfomance shown in figure \ref{fig:autozero_deadtime_nplcs_adev} along the \qty{10}{\plc} line.

\begin{figure}[ht]
    \centering
    \input{images/autozero_deadtime_nplcs_adev.pgf}
    \caption{Allan deviation for different integration times before applying the AZ algorithm. Deadtime $\theta = \qty{1}{\s}$. The dashed line denotes the Allan variance without AZ.}
    \label{fig:autozero_deadtime_nplcs_adev}
\end{figure}

It should be stressed here, that the dead time is not be the only factor to consider when chossing the autozero interval. For example, in case of an amplifier, switching the input also adds an error current due to the charge injection of the switching transistors. This may negatively impact the measurement of a high impedance source. These additional drawbacks are implementation specific and must considered during the design phase.

\clearpage
\subsection{Gain Correction}
\label{sec:autozero_gain}
The effect of the gain correction, where the input value $x$ is scaled by a scaling factor $y$ to adjust the changing, can be calculated, assuming white noise, as follows:
\begin{align}
    \sigma_{x \cdot y}^2 &= \langle x^2 y^2 \rangle - \langle x y \rangle^2 \nonumber\\
    &= \langle x^2 \rangle \langle y^2 \rangle + \underbrace{2\,\mathrm{Cov}\left(x^2,y^2\right)}_{\text{uncorrelated} \, = \, 0} - \left( \langle x \rangle \langle y \rangle + \underbrace{2\,\mathrm{Cov}\left(x,y\right)}_{=\, 0} \right)^2 \nonumber\\
    &= \left(\sigma_x^2 + \langle x \rangle^2\right) \cdot \left(\sigma_y^2 + \langle y \rangle^2\right) - \langle x \rangle \langle y \rangle \nonumber\\
    &= \sigma_x^2 \sigma_y^2 + \sigma_x^2 \langle y \rangle^2 + \sigma_y^2 \langle x \rangle^2 \label{eqn:variance_multiplied}\\
\end{align}

With respect to the gain correction, equation \ref{eqn:variance_multiplied} can be further reduced. The scaling factor is derived from the reference voltage $V_{ref}$ and normalized using $\frac{V_{ref, meas}}{V_{ref}}$. The expected value, therefore is $\langle y \rangle \approx 1$, as the ADC should not drift much from its calibrated value. Furthermore, $\sigma_y^2$ is scaled by the constant $1/V_{ref}$ and $\sigma_x^2 \sigma_y^2 \ll \sigma_x^2$. The latter should be true for any measurement of significance.

\begin{equation}
    \sigma_{x \cdot y}^2 \approx \sigma_x^2 + \sigma_y^2 \langle x \rangle^2
\end{equation}

The gain correction noise therefore behaves similar to the offset correction case, except, that it scales with input voltage and has no effect, with a shorted input, while introducing its noise only for a full scale input.


% check \cite{psd_to_adev} Appendix II for details on dead time
% Compare PSD in Generation-Recombination Noise, Allan Variance, and Low-Frequency Gain Instabilities in Microwave Amplifiers to our controller. The hump look similar. Due to popcorn noise

\clearpage
\section{Temperature Controller}

% include Investigation of Long-Term Drift of NTC Temperature Sensors with less than 1 mK Uncertainty

\subsection{Tuning of a PID controller}
The number of emperical algorithms to determine a set of PID parameters ($\mathrm{K_p, K_i, K_d}$) are numerous. In this work only the most common algorithms and a few notable exceptions will be presented.
\subsection{Design}
